{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import hashlib\n",
    "import json\n",
    "from enum import Enum\n",
    "import os\n",
    "\n",
    "import pprint\n",
    "printer = pprint.PrettyPrinter(depth=4, width=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data.data_dicts import character_dict\n",
    "\n",
    "def is_character(char):\n",
    "    if char in character_dict.keys() and char != 'Default':\n",
    "        return True\n",
    "    elif char == 'Base' or char == 'Common':\n",
    "        return False\n",
    "    else:\n",
    "        logging.error(\"Unknown character name \" + char + \"!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount google drive\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive',force_remount=True)\n",
    "    base_folder = '/content/drive/My Drive/unibo/NLP_project/BarneyBot'\n",
    "    # Install dependencies\n",
    "    os.system(\"pip install datasets\")\n",
    "    os.system(\"pip install transformers\")\n",
    "    os.system(\"pip install rouge_score\")\n",
    "    os.system(\"pip install -U sentence-transformers\")\n",
    "else:\n",
    "    base_folder = os.getcwd()\n",
    "    \n",
    "# Set metrics folder for inputs\n",
    "in_metrics_folder = os.path.join(base_folder, 'Metrics', 'New')\n",
    "if not os.path.exists(in_metrics_folder):\n",
    "    os.makedirs(in_metrics_folder)\n",
    "\n",
    "# Set metrics folder for outputs\n",
    "out_metrics_folder = os.path.join(base_folder, 'Metrics', 'New')\n",
    "if not os.path.exists(out_metrics_folder):\n",
    "    os.makedirs(out_metrics_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store a json file\n",
    "def save_as_json(filepath, filename, data):\n",
    "    if not os.path.exists(filepath):\n",
    "        os.makedirs(filepath, exist_ok=True)\n",
    "    with open(os.path.join(filepath, filename + \".json\"), 'w') as f:\n",
    "        f.write(json.dumps(data, indent=4))\n",
    "\n",
    "# Load a json file\n",
    "def load_from_json(filepath, filename):\n",
    "    if not os.path.exists(os.path.join(filepath, filename + '.json')):\n",
    "        return dict()\n",
    "    with open(os.path.join(filepath, filename + '.json'), 'r') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "# Compute a string hash from a (nested) dict\n",
    "def dict_hash(dictionary: Dict[str, Any]) -> str:\n",
    "    dhash = hashlib.md5()\n",
    "    encoded = json.dumps(dictionary, sort_keys=True).encode()\n",
    "    dhash.update(encoded)\n",
    "    return dhash.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricDependency(int, Enum):\n",
    "    DATASET = 0      # Metric depends on datasets only and/or base DialoGPT model\n",
    "    COHERENT = 1     # Metric depends on chatbot trained on its dataset\n",
    "    ADVERSARIAL = 2  # Metric depends on chatbot trained on a dataset but with another dataset of reference\n",
    "    COMPARATIVE = 3  # Metric depends on comparison between chatbots\n",
    "\n",
    "class MetricArity(int, Enum):\n",
    "    SINGLE = 1\n",
    "    PAIRWISE = 2\n",
    "\n",
    "class MetricDeterminism(int, Enum):\n",
    "    DETERMINISTIC = 0 # There is a closed-form equation for this metric, which is fully computed\n",
    "    PROBABILISTIC = 1 # The metric is obtained through explainable approx., e.g. SGD, partial computation on a subset...\n",
    "    NEURAL = 2        # The metric is obtained via a neural network\n",
    "    HUMAN = 4         # The metric is obtained via human surveying\n",
    "\n",
    "class MetricActor(int, Enum):\n",
    "    DATASET_CHARCONTEXT = 0     # any character but not 'Default', including \"Common\"\n",
    "    DATASET_CHAR = 1   # any character but not 'Default', including \"Common\"\n",
    "    DIALOGPT_GREEDY = 10  # any character including 'Base'\n",
    "    DIALOGPT_NBEAMS = 11  # any character including 'Base'\n",
    "    DIALOGPT_SAMPLE = 12  # any character including 'Base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric_arity(metric_name):\n",
    "    if metric_name == 'bleu' or metric_name == 'rouge l' or metric_name == 'semantic similarity' or \\\n",
    "       metric_name == 'semantic answer similarity' or metric_name == 'chatbot classifier' or metric_name == 'perplexity':\n",
    "        return MetricArity.PAIRWISE\n",
    "    elif metric_name == 'distinct' or metric_name == 'emotion classifier' or metric_name == 'lines count':\n",
    "        return MetricArity.SINGLE\n",
    "    elif metric_name == 'dummy metric':\n",
    "        return MetricArity.PAIRWISE\n",
    "    else:\n",
    "        logging.error(\"Unknown arity for metric \" + metric_name)\n",
    "        \n",
    "def get_metric_determinism(metric_name, metric_version):\n",
    "    if metric_name == 'lines count' and metric_version == 1:\n",
    "        return MetricDeterminism.DETERMINISTIC\n",
    "    elif metric_name == 'bleu' and metric_version == 1:\n",
    "        return MetricDeterminism.DETERMINISTIC\n",
    "    elif metric_name == 'rouge l' and metric_version == 1:\n",
    "        return MetricDeterminism.DETERMINISTIC\n",
    "    elif metric_name == 'semantic similarity' and metric_version == 1:\n",
    "        return MetricDeterminism.NEURAL\n",
    "    elif metric_name == 'semantic answer similarity' and metric_version == 1:\n",
    "        return MetricDeterminism.NEURAL\n",
    "    elif metric_name == 'perplexity' and metric_version == 1:\n",
    "        return MetricDeterminism.DETERMINISTIC\n",
    "    elif metric_name == 'chatbot classifier' and metric_version == 1:\n",
    "        return MetricDeterminism.PROBABILISTIC\n",
    "    elif metric_name == 'distinct' and metric_version == 1:\n",
    "        return MetricDeterminism.DETERMINISTIC\n",
    "    elif metric_name == 'emotion classifier' and metric_version == 1:\n",
    "        return MetricDeterminism.NEURAL\n",
    "    elif metric_name == 'dummy metric':\n",
    "        return MetricDeterminism.DETERMINISTIC\n",
    "    else:\n",
    "        logging.error(\"Unknown determinism for metric \" + metric_name)\n",
    "        \n",
    "def get_metric_dependency(metric_name, metric_actors):\n",
    "    actors_order = ['training_set', 'predictor', 'reference', 'document', 'document0', 'document1'] \n",
    "    actor_types = [metric_actors[key][0] for key in actors_order if key in metric_actors]\n",
    "    actor_chars = [metric_actors[key][1] for key in actors_order if key in metric_actors]\n",
    "    if metric_name == 'lines count' or metric_name == 'distinct' or metric_name == 'emotion classifier':\n",
    "        if all(at.value < 10 for at in actor_types):\n",
    "            return MetricDependency.DATASET\n",
    "        elif actor_chars[0] == 'Base':\n",
    "            return MetricDependency.DATASET\n",
    "        else:\n",
    "            return MetricDependency.COHERENT\n",
    "    elif metric_name == 'bleu' or metric_name == 'rouge l' or \\\n",
    "         metric_name == 'semantic answer similarity' or metric_name == 'chatbot classifier' or metric_name == 'perplexity':\n",
    "        if all(at.value < 10 for at in actor_types):\n",
    "            return MetricDependency.DATASET\n",
    "        elif actor_types[0].value < 10 and actor_chars[1] == 'Base':\n",
    "            return MetricDependency.DATASET\n",
    "        elif actor_chars[0] == actor_chars[1]:\n",
    "            return MetricDependency.COHERENT\n",
    "        elif all(at.value >= 10 for at in actor_types):\n",
    "            return MetricDependency.COMPARATIVE\n",
    "        else:\n",
    "            return MetricDependency.ADVERSARIAL\n",
    "    elif metric_name == 'dummy metric':\n",
    "        return MetricDependency.DATASET\n",
    "    else:\n",
    "        logging.error(\"Unknown dependency for metric \" + metric_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': {'score': 0.9984034299850464, 'std': 0.027748608961701393},\n",
      " 'context': {'dialogpt_context_sentences': 5, 'dialogpt_nbeams_beams': 3, 'dialogpt_sample_top_k': 50, 'dialogpt_sample_top_p': 0.92, 'dialogpt_size': 'small'},\n",
      " 'hash': '220b59ba420e4af16b9217f05635815d',\n",
      " 'metric_actors': {'document': [<MetricActor.DATASET_CHAR: 1>, 'Barney'], 'training_set': [<MetricActor.DATASET_CHAR: 1>, 'Barney']},\n",
      " 'metric_arity': <MetricArity.PAIRWISE: 2>,\n",
      " 'metric_attempt': 0,\n",
      " 'metric_dependency': <MetricDependency.DATASET: 0>,\n",
      " 'metric_determinism': <MetricDeterminism.DETERMINISTIC: 0>,\n",
      " 'metric_name': 'dummy metric',\n",
      " 'metric_params': {},\n",
      " 'metric_samples': 'Unknown',\n",
      " 'metric_version': 1}\n"
     ]
    }
   ],
   "source": [
    "# Metric metadata creation\n",
    "metric_name = 'dummy metric'\n",
    "metric_name_pretty = 'Dummy Metric'\n",
    "metric_version = 1\n",
    "metric_actors = {\n",
    "    \"document\": [\n",
    "        MetricActor.DATASET_CHAR,\n",
    "        \"Barney\"\n",
    "    ],\n",
    "    \"training_set\": [\n",
    "        MetricActor.DATASET_CHAR,\n",
    "        \"Barney\"\n",
    "    ]\n",
    "}\n",
    "metric_result = {\n",
    "    \"score\": 0.9984034299850464,\n",
    "    \"std\": 0.027748608961701393\n",
    "}\n",
    "metric_attempt = 0\n",
    "metric_context = {                          \n",
    "    \"dialogpt_size\": \"small\",\n",
    "    \"dialogpt_context_sentences\": 5,\n",
    "    \"dialogpt_nbeams_beams\": 3,\n",
    "    \"dialogpt_sample_top_p\": 0.92,\n",
    "    \"dialogpt_sample_top_k\": 50\n",
    "}\n",
    "metric_params = {}\n",
    "metric_samples = 'Unknown'\n",
    "metric_hash = dict_hash({'metric_name': metric_name,\n",
    "                         'metric_version': metric_version,\n",
    "                         'metric_attempt': metric_attempt,\n",
    "                         'metric_actors': metric_actors,\n",
    "                         'context': metric_context,\n",
    "                         'metric_params': metric_params,\n",
    "                         'metric_samples': metric_samples})\n",
    "\n",
    "# This is the important one. Each metric should contain all these entries\n",
    "metric_dict = {\n",
    "        \"metric_name\": metric_name,           # Unique name of the metric\n",
    "        \"metric_version\": metric_version,     # Metric version (useful if you change how a metric works and recompute)\n",
    "        \"metric_attempt\": metric_attempt,     # Incremental value for multiple computations of the same metric (e.g. for std)\n",
    "        \"metric_actors\": metric_actors,       # Who this metric is computed on\n",
    "        \"metric_dependency\": get_metric_dependency(metric_name, metric_actors), # Is this metric a function of data or chatbot?\n",
    "        \"metric_params\": metric_params,       # Additional params of the metric (e.g. ngram_size for distinct)\n",
    "        \"context\": metric_context,            # External parameters, such as chatbot characteristics\n",
    "        \"metric_arity\": get_metric_arity(metric_name), # Metric arity\n",
    "        \"metric_samples\": metric_samples,     # Batch size of the metric\n",
    "        \"metric_determinism\": get_metric_determinism(metric_name, metric_version), # Is this metric algorithmic or not?\n",
    "        \"answer\": metric_result,              # Score of the metric, may include std (Any dictionary can go here)\n",
    "        \"hash\": metric_hash                   # Unique hash for this metric, used to not store duplicates\n",
    "    }\n",
    "\n",
    "metric_dict = {\n",
    "    metric_hash: metri\n",
    "}\n",
    "printer.pprint(metric_dict) # Metric is now ready to be saved!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metric_by_name(metric_name_pretty, metric_dict):\n",
    "    # Multiple metrics are stored as a dictionary, indexed by hash. Load the existing ones\n",
    "    if os.path.exists(os.path.join(in_metrics_folder, metric_name_pretty)):\n",
    "        metrics = load_from_json(in_metrics_folder, metric_name_pretty)\n",
    "    else:\n",
    "        metrics = dict()\n",
    "    # Add new entry\n",
    "    metrics.update(metric_dict)\n",
    "    # Save metrics file\n",
    "    save_as_json(in_metrics_folder, metric_name_pretty, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_metric_by_name(\"Dummy Metric\", metric_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metric_by_name(metric_name_pretty):\n",
    "    metrics = load_from_json(in_metrics_folder, metric_name_pretty)\n",
    "    for entry in metrics.values():\n",
    "        for actor in entry['metric_actors'].values():\n",
    "            actor[0] = MetricActor(actor[0])\n",
    "        entry['metric_dependency'] = MetricDependency(entry['metric_dependency'])\n",
    "        entry['metric_determinism'] = MetricDependency(entry['metric_determinism'])\n",
    "        entry['metric_arity'] = MetricArity(entry['metric_arity'])\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'220b59ba420e4af16b9217f05635815d': {'metric_name': 'dummy metric', 'metric_version': 1, 'metric_attempt': 0, 'metric_actors': {'document': [<MetricActor.DATASET_CHAR: 1>, 'Barney'], 'training_set': [<MetricActor.DATASET_CHAR: 1>, 'Barney']}, 'metric_dependency': <MetricDependency.DATASET: 0>, 'metric_params': {}, 'context': {'dialogpt_size': 'small', 'dialogpt_context_sentences': 5, 'dialogpt_nbeams_beams': 3, 'dialogpt_sample_top_p': 0.92, 'dialogpt_sample_top_k': 50}, 'metric_arity': <MetricArity.PAIRWISE: 2>, 'metric_samples': 'Unknown', 'metric_determinism': <MetricDependency.DATASET: 0>, 'answer': {'score': 0.9984034299850464, 'std': 0.027748608961701393}, 'hash': '220b59ba420e4af16b9217f05635815d'}}\n"
     ]
    }
   ],
   "source": [
    "metrics = load_metric_by_name(\"Dummy Metric\")\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

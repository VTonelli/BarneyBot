{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data.data_dicts import character_dict, source_dict, random_state\n",
    "\n",
    "model_name = 'microsoft/DialoGPT-small'\n",
    "character = 'Barney' # 'Barney' | 'Sheldon' | 'Harry' | 'Fry' | 'Vader' | 'Joey' | 'Phoebe' | 'Bender' | Default'\n",
    "character_2 = 'Sheldon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount google drive\n",
    "import os\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive',force_remount=True)\n",
    "    base_folder = '/content/drive/My Drive/unibo/NLP_project/BarneyBot'\n",
    "    os.system(\"pip install datasets\")\n",
    "    os.system(\"pip install transformers\")\n",
    "    os.system(\"pip install rouge_score\")\n",
    "    os.system(\"pip install -U sentence-transformers\")\n",
    "else:\n",
    "    base_folder = os.getcwd()\n",
    "    \n",
    "in_folder = os.path.join(base_folder, 'Data', 'Characters', character)\n",
    "if not os.path.exists(in_folder):\n",
    "    os.makedirs(in_folder)\n",
    "out_folder = os.path.join(base_folder, 'Data', 'Characters', character)\n",
    "if not os.path.exists(out_folder):\n",
    "    os.makedirs(out_folder)\n",
    "    \n",
    "in_folder_2 = os.path.join(base_folder, 'Data', 'Characters', character_2)\n",
    "if not os.path.exists(in_folder_2):\n",
    "    os.makedirs(in_folder_2)\n",
    "out_folder_2 = os.path.join(base_folder, 'Data', 'Characters', character_2)\n",
    "if not os.path.exists(out_folder_2):\n",
    "    os.makedirs(out_folder_2)\n",
    "    \n",
    "in_folder_def = os.path.join(base_folder, 'Data', 'Characters', 'Default')\n",
    "if not os.path.exists(in_folder_def):\n",
    "    os.makedirs(in_folder_def)\n",
    "out_folder_def = os.path.join(base_folder, 'Data', 'Characters', 'Default')\n",
    "if not os.path.exists(out_folder_def):\n",
    "    os.makedirs(out_folder_def)\n",
    "    \n",
    "metrics_folder = os.path.join(base_folder, 'Metrics')\n",
    "if not os.path.exists(metrics_folder):\n",
    "    os.makedirs(metrics_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_json(filepath, filename, data):\n",
    "    if not os.path.exists(filepath):\n",
    "        os.makedirs(filepath, exist_ok=True)\n",
    "    with open(os.path.join(filepath, filename + \".json\"), 'w') as f:\n",
    "        f.write(json.dumps(data, indent=4))\n",
    "\n",
    "def load_from_json(filepath, filename):\n",
    "    if not os.path.exists(os.path.join(filepath, filename + '.json')):\n",
    "        return dict()\n",
    "    with open(os.path.join(filepath, filename + '.json'), 'r') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "def load_df(character):\n",
    "    dataset_path = os.path.join(base_folder, \"Data\", \"Characters\", character, character+'.csv')\n",
    "    \n",
    "    character_hg = load_dataset('csv', \n",
    "                                data_files=dataset_path, \n",
    "                                cache_dir=os.path.join(base_folder, \"cache\"))\n",
    "    \n",
    "    # 85% train / 10% test / 5% validation\n",
    "    train_test_hg = character_hg['train'].train_test_split(test_size=0.15, seed=random_state)\n",
    "    test_val = train_test_hg['test'].train_test_split(test_size=0.33, seed=random_state)\n",
    "    \n",
    "    \n",
    "    character_hg = DatasetDict({\n",
    "        'train': train_test_hg['train'],\n",
    "        'test': test_val['train'],\n",
    "        'val': test_val['test']\n",
    "    })\n",
    "    \n",
    "    return character_hg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_conv(row, tokenizer):\n",
    "    MAX_LENGTH = 512\n",
    "    row = list(reversed(list(row.values())))\n",
    "    model_inputs = tokenizer(row)\n",
    "    tokenizer_pad_token_id = tokenizer.encode('#')[0]\n",
    "    for i in range(len(model_inputs['input_ids'])):\n",
    "        model_inputs['input_ids'][i].append(tokenizer.eos_token_id)\n",
    "        model_inputs['attention_mask'][i].append(1)\n",
    "    model_inputs['input_ids'] = [item for sublist in model_inputs['input_ids'] for item in sublist]\n",
    "    model_inputs['attention_mask'] = [item for sublist in model_inputs['attention_mask'] for item in sublist]\n",
    "    if MAX_LENGTH > len(model_inputs['input_ids']):\n",
    "        model_inputs['input_ids'] += [tokenizer_pad_token_id] * (MAX_LENGTH - len(model_inputs['input_ids']))\n",
    "        model_inputs['attention_mask'] += [0] * (MAX_LENGTH - len(model_inputs['attention_mask']))\n",
    "    elif MAX_LENGTH < len(model_inputs['input_ids']):\n",
    "        model_inputs['input_ids'] = model_inputs['input_ids'][:MAX_LENGTH-1]\n",
    "        model_inputs['input_ids'][-1] = tokenizer.eos_token_id\n",
    "        model_inputs['attention_mask'] = model_inputs['attention_mask'][:MAX_LENGTH-1]\n",
    "        model_inputs['attention_mask'][-1] = 1\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    tokenizer.pad_token = '#'\n",
    "    model_inputs = construct_conv(examples, tokenizer)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-bf922d7ec0ca33b8\n",
      "Reusing dataset csv (C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\cache\\csv\\default-bf922d7ec0ca33b8\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94d08461243e4c2bb0765c655cab5dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\cache\\csv\\default-bf922d7ec0ca33b8\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-fc24b00ef0a2bb87.arrow and C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\cache\\csv\\default-bf922d7ec0ca33b8\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-762e65cc2da69210.arrow\n",
      "Loading cached split indices for dataset at C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\cache\\csv\\default-bf922d7ec0ca33b8\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-9584f7c57efdb89c.arrow and C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\cache\\csv\\default-bf922d7ec0ca33b8\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-ba54ddf4f2459593.arrow\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"HF_DATASETS_CACHE\"] = os.path.join(base_folder, \"cache\")\n",
    "character_hg = load_df(character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_folder = os.path.join(out_folder, character_dict[character]['checkpoint_folder'])\n",
    "checkpoint_folder_2 = os.path.join(out_folder_2, character_dict[character_2]['checkpoint_folder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\Data\\Characters\\Barney\\barney_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\Data\\Characters\\Sheldon\\sheldon_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at microsoft/DialoGPT-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=os.path.join(base_folder, \"cache\"))\n",
    "tokenizer.pad_token = '#'\n",
    "\n",
    "model = TFAutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=checkpoint_folder)\n",
    "model_2 = TFAutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=checkpoint_folder_2)\n",
    "model_def = TFAutoModelForCausalLM.from_pretrained(model_name, cache_dir=os.path.join(base_folder, \"cache\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_questions = character_hg['test']['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_beams = 3\n",
    "top_k = 50\n",
    "top_p = 0.92\n",
    "\n",
    "def get_predictions_cached(sample_questions, model, filename, generation_method, override_predictions=False):\n",
    "    prediction_path = os.path.join(in_folder, filename)\n",
    "    if os.path.exists(prediction_path) and not override_predictions:\n",
    "        print(\"Loading predictions from stored file\")\n",
    "        with open(prediction_path, 'r') as file:\n",
    "            json_string = file.read()\n",
    "        predictions = json.loads(json_string)\n",
    "        print(\"Loaded predictions from stored file\")\n",
    "\n",
    "    else:\n",
    "        print(\"Creating predictions\")\n",
    "        predictions = list()\n",
    "        for x in tqdm(sample_questions):\n",
    "            tokenized_question = tokenizer.encode(x + tokenizer.eos_token, return_tensors='tf')\n",
    "            max_length = 128 + tokenized_question.shape[1]\n",
    "            if generation_method == \"Greedy\":\n",
    "                generated_answer = model.generate(tokenized_question,\n",
    "                                    pad_token_id=tokenizer.eos_token_id, max_length=max_length)[0].numpy().tolist()\n",
    "            elif generation_method == \"Beam Search\":\n",
    "                generated_answer = model.generate(tokenized_question,\n",
    "                                             pad_token_id=tokenizer.eos_token_id, max_length=max_length,\n",
    "                                             n_beams=n_beams)[0].numpy().tolist()\n",
    "            elif generation_method == \"Sampling\":\n",
    "                generated_answer = model.generate(tokenized_question,\n",
    "                                             pad_token_id=tokenizer.eos_token_id, max_length=max_length,\n",
    "                                             do_sample=True, top_k=top_k, top_p=top_p)[0].numpy().tolist()\n",
    "            predictions.append(generated_answer[len(tokenized_question[0]):])\n",
    "\n",
    "        # Save predictions as a JSON file\n",
    "        output_string = json.dumps(predictions)\n",
    "        with open(prediction_path, 'w') as file:\n",
    "            file.write(output_string)\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading predictions from stored file\n",
      "Loaded predictions from stored file\n",
      "Loading predictions from stored file\n",
      "Loaded predictions from stored file\n",
      "Loading predictions from stored file\n",
      "Loaded predictions from stored file\n"
     ]
    }
   ],
   "source": [
    "predictions_greedy = get_predictions_cached(sample_questions, model,\n",
    "                                            character_dict[character]['prediction_filename'] + '_greedy.json',\n",
    "                                            \"Greedy\")\n",
    "predictions_nbeams = get_predictions_cached(sample_questions, model,\n",
    "                                            character_dict[character]['prediction_filename'] + '_nbeams.json',\n",
    "                                            \"Beam Search\")\n",
    "predictions_sampling = get_predictions_cached(sample_questions, model,\n",
    "                                              character_dict[character]['prediction_filename'] + '_sampling.json',\n",
    "                                              \"Sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe_for_metrics(data_test, predictions_greedy, predictions_nbeams, predictions_sampling):\n",
    "    i = 0\n",
    "    df = {'ctx':[], 'ctx_tk':[]}\n",
    "    has_labels = 'response' in data_test.features\n",
    "    if has_labels:\n",
    "        df['lbl'] = []\n",
    "        df['lbl_tk'] = []\n",
    "    if predictions_greedy:\n",
    "        df['prd_greedy'] = []\n",
    "        df['prd_greedy_tk'] = []\n",
    "    if predictions_nbeams:\n",
    "        df['prd_nbeams'] = []\n",
    "        df['prd_nbeams_tk'] = [] \n",
    "    if predictions_sampling:\n",
    "        df['prd_sampling'] = []\n",
    "        df['prd_sampling_tk'] = []\n",
    "    for sample in tqdm(data_test):\n",
    "        # encode the context and label sentences, add the eos_token and return a tensor\n",
    "        ctx_tk = tokenizer.encode(sample['context'] + tokenizer.eos_token, return_tensors='tf').numpy().tolist()\n",
    "        ctx = sample['context']\n",
    "        df['ctx_tk'].append(ctx_tk)\n",
    "        df['ctx'].append(ctx)\n",
    "        if has_labels:\n",
    "            lbl_tk = tokenizer.encode(sample['response'] + tokenizer.eos_token, return_tensors='tf').numpy().tolist()\n",
    "            lbl = sample['response']\n",
    "            df['lbl'].append(lbl)\n",
    "            df['lbl_tk'].append(lbl_tk)\n",
    "        if predictions_greedy:\n",
    "            prd_greedy_tk = predictions_greedy[i]\n",
    "            prd_greedy = tokenizer.decode(prd_greedy_tk, skip_special_tokens=True)\n",
    "            df['prd_greedy'].append(prd_greedy)\n",
    "            df['prd_greedy_tk'].append(prd_greedy_tk)\n",
    "        if predictions_nbeams:\n",
    "            prd_nbeams_tk = predictions_nbeams[i]\n",
    "            prd_nbeams = tokenizer.decode(prd_nbeams_tk, skip_special_tokens=True)\n",
    "            df['prd_nbeams'].append(prd_nbeams)\n",
    "            df['prd_nbeams_tk'].append(prd_nbeams_tk)\n",
    "        if predictions_sampling:\n",
    "            prd_sampling_tk = predictions_sampling[i]\n",
    "            prd_sampling = tokenizer.decode(prd_sampling_tk, skip_special_tokens=True)\n",
    "            df['prd_sampling'].append(prd_sampling)\n",
    "            df['prd_sampling_tk'].append(prd_sampling_tk)\n",
    "        i += 1\n",
    "    return pd.DataFrame(data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 522/522 [00:00<00:00, 1614.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ctx</th>\n",
       "      <th>ctx_tk</th>\n",
       "      <th>lbl</th>\n",
       "      <th>lbl_tk</th>\n",
       "      <th>prd_greedy</th>\n",
       "      <th>prd_greedy_tk</th>\n",
       "      <th>prd_nbeams</th>\n",
       "      <th>prd_nbeams_tk</th>\n",
       "      <th>prd_sampling</th>\n",
       "      <th>prd_sampling_tk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I know, it's two years of my life I'm never ge...</td>\n",
       "      <td>[[40, 760, 11, 340, 338, 734, 812, 286, 616, 1...</td>\n",
       "      <td>Daddy's home.</td>\n",
       "      <td>[[48280, 338, 1363, 13, 50256]]</td>\n",
       "      <td>Oh, God!</td>\n",
       "      <td>[5812, 11, 1793, 0, 50256]</td>\n",
       "      <td>Oh, God!</td>\n",
       "      <td>[5812, 11, 1793, 0, 50256]</td>\n",
       "      <td>Oh, of course.</td>\n",
       "      <td>[5812, 11, 286, 1781, 13, 50256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wh-Where'd you get a meatball...</td>\n",
       "      <td>[[1199, 12, 8496, 1549, 345, 651, 257, 6174, 1...</td>\n",
       "      <td>I don't have much time!</td>\n",
       "      <td>[[40, 836, 470, 423, 881, 640, 0, 50256]]</td>\n",
       "      <td>I don't know. I just saw a meatball sub.</td>\n",
       "      <td>[40, 836, 470, 760, 13, 314, 655, 2497, 257, 6...</td>\n",
       "      <td>I don't know. I just saw a meatball sub.</td>\n",
       "      <td>[40, 836, 470, 760, 13, 314, 655, 2497, 257, 6...</td>\n",
       "      <td>You don't remember?</td>\n",
       "      <td>[1639, 836, 470, 3505, 30, 50256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Okay, what is so urgent that you called me and...</td>\n",
       "      <td>[[16454, 11, 644, 318, 523, 18039, 326, 345, 1...</td>\n",
       "      <td>I could tell you knew something was up with me...</td>\n",
       "      <td>[[40, 714, 1560, 345, 2993, 1223, 373, 510, 35...</td>\n",
       "      <td>I'm sorry, I don't follow you.</td>\n",
       "      <td>[40, 1101, 7926, 11, 314, 836, 470, 1061, 345,...</td>\n",
       "      <td>I'm sorry, I don't follow you.</td>\n",
       "      <td>[40, 1101, 7926, 11, 314, 836, 470, 1061, 345,...</td>\n",
       "      <td>All right. It's time to start?</td>\n",
       "      <td>[3237, 826, 13, 632, 338, 640, 284, 923, 30, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How much?</td>\n",
       "      <td>[[2437, 881, 30, 50256]]</td>\n",
       "      <td>A little.</td>\n",
       "      <td>[[32, 1310, 13, 50256]]</td>\n",
       "      <td>I have not decided. I want to get married in a...</td>\n",
       "      <td>[40, 423, 407, 3066, 13, 314, 765, 284, 651, 6...</td>\n",
       "      <td>I have not decided. I want to get married in a...</td>\n",
       "      <td>[40, 423, 407, 3066, 13, 314, 765, 284, 651, 6...</td>\n",
       "      <td>He said he had like 3,000 in suits.</td>\n",
       "      <td>[1544, 531, 339, 550, 588, 513, 11, 830, 287, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You're being super nice. It's... freaking me o...</td>\n",
       "      <td>[[1639, 821, 852, 2208, 3621, 13, 632, 338, 98...</td>\n",
       "      <td>I'm being Barney, and I think tonight's going ...</td>\n",
       "      <td>[[40, 1101, 852, 41921, 11, 290, 314, 892, 997...</td>\n",
       "      <td>I'm not gross. I'm just... gross.</td>\n",
       "      <td>[40, 1101, 407, 10319, 13, 314, 1101, 655, 986...</td>\n",
       "      <td>I'm not gross. I'm just... gross.</td>\n",
       "      <td>[40, 1101, 407, 10319, 13, 314, 1101, 655, 986...</td>\n",
       "      <td>Buckminster Fuller?</td>\n",
       "      <td>[33, 1347, 18462, 31863, 30, 50256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>Okay, I want to lay down some ground rules for...</td>\n",
       "      <td>[[16454, 11, 314, 765, 284, 3830, 866, 617, 23...</td>\n",
       "      <td>Well, well, well. How rich. You make me promis...</td>\n",
       "      <td>[[5779, 11, 880, 11, 880, 13, 1374, 5527, 13, ...</td>\n",
       "      <td>Oh, I know. I just want to be as awesome as sh...</td>\n",
       "      <td>[5812, 11, 314, 760, 13, 314, 655, 765, 284, 3...</td>\n",
       "      <td>Oh, I know. I just want to be as awesome as sh...</td>\n",
       "      <td>[5812, 11, 314, 760, 13, 314, 655, 765, 284, 3...</td>\n",
       "      <td>No, I'll be right over.</td>\n",
       "      <td>[2949, 11, 314, 1183, 307, 826, 625, 13, 50256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>It looks to be a... sacred... spa.</td>\n",
       "      <td>[[1026, 3073, 284, 307, 257, 986, 13626, 986, ...</td>\n",
       "      <td>Owl. How do we go? We will do what? Jump?</td>\n",
       "      <td>[[46, 40989, 13, 1374, 466, 356, 467, 30, 775,...</td>\n",
       "      <td>I'm sorry, I don't follow you.</td>\n",
       "      <td>[40, 1101, 7926, 11, 314, 836, 470, 1061, 345,...</td>\n",
       "      <td>I'm sorry, I don't follow you.</td>\n",
       "      <td>[40, 1101, 7926, 11, 314, 836, 470, 1061, 345,...</td>\n",
       "      <td>It's called a spa.</td>\n",
       "      <td>[1026, 338, 1444, 257, 41900, 13, 50256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>That's putting it a bit strongly.</td>\n",
       "      <td>[[2504, 338, 5137, 340, 257, 1643, 7634, 13, 5...</td>\n",
       "      <td>A bit strongly. She's not my girlfriend.</td>\n",
       "      <td>[[32, 1643, 7634, 13, 1375, 338, 407, 616, 110...</td>\n",
       "      <td>I'm not going to put it in a little strong.</td>\n",
       "      <td>[40, 1101, 407, 1016, 284, 1234, 340, 287, 257...</td>\n",
       "      <td>I'm not going to put it in a little strong.</td>\n",
       "      <td>[40, 1101, 407, 1016, 284, 1234, 340, 287, 257...</td>\n",
       "      <td>And here comes the fun part.</td>\n",
       "      <td>[1870, 994, 2058, 262, 1257, 636, 13, 50256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>I do.</td>\n",
       "      <td>[[40, 466, 13, 50256]]</td>\n",
       "      <td>I'm gonna head out to a reggae concert. I'm a ...</td>\n",
       "      <td>[[40, 1101, 8066, 1182, 503, 284, 257, 842, 25...</td>\n",
       "      <td>You're a good man.</td>\n",
       "      <td>[1639, 821, 257, 922, 582, 13, 50256]</td>\n",
       "      <td>You're a good man.</td>\n",
       "      <td>[1639, 821, 257, 922, 582, 13, 50256]</td>\n",
       "      <td>Hey.</td>\n",
       "      <td>[10814, 13, 50256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>It's great. Drinking at work.</td>\n",
       "      <td>[[1026, 338, 1049, 13, 43963, 379, 670, 13, 50...</td>\n",
       "      <td>Basically, it is of Mad Men.</td>\n",
       "      <td>[[31524, 11, 340, 318, 286, 4627, 6065, 13, 50...</td>\n",
       "      <td>I'm not drinking at work.</td>\n",
       "      <td>[40, 1101, 407, 7722, 379, 670, 13, 50256]</td>\n",
       "      <td>I'm not drinking at work.</td>\n",
       "      <td>[40, 1101, 407, 7722, 379, 670, 13, 50256]</td>\n",
       "      <td>Barney, I didn't mean any personal attacks!</td>\n",
       "      <td>[10374, 1681, 11, 314, 1422, 470, 1612, 597, 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>522 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   ctx  \\\n",
       "0    I know, it's two years of my life I'm never ge...   \n",
       "1                     Wh-Where'd you get a meatball...   \n",
       "2    Okay, what is so urgent that you called me and...   \n",
       "3                                            How much?   \n",
       "4    You're being super nice. It's... freaking me o...   \n",
       "..                                                 ...   \n",
       "517  Okay, I want to lay down some ground rules for...   \n",
       "518                 It looks to be a... sacred... spa.   \n",
       "519                  That's putting it a bit strongly.   \n",
       "520                                              I do.   \n",
       "521                      It's great. Drinking at work.   \n",
       "\n",
       "                                                ctx_tk  \\\n",
       "0    [[40, 760, 11, 340, 338, 734, 812, 286, 616, 1...   \n",
       "1    [[1199, 12, 8496, 1549, 345, 651, 257, 6174, 1...   \n",
       "2    [[16454, 11, 644, 318, 523, 18039, 326, 345, 1...   \n",
       "3                             [[2437, 881, 30, 50256]]   \n",
       "4    [[1639, 821, 852, 2208, 3621, 13, 632, 338, 98...   \n",
       "..                                                 ...   \n",
       "517  [[16454, 11, 314, 765, 284, 3830, 866, 617, 23...   \n",
       "518  [[1026, 3073, 284, 307, 257, 986, 13626, 986, ...   \n",
       "519  [[2504, 338, 5137, 340, 257, 1643, 7634, 13, 5...   \n",
       "520                             [[40, 466, 13, 50256]]   \n",
       "521  [[1026, 338, 1049, 13, 43963, 379, 670, 13, 50...   \n",
       "\n",
       "                                                   lbl  \\\n",
       "0                                        Daddy's home.   \n",
       "1                              I don't have much time!   \n",
       "2    I could tell you knew something was up with me...   \n",
       "3                                            A little.   \n",
       "4    I'm being Barney, and I think tonight's going ...   \n",
       "..                                                 ...   \n",
       "517  Well, well, well. How rich. You make me promis...   \n",
       "518          Owl. How do we go? We will do what? Jump?   \n",
       "519           A bit strongly. She's not my girlfriend.   \n",
       "520  I'm gonna head out to a reggae concert. I'm a ...   \n",
       "521                       Basically, it is of Mad Men.   \n",
       "\n",
       "                                                lbl_tk  \\\n",
       "0                      [[48280, 338, 1363, 13, 50256]]   \n",
       "1            [[40, 836, 470, 423, 881, 640, 0, 50256]]   \n",
       "2    [[40, 714, 1560, 345, 2993, 1223, 373, 510, 35...   \n",
       "3                              [[32, 1310, 13, 50256]]   \n",
       "4    [[40, 1101, 852, 41921, 11, 290, 314, 892, 997...   \n",
       "..                                                 ...   \n",
       "517  [[5779, 11, 880, 11, 880, 13, 1374, 5527, 13, ...   \n",
       "518  [[46, 40989, 13, 1374, 466, 356, 467, 30, 775,...   \n",
       "519  [[32, 1643, 7634, 13, 1375, 338, 407, 616, 110...   \n",
       "520  [[40, 1101, 8066, 1182, 503, 284, 257, 842, 25...   \n",
       "521  [[31524, 11, 340, 318, 286, 4627, 6065, 13, 50...   \n",
       "\n",
       "                                            prd_greedy  \\\n",
       "0                                             Oh, God!   \n",
       "1             I don't know. I just saw a meatball sub.   \n",
       "2                       I'm sorry, I don't follow you.   \n",
       "3    I have not decided. I want to get married in a...   \n",
       "4                    I'm not gross. I'm just... gross.   \n",
       "..                                                 ...   \n",
       "517  Oh, I know. I just want to be as awesome as sh...   \n",
       "518                     I'm sorry, I don't follow you.   \n",
       "519        I'm not going to put it in a little strong.   \n",
       "520                                 You're a good man.   \n",
       "521                          I'm not drinking at work.   \n",
       "\n",
       "                                         prd_greedy_tk  \\\n",
       "0                           [5812, 11, 1793, 0, 50256]   \n",
       "1    [40, 836, 470, 760, 13, 314, 655, 2497, 257, 6...   \n",
       "2    [40, 1101, 7926, 11, 314, 836, 470, 1061, 345,...   \n",
       "3    [40, 423, 407, 3066, 13, 314, 765, 284, 651, 6...   \n",
       "4    [40, 1101, 407, 10319, 13, 314, 1101, 655, 986...   \n",
       "..                                                 ...   \n",
       "517  [5812, 11, 314, 760, 13, 314, 655, 765, 284, 3...   \n",
       "518  [40, 1101, 7926, 11, 314, 836, 470, 1061, 345,...   \n",
       "519  [40, 1101, 407, 1016, 284, 1234, 340, 287, 257...   \n",
       "520              [1639, 821, 257, 922, 582, 13, 50256]   \n",
       "521         [40, 1101, 407, 7722, 379, 670, 13, 50256]   \n",
       "\n",
       "                                            prd_nbeams  \\\n",
       "0                                             Oh, God!   \n",
       "1             I don't know. I just saw a meatball sub.   \n",
       "2                       I'm sorry, I don't follow you.   \n",
       "3    I have not decided. I want to get married in a...   \n",
       "4                    I'm not gross. I'm just... gross.   \n",
       "..                                                 ...   \n",
       "517  Oh, I know. I just want to be as awesome as sh...   \n",
       "518                     I'm sorry, I don't follow you.   \n",
       "519        I'm not going to put it in a little strong.   \n",
       "520                                 You're a good man.   \n",
       "521                          I'm not drinking at work.   \n",
       "\n",
       "                                         prd_nbeams_tk  \\\n",
       "0                           [5812, 11, 1793, 0, 50256]   \n",
       "1    [40, 836, 470, 760, 13, 314, 655, 2497, 257, 6...   \n",
       "2    [40, 1101, 7926, 11, 314, 836, 470, 1061, 345,...   \n",
       "3    [40, 423, 407, 3066, 13, 314, 765, 284, 651, 6...   \n",
       "4    [40, 1101, 407, 10319, 13, 314, 1101, 655, 986...   \n",
       "..                                                 ...   \n",
       "517  [5812, 11, 314, 760, 13, 314, 655, 765, 284, 3...   \n",
       "518  [40, 1101, 7926, 11, 314, 836, 470, 1061, 345,...   \n",
       "519  [40, 1101, 407, 1016, 284, 1234, 340, 287, 257...   \n",
       "520              [1639, 821, 257, 922, 582, 13, 50256]   \n",
       "521         [40, 1101, 407, 7722, 379, 670, 13, 50256]   \n",
       "\n",
       "                                    prd_sampling  \\\n",
       "0                                 Oh, of course.   \n",
       "1                            You don't remember?   \n",
       "2                 All right. It's time to start?   \n",
       "3            He said he had like 3,000 in suits.   \n",
       "4                            Buckminster Fuller?   \n",
       "..                                           ...   \n",
       "517                      No, I'll be right over.   \n",
       "518                           It's called a spa.   \n",
       "519                 And here comes the fun part.   \n",
       "520                                         Hey.   \n",
       "521  Barney, I didn't mean any personal attacks!   \n",
       "\n",
       "                                       prd_sampling_tk  \n",
       "0                     [5812, 11, 286, 1781, 13, 50256]  \n",
       "1                    [1639, 836, 470, 3505, 30, 50256]  \n",
       "2    [3237, 826, 13, 632, 338, 640, 284, 923, 30, 5...  \n",
       "3    [1544, 531, 339, 550, 588, 513, 11, 830, 287, ...  \n",
       "4                  [33, 1347, 18462, 31863, 30, 50256]  \n",
       "..                                                 ...  \n",
       "517    [2949, 11, 314, 1183, 307, 826, 625, 13, 50256]  \n",
       "518           [1026, 338, 1444, 257, 41900, 13, 50256]  \n",
       "519       [1870, 994, 2058, 262, 1257, 636, 13, 50256]  \n",
       "520                                 [10814, 13, 50256]  \n",
       "521  [10374, 1681, 11, 314, 1422, 470, 1612, 597, 2...  \n",
       "\n",
       "[522 rows x 10 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_char = get_dataframe_for_metrics(character_hg['test'], predictions_greedy, predictions_nbeams, predictions_sampling)\n",
    "df_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics For Character 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ccl_sim(ctx_lbl, ctx_cht, lbl_cht):\n",
    "    return ((1 - abs(ctx_lbl - ctx_cht))**2 + lbl_cht**2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Lib.BBMetrics import BBMetric\n",
    "\n",
    "def compute_sample_metrics(context_sentence, label_response, chatbot_response, verbose=True,\n",
    "                           label_chatbot_symmetry=False):\n",
    "    scores = {}\n",
    "    lbl_text = 'label' if not label_chatbot_symmetry else 'chatbota'\n",
    "    cht_text = 'chatbot' if not label_chatbot_symmetry else 'chatbotb'\n",
    "    \n",
    "    scores['metadata'] = {}\n",
    "    scores['metadata']['ordering'] = ['context-'+lbl_text,\n",
    "                                      'context-'+cht_text,\n",
    "                                      cht_text+'-'+lbl_text,\n",
    "                                      'ccl']\n",
    "    \n",
    "    if verbose:\n",
    "        # prints the sample\n",
    "        print('* context:', context_sentence) \n",
    "        print('* ' + lbl_text  + ':  ', label_response)\n",
    "        print('* ' + cht_text  + ':', chatbot_response) \n",
    "    # 1) computes metrics for semantic similarity\n",
    "    metric = BBMetric.load_metric(\"semantic similarity\")\n",
    "    scores['semantic similarity'] = [metric.compute(sentences_a=context_sentence,\n",
    "                                                      sentences_b=label_response)['score']]\n",
    "    scores['semantic similarity'].append(metric.compute(sentences_a=context_sentence,\n",
    "                                                      sentences_b=chatbot_response)['score'])\n",
    "    scores['semantic similarity'].append(metric.compute(sentences_a=label_response,\n",
    "                                                      sentences_b=chatbot_response)['score'])\n",
    "    scores['semantic similarity'].append(ccl_sim(scores['semantic similarity'][0],\n",
    "                                                 scores['semantic similarity'][1],\n",
    "                                                 scores['semantic similarity'][2]))\n",
    "    if verbose:\n",
    "        print('=== SEMANTIC SIMILARITY ===')\n",
    "        print('context-'+lbl_text+' similarity:   ', scores['semantic similarity'][0])\n",
    "        print('context-'+cht_text+' similarity: ', scores['semantic similarity'][1])\n",
    "        print(cht_text+'-'+lbl_text+' similarity:   ', scores['semantic similarity'][2])\n",
    "        print('ccl-sim similarity:            ', scores['semantic similarity'][3])\n",
    "    # 2) computes metrics for bleu\n",
    "    metric = BBMetric.load_metric(\"bleu\")\n",
    "    scores['bleu'] = [metric.compute(predictions=label_response, references=context_sentence)['score']]\n",
    "    scores['bleu'].append(metric.compute(predictions=chatbot_response, references=context_sentence)['score'])\n",
    "    scores['bleu'].append(metric.compute(predictions=chatbot_response, references=label_response)['score'])\n",
    "    scores['bleu'].append(ccl_sim(scores['bleu'][0],\n",
    "                                  scores['bleu'][1],\n",
    "                                  scores['bleu'][2]))\n",
    "    if verbose:\n",
    "        print('===         BLEU         ===')\n",
    "        print('context-to-'+lbl_text+' bleu:      ', scores['bleu'][0])\n",
    "        print('context-to-'+cht_text+' bleu:    ', scores['bleu'][1])\n",
    "        print(lbl_text+'-to-'+cht_text+' bleu:      ', scores['bleu'][2])\n",
    "        print('ccl-sim bleu:            ', scores['bleu'][3])\n",
    "    # 3) computes metrics for rouge-L\n",
    "    metric = BBMetric.load_metric(\"rouge l\")\n",
    "    scores['rouge l'] = [metric.compute(predictions=label_response, references=context_sentence)['score']]\n",
    "    scores['rouge l'].append(metric.compute(predictions=chatbot_response, references=context_sentence)['score'])\n",
    "    scores['rouge l'].append(metric.compute(predictions=chatbot_response, references=label_response)['score'])\n",
    "    scores['rouge l'].append(ccl_sim(scores['rouge l'][0],\n",
    "                                     scores['rouge l'][1],\n",
    "                                     scores['rouge l'][2]))\n",
    "    if verbose:\n",
    "        print('===        ROUGE-L       ===')\n",
    "        print('context-to-'+lbl_text+' rouge:     ', scores['rouge l'][0])\n",
    "        print('context-to-'+cht_text+' rouge:   ', scores['rouge l'][1])\n",
    "        print(lbl_text+'-to-'+cht_text+' rouge:     ', scores['rouge l'][2])\n",
    "        print('ccl-sim rouge:            ', scores['rouge l'][3])\n",
    "    # 6) computes metrics for distinct\n",
    "    metric = BBMetric.load_metric(\"distinct\")\n",
    "    scores['distinct'] = [metric.compute(sentences=context_sentence)['score']]\n",
    "    scores['distinct'].append(metric.compute(sentences=label_response)['score'])\n",
    "    scores['distinct'].append(metric.compute(sentences=chatbot_response)['score'])\n",
    "    scores['distinct'].append(ccl_sim(scores['distinct'][0],\n",
    "                                      scores['distinct'][1],\n",
    "                                      scores['distinct'][2]))\n",
    "    if verbose:\n",
    "        print('===       DISTINCT      ===')\n",
    "        print('context distinct:          ', scores['distinct'][0])\n",
    "        print(lbl_text+' distinct:          ', scores['distinct'][1])\n",
    "        print(cht_text+' distinct:          ', scores['distinct'][2])\n",
    "        print('ccl-sim distinct:          ', scores['distinct'][3])\n",
    "    # 4) computes sas metric\n",
    "    metric = BBMetric.load_metric(\"semantic answer similarity\")\n",
    "    scores['semantic answer similarity'] = [metric.compute(predictions=context_sentence,\n",
    "                                                    references=label_response)['score']]\n",
    "    scores['semantic answer similarity'].append(metric.compute(predictions=context_sentence,\n",
    "                                                        references=chatbot_response)['score'])\n",
    "    scores['semantic answer similarity'].append(metric.compute(predictions=label_response,\n",
    "                                                        references=chatbot_response)['score'])\n",
    "    scores['semantic answer similarity'].append(ccl_sim(scores['semantic answer similarity'][0],\n",
    "                                                        scores['semantic answer similarity'][1],\n",
    "                                                        scores['semantic answer similarity'][2]))\n",
    "    if verbose:\n",
    "        print('===         SAS         ===')\n",
    "        print('context-'+lbl_text+' sas:          ', scores['semantic answer similarity'][0])\n",
    "        print('context-'+cht_text+' sas:        ', scores['semantic answer similarity'][1])\n",
    "        print(lbl_text+'-'+cht_text+' sas:          ', scores['semantic answer similarity'][2])\n",
    "        print('ccl-sim sas:               ', scores['semantic answer similarity'][3])\n",
    "    # 5) computes emotion metric\n",
    "    metric = BBMetric.load_metric(\"emotion\")\n",
    "    scores['emotion'] = [metric.compute(sentences=context_sentence)]\n",
    "    scores['emotion'].append(metric.compute(sentences=label_response))\n",
    "    scores['emotion'].append(metric.compute(sentences=chatbot_response))\n",
    "    scores['emotion'].append(sp.stats.stats.pearsonr(scores['emotion'][1]['score'],\n",
    "                                                     scores['emotion'][2]['score'])[0])\n",
    "    if verbose:\n",
    "        print('===       EMOTION       ===')\n",
    "        print('context emotions:            \\n', list(zip(scores['emotion'][0]['label'], scores['emotion'][0]['score'])))\n",
    "        print(lbl_text+' emotions:              \\n', list(zip(scores['emotion'][1]['label'], scores['emotion'][1]['score'])))\n",
    "        print(cht_text+' emotions:            \\n', list(zip(scores['emotion'][2]['label'], scores['emotion'][2]['score'])))\n",
    "        print(lbl_text+'-'+cht_text+'emotion corr:  \\n', scores['emotion'][3])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_set_metrics(model, tokenizer, context_sentences, label_responses, chatbot_responses, character, verbose=True,\n",
    "                        classifier_n_sentences=50, include_sentences=False, label_chatbot_symmetry=False):\n",
    "    scores = {}\n",
    "    \n",
    "    lbl_text = 'label' if not label_chatbot_symmetry else 'chatbota'\n",
    "    cht_text = 'chatbot' if not label_chatbot_symmetry else 'chatbotb'\n",
    "    \n",
    "    scores['metadata'] = {}\n",
    "    scores['metadata']['ordering'] = ['context-'+lbl_text,\n",
    "                                      'context-'+cht_text,\n",
    "                                      cht_text+'-'+lbl_text,\n",
    "                                      'ccl']\n",
    "    \n",
    "    # 0) computes metrics for perplexity\n",
    "    metric = BBMetric.load_metric(\"semantic similarity\")\n",
    "    scores['semantic similarity'] = [metric.compute(sentences_a=context_sentences,\n",
    "                                            sentences_b=label_responses)]\n",
    "    scores['semantic similarity'].append(metric.compute(sentences_a=context_sentences,\n",
    "                                            sentences_b=chatbot_responses)),\n",
    "    scores['semantic similarity'].append(metric.compute(sentences_a=label_responses,\n",
    "                                              sentences_b=chatbot_responses))\n",
    "    scores['semantic similarity'].append(ccl_sim(scores['semantic similarity'][0]['score'],\n",
    "                                                 scores['semantic similarity'][1]['score'],\n",
    "                                                 scores['semantic similarity'][2]['score']))\n",
    "    if verbose:\n",
    "        print('=== SEMANTIC SIMILARITY ===')\n",
    "        print('context-'+lbl_text+' similarity:   ', scores['semantic similarity'][0])\n",
    "        print('context-'+cht_text+' similarity: ', scores['semantic similarity'][1])\n",
    "        print(cht_text+'-'+lbl_text+' similarity:   ', scores['semantic similarity'][2])\n",
    "        print('ccl-sim similarity:            ', scores['semantic similarity'][3])\n",
    "    # 1) computes metrics for perplexity\n",
    "    if not label_chatbot_symmetry:\n",
    "        metric = BBMetric.load_metric(\"perplexity\")\n",
    "        scores['perplexity'] = metric.compute(model=model, tokenizer=tokenizer, sentences=chatbot_responses)['score_concat']\n",
    "        if verbose:\n",
    "            print('===       PERPLEXITY     ===')\n",
    "            print('chatbot perplexity:         ', scores['perplexity'])\n",
    "    elif verbose:\n",
    "        print(\"Symmetric mode, skipping Perplexity.\")\n",
    "    # 2) computes metrics for bleu\n",
    "    metric = BBMetric.load_metric(\"bleu\")\n",
    "    scores['bleu'] = [metric.compute(predictions=label_responses, references=context_sentences)]\n",
    "    scores['bleu'].append(metric.compute(predictions=chatbot_responses, references=context_sentences))\n",
    "    scores['bleu'].append(metric.compute(predictions=chatbot_responses, references=label_responses))\n",
    "    scores['bleu'].append(ccl_sim(scores['bleu'][0]['score'],\n",
    "                                  scores['bleu'][1]['score'],\n",
    "                                  scores['bleu'][2]['score']))\n",
    "    if verbose:\n",
    "        print('===         BLEU         ===')\n",
    "        print('context-to-'+lbl_text+' bleu:      ', scores['bleu'][0])\n",
    "        print('context-to-'+cht_text+' bleu:    ', scores['bleu'][1])\n",
    "        print(lbl_text+'-to-'+cht_text+' bleu:      ', scores['bleu'][2])\n",
    "        print('ccl-sim bleu:            ', scores['bleu'][3])\n",
    "    # 3) computes metrics for rouge-L\n",
    "    metric = BBMetric.load_metric(\"rouge l\")\n",
    "    scores['rouge l'] = [metric.compute(predictions=label_responses, references=context_sentences)]\n",
    "    scores['rouge l'].append(metric.compute(predictions=chatbot_responses, references=context_sentences))\n",
    "    scores['rouge l'].append(metric.compute(predictions=chatbot_responses, references=label_responses))\n",
    "    scores['rouge l'].append(ccl_sim(scores['rouge l'][0]['score'],\n",
    "                                     scores['rouge l'][1]['score'],\n",
    "                                     scores['rouge l'][2]['score']))\n",
    "    if verbose:\n",
    "        print('===        ROUGE-L       ===')\n",
    "        print('context-to-'+lbl_text+' rouge:     ', scores['rouge l'][0])\n",
    "        print('context-to-'+cht_text+' rouge:   ', scores['rouge l'][1])\n",
    "        print(lbl_text+'-to-'+cht_text+' rouge:     ', scores['rouge l'][2])\n",
    "        print('ccl-sim rouge:            ', scores['rouge l'][3])\n",
    "    # 4) computes metrics for distinct\n",
    "    metric = BBMetric.load_metric(\"distinct\")\n",
    "    scores['distinct'] = [metric.compute(sentences=context_sentences)]\n",
    "    scores['distinct'].append(metric.compute(sentences=label_responses))\n",
    "    scores['distinct'].append(metric.compute(sentences=chatbot_responses))\n",
    "    scores['distinct'].append(ccl_sim(scores['distinct'][0]['score'],\n",
    "                                      scores['distinct'][1]['score'],\n",
    "                                      scores['distinct'][2]['score']))\n",
    "    if verbose:\n",
    "        print('===       DISTINCT      ===')\n",
    "        print('context distinct:          ', scores['distinct'][0])\n",
    "        print(lbl_text+' distinct:          ', scores['distinct'][1])\n",
    "        print(cht_text+' distinct:          ', scores['distinct'][2])\n",
    "        print('ccl-sim distinct:          ', scores['distinct'][3])\n",
    "    # 6) computes emotion metric\n",
    "    metric = BBMetric.load_metric(\"emotion\")\n",
    "    scores['emotion'] = [metric.compute(sentences=context_sentence)]\n",
    "    scores['emotion'].append(metric.compute(sentences=label_response))\n",
    "    scores['emotion'].append(metric.compute(sentences=chatbot_response))\n",
    "    scores['emotion'].append(sp.stats.stats.pearsonr(scores['emotion'][1]['score'],\n",
    "                                                     scores['emotion'][2]['score'])[0])\n",
    "    if verbose:\n",
    "        print('===       EMOTION       ===')\n",
    "        print('context emotions:            \\n', list(zip(scores['emotion'][0]['label'], scores['emotion'][0]['score'])))\n",
    "        print(lbl_text+' emotions:              \\n', list(zip(scores['emotion'][1]['label'], scores['emotion'][1]['score'])))\n",
    "        print(cht_text+' emotions:            \\n', list(zip(scores['emotion'][2]['label'], scores['emotion'][2]['score'])))\n",
    "        print(lbl_text+'-'+cht_text+'emotion corr:  \\n', scores['emotion'][3])\n",
    "    # 8) computes sas metric\n",
    "    metric = BBMetric.load_metric(\"semantic answer similarity\")\n",
    "    scores['semantic answer similarity'] = [metric.compute(predictions=context_sentences,\n",
    "                                                    references=label_responses)]\n",
    "    scores['semantic answer similarity'].append(metric.compute(predictions=context_sentences,\n",
    "                                                        references=chatbot_responses))\n",
    "    scores['semantic answer similarity'].append(metric.compute(predictions=label_responses,\n",
    "                                                        references=chatbot_responses))\n",
    "    scores['semantic answer similarity'].append(ccl_sim(scores['semantic answer similarity'][0]['score'],\n",
    "                                                        scores['semantic answer similarity'][1]['score'],\n",
    "                                                        scores['semantic answer similarity'][2]['score']))\n",
    "    if verbose:\n",
    "        print('===         SAS         ===')\n",
    "        print('context-'+lbl_text+' sas:          ', scores['semantic answer similarity'][0])\n",
    "        print('context-'+cht_text+' sas:        ', scores['semantic answer similarity'][1])\n",
    "        print(lbl_text+'-'+cht_text+' sas:          ', scores['semantic answer similarity'][2])\n",
    "        print('ccl-sim sas:               ', scores['semantic answer similarity'][3])\n",
    "    # 9) computes metrics for semantic classifier\n",
    "    metric = BBMetric.load_metric(\"semantic classifier\")\n",
    "    start_time = time.time()\n",
    "    scores['semantic classifier'] = [metric.compute(character=character, character_dict=character_dict, \n",
    "                                                   base_folder=base_folder, sentences=label_responses,\n",
    "                                                   n_sentences=classifier_n_sentences)]\n",
    "    scores['semantic classifier'].append(metric.compute(character=character, character_dict=character_dict, \n",
    "                                                   base_folder=base_folder, sentences=chatbot_responses,\n",
    "                                                   n_sentences=classifier_n_sentences))\n",
    "    end_time = time.time()\n",
    "    if verbose:\n",
    "        print('=== SEMANTIC CLASSIFIER ===')\n",
    "        print('sem-classifier '+lbl_text+':                ', scores['semantic classifier'][0])\n",
    "        print('sem-classifier '+cht_text+':                  ', scores['semantic classifier'][1])\n",
    "        print('time elapsed computing semantic classifier:  {:.2f} s'.format(end_time - start_time))\n",
    "    if not label_chatbot_symmetry and os.path.exists(os.path.join(os.getcwd(), \"Data\", \"Characters\", character, \"humancoherence.csv\")):\n",
    "        scores['human'] = {}\n",
    "        metric = BBMetric.load_metric(\"human - coherence\")\n",
    "        scores['human']['coherence'] = metric.compute(filepath=os.path.join(os.getcwd(), \"Data\", \"Characters\",\n",
    "                                                                            character, \"humancoherence.csv\"))\n",
    "        metric = BBMetric.load_metric(\"human - style\")\n",
    "        scores['human']['style'] = metric.compute(filepath=os.path.join(os.getcwd(), \"Data\", \"Characters\",\n",
    "                                                                        character, \"humanstyle.csv\"))\n",
    "        metric = BBMetric.load_metric(\"human - consistency\")\n",
    "        scores['human']['consistency'] = metric.compute(filepath=os.path.join(os.getcwd(), \"Data\", \"Characters\",\n",
    "                                                                              character, \"humanconsistency.csv\"))\n",
    "        if verbose:\n",
    "            print('===    HUMAN METRICS    ===')\n",
    "            print('coherence:                 ', scores['human']['coherence'])\n",
    "            print('consistency:               ', scores['human']['consistency'])\n",
    "            print('style:                     ', scores['human']['style'])\n",
    "    elif verbose:\n",
    "        print(\"Symmetric mode, skipping Human metrics.\")\n",
    "    if include_sentences:\n",
    "        sentences_df = {}\n",
    "        sentences_df['context'] = context_sentences\n",
    "        sentences_df[lbl_text] = label_responses\n",
    "        sentences_df[cht_text] = chatbot_responses\n",
    "        scores['sentences'] = sentences_df\n",
    "        if verbose:\n",
    "            print('===      SENTENCES      ===')\n",
    "            for i in range(len(context_sentences)):\n",
    "                print(\"* context: \", context_sentences[i])\n",
    "                print(\"* \" + lbl_text + \":\", label_responses[i])\n",
    "                print(\"* \" + cht_text + \":\", chatbot_responses[i])\n",
    "                print()\n",
    "    elif verbose:\n",
    "        print(\"Skipping sentence outputting.\")\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Sample 1 #####\n",
      "* context: I know, it's two years of my life I'm never getting back. A little part of me just wants to jump the bones of the next guy I see.\n",
      "* label:   Daddy's home.\n",
      "* chatbot: Oh, God!\n",
      "=== SEMANTIC SIMILARITY ===\n",
      "context-label similarity:    0.009414163418114185\n",
      "context-chatbot similarity:  0.01723679155111313\n",
      "chatbot-label similarity:    0.008852469734847546\n",
      "ccl-sim similarity:             0.4922471517326578\n",
      "===         BLEU         ===\n",
      "context-to-label bleu:       0.0\n",
      "context-to-chatbot bleu:     0.0\n",
      "label-to-chatbot bleu:       0.0\n",
      "ccl-sim bleu:             0.5\n",
      "===        ROUGE-L       ===\n",
      "context-to-label rouge:      0.0588235294117647\n",
      "context-to-chatbot rouge:    0.0\n",
      "label-to-chatbot rouge:      0.0\n",
      "ccl-sim rouge:             0.4429065743944637\n",
      "===       DISTINCT      ===\n",
      "context distinct:           0.20930232558139536\n",
      "label distinct:           0.0\n",
      "chatbot distinct:           0.0\n",
      "ccl-sim distinct:           0.3126014061654948\n",
      "===         SAS         ===\n",
      "context-label sas:           0.14686787128448486\n",
      "context-chatbot sas:         0.18236687779426575\n",
      "label-chatbot sas:           0.21476761996746063\n",
      "ccl-sim sas:                0.48819364851505365\n",
      "===       EMOTION       ===\n",
      "context emotions:            \n",
      " [('sadness', 0.006270758341997862), ('joy', 0.015778280794620514), ('love', 0.0008533376385457814), ('anger', 0.9678379893302917), ('fear', 0.008764993399381638), ('surprise', 0.000494705920573324)]\n",
      "label emotions:              \n",
      " [('sadness', 0.1004064530134201), ('joy', 0.08988457173109055), ('love', 0.0046431077644228935), ('anger', 0.7096097469329834), ('fear', 0.09349454194307327), ('surprise', 0.0019615974742919207)]\n",
      "chatbot emotions:            \n",
      " [('sadness', 0.06172791123390198), ('joy', 0.5121124386787415), ('love', 0.01288512907922268), ('anger', 0.09026128798723221), ('fear', 0.21414399147033691), ('surprise', 0.10886922478675842)]\n",
      "label-chatbotemotion corr:  \n",
      " -0.1226663402048312\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    print(\"##### Sample \" + str(i+1) + \" #####\")\n",
    "    context_sentence = df_char['ctx'][i]\n",
    "    chatbot_response = df_char['prd_greedy'][i]\n",
    "    label_response   = df_char['lbl'][i]\n",
    "    compute_sample_metrics(context_sentence, label_response, chatbot_response)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_size = 10\n",
    "i = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Set (Size 10) #####\n",
      "=== SEMANTIC SIMILARITY ===\n",
      "context-label similarity:    {'score': 0.013566510751843452, 'std': 0.004443929065018892}\n",
      "context-chatbot similarity:  {'score': 0.11592505127191544, 'std': 0.17320799827575684}\n",
      "chatbot-label similarity:    {'score': 0.056996725499629974, 'std': 0.07998812943696976}\n",
      "ccl-sim similarity:             0.4045044082474677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 10.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===       PERPLEXITY     ===\n",
      "chatbot perplexity:          57.42257428571708\n",
      "===         BLEU         ===\n",
      "context-to-label bleu:       {'score': 0.0, 'std': 0.0}\n",
      "context-to-chatbot bleu:     {'score': 0.0, 'std': 0.0}\n",
      "label-to-chatbot bleu:       {'score': 0.0, 'std': 0.0}\n",
      "ccl-sim bleu:             0.5\n",
      "===        ROUGE-L       ===\n",
      "context-to-label rouge:      {'score': 0.031536954087346025, 'std': 0.04899248320162975}\n",
      "context-to-chatbot rouge:    {'score': 0.09306625577812018, 'std': 0.16891792560453}\n",
      "label-to-chatbot rouge:      {'score': 0.02930093089867526, 'std': 0.04120352281931709}\n",
      "ccl-sim rouge:             0.4407928980682674\n",
      "===       DISTINCT      ===\n",
      "context distinct:           {'score': 0.09560764577892737, 'std': 0.0692566770759311}\n",
      "label distinct:           {'score': 0.13910765029661293, 'std': 0.04298315932942512}\n",
      "chatbot distinct:           {'score': 0.1097739725969051, 'std': 0.05256244507214358}\n",
      "ccl-sim distinct:           0.4634712832086868\n",
      "===       EMOTION       ===\n",
      "context emotions:            \n",
      " [('sadness', 0.006270758341997862), ('joy', 0.015778280794620514), ('love', 0.0008533376385457814), ('anger', 0.9678379893302917), ('fear', 0.008764993399381638), ('surprise', 0.000494705920573324)]\n",
      "label emotions:              \n",
      " [('sadness', 0.1004064530134201), ('joy', 0.08988457173109055), ('love', 0.0046431077644228935), ('anger', 0.7096097469329834), ('fear', 0.09349454194307327), ('surprise', 0.0019615974742919207)]\n",
      "chatbot emotions:            \n",
      " [('sadness', 0.06172791123390198), ('joy', 0.5121124386787415), ('love', 0.01288512907922268), ('anger', 0.09026128798723221), ('fear', 0.21414399147033691), ('surprise', 0.10886922478675842)]\n",
      "label-chatbotemotion corr:  \n",
      " -0.1226663402048312\n",
      "===         SAS         ===\n",
      "context-label sas:           {'score': 0.21949104964733124, 'std': 0.07066471129655838}\n",
      "context-chatbot sas:         {'score': 0.2867209017276764, 'std': 0.09131180495023727}\n",
      "label-chatbot sas:           {'score': 0.23960378766059875, 'std': 0.10616812109947205}\n",
      "ccl-sim sas:                0.46373506195568004\n",
      "=== SEMANTIC CLASSIFIER ===\n",
      "sem-classifier label:                 {'score': 0.9981417059898376, 'std': 0.02565186657011509}\n",
      "sem-classifier chatbot:                   {'score': 0.4036124050617218, 'std': 0.44909152388572693}\n",
      "time elapsed computing semantic classifier:  10.78 s\n",
      "Symmetric mode, skipping Human metrics.\n",
      "Skipping sentence outputting.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'metadata': {'ordering': ['context-label',\n",
       "   'context-chatbot',\n",
       "   'chatbot-label',\n",
       "   'ccl']},\n",
       " 'semantic similarity': [{'score': 0.013566510751843452,\n",
       "   'std': 0.004443929065018892},\n",
       "  {'score': 0.11592505127191544, 'std': 0.17320799827575684},\n",
       "  {'score': 0.056996725499629974, 'std': 0.07998812943696976},\n",
       "  0.4045044082474677],\n",
       " 'perplexity': 57.42257428571708,\n",
       " 'bleu': [{'score': 0.0, 'std': 0.0},\n",
       "  {'score': 0.0, 'std': 0.0},\n",
       "  {'score': 0.0, 'std': 0.0},\n",
       "  0.5],\n",
       " 'rouge l': [{'score': 0.031536954087346025, 'std': 0.04899248320162975},\n",
       "  {'score': 0.09306625577812018, 'std': 0.16891792560453},\n",
       "  {'score': 0.02930093089867526, 'std': 0.04120352281931709},\n",
       "  0.4407928980682674],\n",
       " 'distinct': [{'score': 0.09560764577892737, 'std': 0.0692566770759311},\n",
       "  {'score': 0.13910765029661293, 'std': 0.04298315932942512},\n",
       "  {'score': 0.1097739725969051, 'std': 0.05256244507214358},\n",
       "  0.4634712832086868],\n",
       " 'emotion': [{'score': [0.006270758341997862,\n",
       "    0.015778280794620514,\n",
       "    0.0008533376385457814,\n",
       "    0.9678379893302917,\n",
       "    0.008764993399381638,\n",
       "    0.000494705920573324],\n",
       "   'std': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "   'label': ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']},\n",
       "  {'score': [0.1004064530134201,\n",
       "    0.08988457173109055,\n",
       "    0.0046431077644228935,\n",
       "    0.7096097469329834,\n",
       "    0.09349454194307327,\n",
       "    0.0019615974742919207],\n",
       "   'std': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "   'label': ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']},\n",
       "  {'score': [0.06172791123390198,\n",
       "    0.5121124386787415,\n",
       "    0.01288512907922268,\n",
       "    0.09026128798723221,\n",
       "    0.21414399147033691,\n",
       "    0.10886922478675842],\n",
       "   'std': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "   'label': ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']},\n",
       "  -0.1226663402048312],\n",
       " 'semantic answer similarity': [{'score': 0.21949104964733124,\n",
       "   'std': 0.07066471129655838},\n",
       "  {'score': 0.2867209017276764, 'std': 0.09131180495023727},\n",
       "  {'score': 0.23960378766059875, 'std': 0.10616812109947205},\n",
       "  0.46373506195568004],\n",
       " 'semantic classifier': [{'score': 0.9981417059898376,\n",
       "   'std': 0.02565186657011509},\n",
       "  {'score': 0.4036124050617218, 'std': 0.44909152388572693}]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"##### Set (Size \" + str(set_size) + \") #####\")\n",
    "context_sentences = list(df_char['ctx'][i:i+set_size])\n",
    "chatbot_responses = list(df_char['prd_greedy'][i:i+set_size])\n",
    "label_responses   = list(df_char['lbl'][i:i+set_size])\n",
    "compute_set_metrics(model, tokenizer,\n",
    "                    context_sentences, label_responses, chatbot_responses, character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"##### Full Test Set #####\")\n",
    "context_sentences = list(df_char['ctx'])\n",
    "chatbot_responses = list(df_char['prd_greedy'])\n",
    "label_responses   = list(df_char['lbl'])\n",
    "scores = compute_set_metrics(model, tokenizer,\n",
    "                    context_sentences, \n",
    "                    label_responses, \n",
    "                    chatbot_responses,\n",
    "                    character,\n",
    "                    classifier_n_sentences=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_json(metrics_folder, character+'_base_metrics', scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Between Character 1 & Character 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_small(sample_questions, model, generation_method):\n",
    "    print(\"Creating predictions\")\n",
    "    predictions = list()\n",
    "    for x in tqdm(sample_questions):\n",
    "        tokenized_question = tokenizer.encode(x + tokenizer.eos_token, return_tensors='tf')\n",
    "        max_length = 128 + tokenized_question.shape[1]\n",
    "        if generation_method == \"Greedy\":\n",
    "            generated_answer = model.generate(tokenized_question,\n",
    "                                pad_token_id=tokenizer.eos_token_id, max_length=max_length)[0].numpy().tolist()\n",
    "        elif generation_method == \"Beam Search\":\n",
    "            generated_answer = model.generate(tokenized_question,\n",
    "                                         pad_token_id=tokenizer.eos_token_id, max_length=max_length,\n",
    "                                         n_beams=n_beams)[0].numpy().tolist()\n",
    "        elif generation_method == \"Sampling\":\n",
    "            generated_answer = model.generate(tokenized_question,\n",
    "                                         pad_token_id=tokenizer.eos_token_id, max_length=max_length,\n",
    "                                         do_sample=True, top_k=top_k, top_p=top_p)[0].numpy().tolist()\n",
    "        predictions.append(generated_answer[len(tokenized_question[0]):])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_common = load_dataset('csv',\n",
    "                         data_files=os.path.join(base_folder, 'Data', 'common_dataset.csv'), \n",
    "                         cache_dir=os.path.join(base_folder, \"cache\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_1_sampling = get_predictions_small(df_common['train']['context'], model, \"Sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_2_sampling = get_predictions_small(df_common['train']['context'], model_2, \"Sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_common_char_1 = get_dataframe_for_metrics(df_common['train'], None, None, predictions_1_sampling)\n",
    "df_common_char_2 = get_dataframe_for_metrics(df_common['train'], None, None, predictions_2_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"##### \" + character + \"  Vs. \" + character_2 + \" #####\")\n",
    "context_sentences   = list(df_common_char_1['ctx'])\n",
    "chatbot_responses   = list(df_common_char_1['prd_sampling'])\n",
    "chatbot_2_responses = list(df_common_char_2['prd_sampling'])\n",
    "scores = compute_set_metrics(None, None,\n",
    "                            context_sentences, chatbot_responses, chatbot_2_responses, None,\n",
    "                            include_sentences=True, label_chatbot_symmetry=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_json(metrics_folder, character+'_vs_'+character_2+'_metrics', scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Between Different Sampling Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scores = {}\n",
    "print(\"##### Greedy vs. N-Beams #####\")\n",
    "context_sentences = list(df_char['ctx'])\n",
    "greedy_responses  = list(df_char['prd_greedy'])\n",
    "nbeams_responses  = list(df_char['prd_nbeams'])\n",
    "scores['greedy_vs_nbeams'] = compute_set_metrics(None, None,\n",
    "                                                 context_sentences,\n",
    "                                                 greedy_responses,\n",
    "                                                 nbeams_responses,\n",
    "                                                 character,\n",
    "                                                 classifier_n_sentences=75,\n",
    "                                                 label_chatbot_symmetry=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"##### Greedy vs. Sampling #####\")\n",
    "context_sentences   = list(df_char['ctx'])\n",
    "greedy_responses    = list(df_char['prd_greedy'])\n",
    "sampling_responses  = list(df_char['prd_sampling'])\n",
    "scores['greedy_vs_sampling'] = compute_set_metrics(None, None,\n",
    "                                                   context_sentences,\n",
    "                                                   greedy_responses,\n",
    "                                                   sampling_responses,\n",
    "                                                   character,\n",
    "                                                   classifier_n_sentences=75,\n",
    "                                                   label_chatbot_symmetry=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"##### N-Beams vs. Sampling #####\")\n",
    "context_sentences   = list(df_char['ctx'])\n",
    "nbeams_responses    = list(df_char['prd_nbeams'])\n",
    "sampling_responses  = list(df_char['prd_sampling'])\n",
    "scores['nbeams_vs_sampling'] = compute_set_metrics(None, None,\n",
    "                                                   context_sentences,\n",
    "                                                   nbeams_responses,\n",
    "                                                   sampling_responses,\n",
    "                                                   character,\n",
    "                                                   classifier_n_sentences=75,\n",
    "                                                   label_chatbot_symmetry=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_json(metrics_folder, character+'_sampling_comparison_metrics', scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Between Non-Finetuned And Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_def_sampling = get_predictions_cached(sample_questions, model_def,\n",
    "                                                  os.path.join(in_folder_def, 'from_' + character + '_df_' + '_sampling.json'),\n",
    "                                                  \"Sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char_def = get_dataframe_for_metrics(character_hg['test'], None, None, predictions_def_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    print(\"##### Sample \" + str(i+1) + \" #####\")\n",
    "    context_sentence   = df_char['ctx'][i]\n",
    "    character_response = df_char['prd_sampling'][i]\n",
    "    default_response   = df_char_def['prd_sampling'][i]\n",
    "    compute_sample_metrics(context_sentence, default_response, character_response, label_chatbot_symmetry=True)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_size = 50\n",
    "i = 30\n",
    "print(\"##### Set (Size \" + str(set_size) + \") #####\")\n",
    "context_sentences   = list(df_char['ctx'][i:i+set_size])\n",
    "character_responses = list(df_char['prd_sampling'][i:i+set_size])\n",
    "default_responses   = list(df_char_def['prd_sampling'][i:i+set_size])\n",
    "compute_set_metrics(None, None,\n",
    "                    context_sentences, default_responses, character_responses, character, label_chatbot_symmetry=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"##### Full Test Set (Size \" + str(set_size) + \") #####\")\n",
    "context_sentences   = list(df_char['ctx'])\n",
    "character_responses = list(df_char['prd_sampling'])\n",
    "default_responses   = list(df_char_def['prd_sampling'])\n",
    "scores = compute_set_metrics(None, None,\n",
    "                             context_sentences, \n",
    "                             default_responses, \n",
    "                             character_responses,\n",
    "                             character,\n",
    "                             classifier_n_sentences=75,\n",
    "                             label_chatbot_symmetry=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_json(metrics_folder, character+'_vs_nonfinetuned_metrics', scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

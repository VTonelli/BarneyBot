{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data.data_dicts import character_dict, source_dict, random_state\n",
    "\n",
    "model_name = 'microsoft/DialoGPT-small'\n",
    "character = 'Vader' # 'Barney' | 'Sheldon' | 'Harry' | 'Fry' | 'Vader' | 'Joey' | 'Phoebe' | 'Bender' | Default'\n",
    "character_2 = 'Harry'\n",
    "override_predictions = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount google drive\n",
    "import os\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive',force_remount=True)\n",
    "    base_folder = '/content/drive/My Drive/unibo/NLP_project/BarneyBot'\n",
    "    os.system(\"pip install datasets\")\n",
    "    os.system(\"pip install transformers\")\n",
    "    os.system(\"pip install rouge_score\")\n",
    "    os.system(\"pip install -U sentence-transformers\")\n",
    "else:\n",
    "    base_folder = os.getcwd()\n",
    "    \n",
    "in_folder = os.path.join(base_folder, 'Data', 'Characters', character)\n",
    "if not os.path.exists(in_folder):\n",
    "    os.makedirs(in_folder)\n",
    "out_folder = os.path.join(base_folder, 'Data', 'Characters', character)\n",
    "if not os.path.exists(out_folder):\n",
    "    os.makedirs(out_folder)\n",
    "    \n",
    "in_folder_2 = os.path.join(base_folder, 'Data', 'Characters', character_2)\n",
    "if not os.path.exists(in_folder_2):\n",
    "    os.makedirs(in_folder_2)\n",
    "out_folder_2 = os.path.join(base_folder, 'Data', 'Characters', character_2)\n",
    "if not os.path.exists(out_folder_2):\n",
    "    os.makedirs(out_folder_2)\n",
    "    \n",
    "in_folder_def = os.path.join(base_folder, 'Data', 'Characters', 'Default')\n",
    "if not os.path.exists(in_folder_def):\n",
    "    os.makedirs(in_folder_def)\n",
    "out_folder_def = os.path.join(base_folder, 'Data', 'Characters', 'Default')\n",
    "if not os.path.exists(out_folder_def):\n",
    "    os.makedirs(out_folder_def)\n",
    "    \n",
    "metrics_folder = os.path.join(base_folder, 'Metrics')\n",
    "if not os.path.exists(metrics_folder):\n",
    "    os.makedirs(metrics_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_json(filepath, filename, data):\n",
    "    if not os.path.exists(filepath):\n",
    "        os.makedirs(filepath, exist_ok=True)\n",
    "    with open(os.path.join(filepath, filename + \".json\"), 'w') as f:\n",
    "        f.write(json.dumps(data, indent=4))\n",
    "\n",
    "def load_from_json(filepath, filename):\n",
    "    if not os.path.exists(os.path.join(filepath, filename + '.json')):\n",
    "        return dict()\n",
    "    with open(os.path.join(filepath, filename + '.json'), 'r') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "def load_df(character):\n",
    "    dataset_path = os.path.join(base_folder, \"Data\", \"Characters\", character, character+'.csv')\n",
    "    \n",
    "    character_hg = load_dataset('csv', \n",
    "                                data_files=dataset_path, \n",
    "                                cache_dir=os.path.join(base_folder, \"cache\"))\n",
    "    \n",
    "    # 85% train / 10% test / 5% validation\n",
    "    train_test_hg = character_hg['train'].train_test_split(test_size=0.15, seed=random_state)\n",
    "    test_val = train_test_hg['test'].train_test_split(test_size=0.33, seed=random_state)\n",
    "    \n",
    "    \n",
    "    character_hg = DatasetDict({\n",
    "        'train': train_test_hg['train'],\n",
    "        'test': test_val['train'],\n",
    "        'val': test_val['test']\n",
    "    })\n",
    "    \n",
    "    return character_hg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_conv(row, tokenizer):\n",
    "    MAX_LENGTH = 512\n",
    "    row = list(reversed(list(row.values())))\n",
    "    model_inputs = tokenizer(row)\n",
    "    tokenizer_pad_token_id = tokenizer.encode('#')[0]\n",
    "    for i in range(len(model_inputs['input_ids'])):\n",
    "        model_inputs['input_ids'][i].append(tokenizer.eos_token_id)\n",
    "        model_inputs['attention_mask'][i].append(1)\n",
    "    model_inputs['input_ids'] = [item for sublist in model_inputs['input_ids'] for item in sublist]\n",
    "    model_inputs['attention_mask'] = [item for sublist in model_inputs['attention_mask'] for item in sublist]\n",
    "    if MAX_LENGTH > len(model_inputs['input_ids']):\n",
    "        model_inputs['input_ids'] += [tokenizer_pad_token_id] * (MAX_LENGTH - len(model_inputs['input_ids']))\n",
    "        model_inputs['attention_mask'] += [0] * (MAX_LENGTH - len(model_inputs['attention_mask']))\n",
    "    elif MAX_LENGTH < len(model_inputs['input_ids']):\n",
    "        model_inputs['input_ids'] = model_inputs['input_ids'][:MAX_LENGTH-1]\n",
    "        model_inputs['input_ids'][-1] = tokenizer.eos_token_id\n",
    "        model_inputs['attention_mask'] = model_inputs['attention_mask'][:MAX_LENGTH-1]\n",
    "        model_inputs['attention_mask'][-1] = 1\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    tokenizer.pad_token = '#'\n",
    "    model_inputs = construct_conv(examples, tokenizer)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-8fc66de038de764b\n",
      "Reusing dataset csv (C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\cache\\csv\\default-8fc66de038de764b\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dcfa0649e0e435ca37e6dd097020ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\cache\\csv\\default-8fc66de038de764b\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-fdf6a751d9e305ad.arrow and C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\cache\\csv\\default-8fc66de038de764b\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-fd4f962239c0d8a3.arrow\n",
      "Loading cached split indices for dataset at C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\cache\\csv\\default-8fc66de038de764b\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-724da1b5b03e5c90.arrow and C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\cache\\csv\\default-8fc66de038de764b\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-6b695a18a6bce864.arrow\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"HF_DATASETS_CACHE\"] = os.path.join(base_folder, \"cache\")\n",
    "character_hg = load_df(character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_folder = os.path.join(out_folder, character_dict[character]['checkpoint_folder'])\n",
    "checkpoint_folder_2 = os.path.join(out_folder_2, character_dict[character_2]['checkpoint_folder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\Data\\Characters\\Vader\\vader_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! Please ensure your labels are passed as keys in the input dict so that they are accessible to the model during the forward pass. To disable this behaviour, please pass a loss argument, or explicitly pass loss=None if you do not want your model to compute a loss.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\Data\\Characters\\Harry\\harry_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! Please ensure your labels are passed as keys in the input dict so that they are accessible to the model during the forward pass. To disable this behaviour, please pass a loss argument, or explicitly pass loss=None if you do not want your model to compute a loss.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at microsoft/DialoGPT-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! Please ensure your labels are passed as keys in the input dict so that they are accessible to the model during the forward pass. To disable this behaviour, please pass a loss argument, or explicitly pass loss=None if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForCausalLM, AutoTokenizer, AdamWeightDecay\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=os.path.join(base_folder, \"cache\"))\n",
    "tokenizer.pad_token = '#'\n",
    "\n",
    "model = TFAutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=checkpoint_folder)\n",
    "model.compile(optimizer=AdamWeightDecay(learning_rate=2e-5))\n",
    "\n",
    "model_2 = TFAutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=checkpoint_folder_2)\n",
    "model_2.compile(optimizer=AdamWeightDecay(learning_rate=2e-5))\n",
    "\n",
    "model_def = TFAutoModelForCausalLM.from_pretrained(model_name, cache_dir=os.path.join(base_folder, \"cache\"))\n",
    "model_def.compile(optimizer=AdamWeightDecay(learning_rate=2e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\cache\\csv\\default-8fc66de038de764b\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-fd9a309bbef55052.arrow\n",
      "Loading cached processed dataset at C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\cache\\csv\\default-8fc66de038de764b\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-2d6e279951cff576.arrow\n",
      "Loading cached processed dataset at C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\cache\\csv\\default-8fc66de038de764b\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-ce7aa8f599523585.arrow\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import AdamWeightDecay\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(mlm=False, tokenizer=tokenizer, return_tensors='tf')\n",
    "\n",
    "tokenized_character_hg = character_hg.map(preprocess_function, batched=False)\n",
    "\n",
    "encoded_test_set = tokenized_character_hg[\"test\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_questions = character_hg['test']['context/0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_beams = 3\n",
    "top_k = 50\n",
    "top_p = 0.92\n",
    "\n",
    "def get_predictions_cached(sample_questions, model, filename, generation_method, override_predictions=False):\n",
    "    prediction_path = os.path.join(in_folder, filename)\n",
    "    if os.path.exists(prediction_path) and not override_predictions:\n",
    "        print(\"Loading predictions from stored file\")\n",
    "        with open(prediction_path, 'r') as file:\n",
    "            json_string = file.read()\n",
    "        predictions = json.loads(json_string)\n",
    "        print(\"Loaded predictions from stored file\")\n",
    "\n",
    "    else:\n",
    "        print(\"Creating predictions\")\n",
    "        predictions = list()\n",
    "        for x in tqdm(sample_questions):\n",
    "            tokenized_question = tokenizer.encode(x + tokenizer.eos_token, return_tensors='tf')\n",
    "            max_length = 128 + tokenized_question.shape[1]\n",
    "            if generation_method == \"Greedy\":\n",
    "                generated_answer = model.generate(tokenized_question,\n",
    "                                    pad_token_id=tokenizer.eos_token_id, max_length=max_length)[0].numpy().tolist()\n",
    "            elif generation_method == \"Beam Search\":\n",
    "                generated_answer = model.generate(tokenized_question,\n",
    "                                             pad_token_id=tokenizer.eos_token_id, max_length=max_length,\n",
    "                                             n_beams=n_beams)[0].numpy().tolist()\n",
    "            elif generation_method == \"Sampling\":\n",
    "                b = True\n",
    "                c = 0\n",
    "                while b:\n",
    "                    generated_answer = model.generate(tokenized_question,\n",
    "                                                 pad_token_id=tokenizer.eos_token_id, max_length=max_length,\n",
    "                                                 do_sample=True, top_k=top_k, top_p=top_p)[0].numpy().tolist()\n",
    "                    c += 1\n",
    "                    if len(generated_answer[len(tokenized_question[0]):])>1:\n",
    "                        b = False       \n",
    "                    if c>100: \n",
    "                        generated_answer[len(tokenized_question[0]):] = tokenizer.encode('hi') + [tokenizer.eos_token_id]\n",
    "                        break\n",
    "            \n",
    "            predictions.append(generated_answer[len(tokenized_question[0]):])\n",
    "\n",
    "        # Save predictions as a JSON file\n",
    "        output_string = json.dumps(predictions)\n",
    "        with open(prediction_path, 'w') as file:\n",
    "            file.write(output_string)\n",
    "        \n",
    "        assert all([len(p)>1 for p in predictions])\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading predictions from stored file\n",
      "Loaded predictions from stored file\n",
      "Loading predictions from stored file\n",
      "Loaded predictions from stored file\n",
      "Loading predictions from stored file\n",
      "Loaded predictions from stored file\n"
     ]
    }
   ],
   "source": [
    "predictions_greedy = get_predictions_cached(sample_questions, model,\n",
    "                                            character_dict[character]['prediction_filename'] + '_greedy.json',\n",
    "                                            \"Greedy\",\n",
    "                                            override_predictions=override_predictions)\n",
    "predictions_nbeams = get_predictions_cached(sample_questions, model,\n",
    "                                            character_dict[character]['prediction_filename'] + '_nbeams.json',\n",
    "                                            \"Beam Search\",\n",
    "                                            override_predictions=override_predictions)\n",
    "predictions_sampling = get_predictions_cached(sample_questions, model,\n",
    "                                              character_dict[character]['prediction_filename'] + '_sampling.json',\n",
    "                                              \"Sampling\",\n",
    "                                              override_predictions=override_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe_for_metrics(data_test, predictions_greedy, predictions_nbeams, predictions_sampling):\n",
    "    i = 0\n",
    "    df = {'ctx':[], 'ctx_tk':[]}\n",
    "    has_labels = 'response' in data_test.features\n",
    "    if has_labels:\n",
    "        df['lbl'] = []\n",
    "        df['lbl_tk'] = []\n",
    "    if predictions_greedy:\n",
    "        df['prd_greedy'] = []\n",
    "        df['prd_greedy_tk'] = []\n",
    "    if predictions_nbeams:\n",
    "        df['prd_nbeams'] = []\n",
    "        df['prd_nbeams_tk'] = [] \n",
    "    if predictions_sampling:\n",
    "        df['prd_sampling'] = []\n",
    "        df['prd_sampling_tk'] = []\n",
    "    for sample in tqdm(data_test):\n",
    "        # encode the context and label sentences, add the eos_token and return a tensor\n",
    "        ctx_tk = tokenizer.encode(sample['context/0'] + tokenizer.eos_token, return_tensors='tf').numpy().tolist()\n",
    "        ctx = sample['context/0']\n",
    "        df['ctx_tk'].append(ctx_tk)\n",
    "        df['ctx'].append(ctx)\n",
    "        if has_labels:\n",
    "            lbl_tk = tokenizer.encode(sample['response'] + tokenizer.eos_token, return_tensors='tf').numpy().tolist()\n",
    "            lbl = sample['response']\n",
    "            df['lbl'].append(lbl)\n",
    "            df['lbl_tk'].append(lbl_tk)\n",
    "        if predictions_greedy:\n",
    "            prd_greedy_tk = predictions_greedy[i]\n",
    "            prd_greedy = tokenizer.decode(prd_greedy_tk, skip_special_tokens=True)\n",
    "            df['prd_greedy'].append(prd_greedy)\n",
    "            df['prd_greedy_tk'].append(prd_greedy_tk)\n",
    "        if predictions_nbeams:\n",
    "            prd_nbeams_tk = predictions_nbeams[i]\n",
    "            prd_nbeams = tokenizer.decode(prd_nbeams_tk, skip_special_tokens=True)\n",
    "            df['prd_nbeams'].append(prd_nbeams)\n",
    "            df['prd_nbeams_tk'].append(prd_nbeams_tk)\n",
    "        if predictions_sampling:\n",
    "            prd_sampling_tk = predictions_sampling[i]\n",
    "            prd_sampling = tokenizer.decode(prd_sampling_tk, skip_special_tokens=True)\n",
    "            df['prd_sampling'].append(prd_sampling)\n",
    "            df['prd_sampling_tk'].append(prd_sampling_tk)\n",
    "        i += 1\n",
    "    return pd.DataFrame(data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 1141.83it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ctx</th>\n",
       "      <th>ctx_tk</th>\n",
       "      <th>lbl</th>\n",
       "      <th>lbl_tk</th>\n",
       "      <th>prd_greedy</th>\n",
       "      <th>prd_greedy_tk</th>\n",
       "      <th>prd_nbeams</th>\n",
       "      <th>prd_nbeams_tk</th>\n",
       "      <th>prd_sampling</th>\n",
       "      <th>prd_sampling_tk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I will not fight you.</td>\n",
       "      <td>[[40, 481, 407, 1907, 345, 13, 50256]]</td>\n",
       "      <td>Give yourself to the dark side. It is the only...</td>\n",
       "      <td>[[23318, 3511, 284, 262, 3223, 1735, 13, 632, ...</td>\n",
       "      <td>I will not fight you.</td>\n",
       "      <td>[40, 481, 407, 1907, 345, 13, 50256]</td>\n",
       "      <td>I will not fight you.</td>\n",
       "      <td>[40, 481, 407, 1907, 345, 13, 50256]</td>\n",
       "      <td>Prepare your ships, Imperial Master.</td>\n",
       "      <td>[37534, 533, 534, 7937, 11, 11773, 5599, 13, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unlock one-five-seven and nine.Release charges.</td>\n",
       "      <td>[[3118, 5354, 530, 12, 13261, 12, 26548, 290, ...</td>\n",
       "      <td>Did you find any droids?</td>\n",
       "      <td>[[11633, 345, 1064, 597, 3102, 2340, 30, 50256]]</td>\n",
       "      <td>Three, four,five,seven,seven,seven,seven,seven...</td>\n",
       "      <td>[12510, 11, 1440, 11, 13261, 11, 26548, 11, 26...</td>\n",
       "      <td>Three, four,five,seven,seven,seven,seven,seven...</td>\n",
       "      <td>[12510, 11, 1440, 11, 13261, 11, 26548, 11, 26...</td>\n",
       "      <td>... a disturbance in theForce, a disturbance i...</td>\n",
       "      <td>[986, 257, 30497, 287, 262, 10292, 11, 257, 30...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lord Vader, what about Leia and theWookiee?</td>\n",
       "      <td>[[22438, 27403, 11, 644, 546, 41212, 290, 262,...</td>\n",
       "      <td>They must never again leave thiscity.</td>\n",
       "      <td>[[2990, 1276, 1239, 757, 2666, 428, 19205, 13,...</td>\n",
       "      <td>I don't know.</td>\n",
       "      <td>[40, 836, 470, 760, 13, 50256]</td>\n",
       "      <td>I don't know.</td>\n",
       "      <td>[40, 836, 470, 760, 13, 50256]</td>\n",
       "      <td>They're dead.</td>\n",
       "      <td>[2990, 821, 2636, 13, 50256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No!</td>\n",
       "      <td>[[2949, 0, 50256]]</td>\n",
       "      <td>All to easy. Perhaps you are not as strong as ...</td>\n",
       "      <td>[[3237, 284, 2562, 13, 8673, 345, 389, 407, 35...</td>\n",
       "      <td>No!No!No!No!No!No!No!No!No!No!No!No!No!No!No!N...</td>\n",
       "      <td>[2949, 0, 2949, 0, 2949, 0, 2949, 0, 2949, 0, ...</td>\n",
       "      <td>No!No!No!No!No!No!No!No!No!No!No!No!No!No!No!N...</td>\n",
       "      <td>[2949, 0, 2949, 0, 2949, 0, 2949, 0, 2949, 0, ...</td>\n",
       "      <td>Oh, my Lord, why, sir!No, sir, no!Only me!</td>\n",
       "      <td>[5812, 11, 616, 4453, 11, 1521, 11, 15967, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Give yourself to the dark side. It is the only...</td>\n",
       "      <td>[[23318, 3511, 284, 262, 3223, 1735, 13, 632, ...</td>\n",
       "      <td>Sister! So...you have a twinsister. Your feeli...</td>\n",
       "      <td>[[50, 1694, 0, 1406, 986, 5832, 423, 257, 2034...</td>\n",
       "      <td>I have found my way.</td>\n",
       "      <td>[40, 423, 1043, 616, 835, 13, 50256]</td>\n",
       "      <td>I have found my way.</td>\n",
       "      <td>[40, 423, 1043, 616, 835, 13, 50256]</td>\n",
       "      <td>I've completed the training. You can't leave t...</td>\n",
       "      <td>[40, 1053, 5668, 262, 3047, 13, 921, 460, 470,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Open the blast doors! Open theblast doors!</td>\n",
       "      <td>[[11505, 262, 11975, 8215, 0, 4946, 262, 39806...</td>\n",
       "      <td>I've been waiting for you, Obi-Wan. We meet ag...</td>\n",
       "      <td>[[40, 1053, 587, 4953, 329, 345, 11, 46662, 12...</td>\n",
       "      <td>Open theblast doors!</td>\n",
       "      <td>[11505, 262, 39806, 8215, 0, 50256]</td>\n",
       "      <td>Open theblast doors!</td>\n",
       "      <td>[11505, 262, 39806, 8215, 0, 50256]</td>\n",
       "      <td>We're trying to land on target, don't get crazy.</td>\n",
       "      <td>[1135, 821, 2111, 284, 1956, 319, 2496, 11, 83...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Strange, that I have not. I wonder if your fee...</td>\n",
       "      <td>[[38114, 11, 326, 314, 423, 407, 13, 314, 4240...</td>\n",
       "      <td>They are clear, my Master.</td>\n",
       "      <td>[[2990, 389, 1598, 11, 616, 5599, 13, 50256]]</td>\n",
       "      <td>I have not felt this way since I've been here.</td>\n",
       "      <td>[40, 423, 407, 2936, 428, 835, 1201, 314, 1053...</td>\n",
       "      <td>I have not felt this way since I've been here.</td>\n",
       "      <td>[40, 423, 407, 2936, 428, 835, 1201, 314, 1053...</td>\n",
       "      <td>You know what? I'll make it clear. Lord Vader,...</td>\n",
       "      <td>[1639, 760, 644, 30, 314, 1183, 787, 340, 1598...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Vader's targeting computer swings around into ...</td>\n",
       "      <td>[[53, 5067, 338, 10822, 3644, 26728, 1088, 656...</td>\n",
       "      <td>I have you now.</td>\n",
       "      <td>[[40, 423, 345, 783, 13, 50256]]</td>\n",
       "      <td>Vader adjusts his control stick and adjusts hi...</td>\n",
       "      <td>[53, 5067, 46094, 465, 1630, 4859, 290, 46094,...</td>\n",
       "      <td>Vader adjusts his control stick and adjusts hi...</td>\n",
       "      <td>[53, 5067, 46094, 465, 1630, 4859, 290, 46094,...</td>\n",
       "      <td>The Emperor stands on the platform as the lase...</td>\n",
       "      <td>[464, 10851, 6296, 319, 262, 3859, 355, 262, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Emperor's coming here?</td>\n",
       "      <td>[[464, 10851, 338, 2406, 994, 30, 50256]]</td>\n",
       "      <td>That is correct, Commander. And heis most disp...</td>\n",
       "      <td>[[2504, 318, 3376, 11, 13353, 13, 843, 339, 27...</td>\n",
       "      <td>The Emperor's coming here!</td>\n",
       "      <td>[464, 10851, 338, 2406, 994, 0, 50256]</td>\n",
       "      <td>The Emperor's coming here!</td>\n",
       "      <td>[464, 10851, 338, 2406, 994, 0, 50256]</td>\n",
       "      <td>The Senate will not be denied.</td>\n",
       "      <td>[464, 3845, 481, 407, 307, 6699, 13, 50256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The princess! Put all sections onalert!</td>\n",
       "      <td>[[464, 21752, 0, 5930, 477, 9004, 319, 44598, ...</td>\n",
       "      <td>Obi-Wan is here. The Force is withhim.</td>\n",
       "      <td>[[5944, 72, 12, 45681, 318, 994, 13, 383, 5221...</td>\n",
       "      <td>The princess is here!</td>\n",
       "      <td>[464, 21752, 318, 994, 0, 50256]</td>\n",
       "      <td>The princess is here!</td>\n",
       "      <td>[464, 21752, 318, 994, 0, 50256]</td>\n",
       "      <td>It was the last known location of the Rebellio...</td>\n",
       "      <td>[1026, 373, 262, 938, 1900, 4067, 286, 262, 34...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Good.Use your aggressivefeelings, boy!Let the ...</td>\n",
       "      <td>[[10248, 13, 11041, 534, 8361, 36410, 654, 11,...</td>\n",
       "      <td>Obi-Wan has taught you well.</td>\n",
       "      <td>[[5944, 72, 12, 45681, 468, 7817, 345, 880, 13...</td>\n",
       "      <td>I've felt it.</td>\n",
       "      <td>[40, 1053, 2936, 340, 13, 50256]</td>\n",
       "      <td>I've felt it.</td>\n",
       "      <td>[40, 1053, 2936, 340, 13, 50256]</td>\n",
       "      <td>Yes, my lord. The end is near.</td>\n",
       "      <td>[5297, 11, 616, 15876, 13, 383, 886, 318, 1474...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Surely he must be dead by now.</td>\n",
       "      <td>[[19457, 306, 339, 1276, 307, 2636, 416, 783, ...</td>\n",
       "      <td>Don't underestimate the power ofthe Force.</td>\n",
       "      <td>[[3987, 470, 34994, 262, 1176, 286, 1169, 5221...</td>\n",
       "      <td>He's dead.</td>\n",
       "      <td>[1544, 338, 2636, 13, 50256]</td>\n",
       "      <td>He's dead.</td>\n",
       "      <td>[1544, 338, 2636, 13, 50256]</td>\n",
       "      <td>Lord Vader!</td>\n",
       "      <td>[22438, 27403, 0, 50256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Shall I hold them?</td>\n",
       "      <td>[[2484, 439, 314, 1745, 606, 30, 50256]]</td>\n",
       "      <td>No. Leave them to me. I will deal</td>\n",
       "      <td>[[2949, 13, 17446, 606, 284, 502, 13, 314, 481...</td>\n",
       "      <td>Yes, my lord.</td>\n",
       "      <td>[5297, 11, 616, 15876, 13, 50256]</td>\n",
       "      <td>Yes, my lord.</td>\n",
       "      <td>[5297, 11, 616, 15876, 13, 50256]</td>\n",
       "      <td>It would be much better, Lord Vader, if we cou...</td>\n",
       "      <td>[1026, 561, 307, 881, 1365, 11, 4453, 27403, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What if he doesn't survive? He's worth a lot t...</td>\n",
       "      <td>[[2061, 611, 339, 1595, 470, 7866, 30, 679, 33...</td>\n",
       "      <td>The Empire will compensate you if he dies. Put...</td>\n",
       "      <td>[[464, 8065, 481, 21392, 345, 611, 339, 10564,...</td>\n",
       "      <td>If he survives this he will be the most powerf...</td>\n",
       "      <td>[1532, 339, 36417, 428, 339, 481, 307, 262, 74...</td>\n",
       "      <td>If he survives this he will be the most powerf...</td>\n",
       "      <td>[1532, 339, 36417, 428, 339, 481, 307, 262, 74...</td>\n",
       "      <td>If he survives</td>\n",
       "      <td>[1532, 339, 36417, 50256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Then you must go to the Sanctuary</td>\n",
       "      <td>[[6423, 345, 1276, 467, 284, 262, 27036, 50256]]</td>\n",
       "      <td>He will come to me?</td>\n",
       "      <td>[[1544, 481, 1282, 284, 502, 30, 50256]]</td>\n",
       "      <td>I want to know what happened to the others.</td>\n",
       "      <td>[40, 765, 284, 760, 644, 3022, 284, 262, 1854,...</td>\n",
       "      <td>I want to know what happened to the others.</td>\n",
       "      <td>[40, 765, 284, 760, 644, 3022, 284, 262, 1854,...</td>\n",
       "      <td>Oh, I'll take you with me.</td>\n",
       "      <td>[5812, 11, 314, 1183, 1011, 345, 351, 502, 13,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Are you sure?</td>\n",
       "      <td>[[8491, 345, 1654, 30, 50256]]</td>\n",
       "      <td>I have felt him, my Master.</td>\n",
       "      <td>[[40, 423, 2936, 683, 11, 616, 5599, 13, 50256]]</td>\n",
       "      <td>Yes, sir.</td>\n",
       "      <td>[5297, 11, 15967, 13, 50256]</td>\n",
       "      <td>Yes, sir.</td>\n",
       "      <td>[5297, 11, 15967, 13, 50256]</td>\n",
       "      <td>Are you sure?</td>\n",
       "      <td>[8491, 345, 1654, 30, 50256]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  ctx  \\\n",
       "0                               I will not fight you.   \n",
       "1     Unlock one-five-seven and nine.Release charges.   \n",
       "2         Lord Vader, what about Leia and theWookiee?   \n",
       "3                                                 No!   \n",
       "4   Give yourself to the dark side. It is the only...   \n",
       "5          Open the blast doors! Open theblast doors!   \n",
       "6   Strange, that I have not. I wonder if your fee...   \n",
       "7   Vader's targeting computer swings around into ...   \n",
       "8                          The Emperor's coming here?   \n",
       "9             The princess! Put all sections onalert!   \n",
       "10  Good.Use your aggressivefeelings, boy!Let the ...   \n",
       "11                     Surely he must be dead by now.   \n",
       "12                                 Shall I hold them?   \n",
       "13  What if he doesn't survive? He's worth a lot t...   \n",
       "14                  Then you must go to the Sanctuary   \n",
       "15                                      Are you sure?   \n",
       "\n",
       "                                               ctx_tk  \\\n",
       "0              [[40, 481, 407, 1907, 345, 13, 50256]]   \n",
       "1   [[3118, 5354, 530, 12, 13261, 12, 26548, 290, ...   \n",
       "2   [[22438, 27403, 11, 644, 546, 41212, 290, 262,...   \n",
       "3                                  [[2949, 0, 50256]]   \n",
       "4   [[23318, 3511, 284, 262, 3223, 1735, 13, 632, ...   \n",
       "5   [[11505, 262, 11975, 8215, 0, 4946, 262, 39806...   \n",
       "6   [[38114, 11, 326, 314, 423, 407, 13, 314, 4240...   \n",
       "7   [[53, 5067, 338, 10822, 3644, 26728, 1088, 656...   \n",
       "8           [[464, 10851, 338, 2406, 994, 30, 50256]]   \n",
       "9   [[464, 21752, 0, 5930, 477, 9004, 319, 44598, ...   \n",
       "10  [[10248, 13, 11041, 534, 8361, 36410, 654, 11,...   \n",
       "11  [[19457, 306, 339, 1276, 307, 2636, 416, 783, ...   \n",
       "12           [[2484, 439, 314, 1745, 606, 30, 50256]]   \n",
       "13  [[2061, 611, 339, 1595, 470, 7866, 30, 679, 33...   \n",
       "14   [[6423, 345, 1276, 467, 284, 262, 27036, 50256]]   \n",
       "15                     [[8491, 345, 1654, 30, 50256]]   \n",
       "\n",
       "                                                  lbl  \\\n",
       "0   Give yourself to the dark side. It is the only...   \n",
       "1                            Did you find any droids?   \n",
       "2               They must never again leave thiscity.   \n",
       "3   All to easy. Perhaps you are not as strong as ...   \n",
       "4   Sister! So...you have a twinsister. Your feeli...   \n",
       "5   I've been waiting for you, Obi-Wan. We meet ag...   \n",
       "6                          They are clear, my Master.   \n",
       "7                                     I have you now.   \n",
       "8   That is correct, Commander. And heis most disp...   \n",
       "9              Obi-Wan is here. The Force is withhim.   \n",
       "10                       Obi-Wan has taught you well.   \n",
       "11         Don't underestimate the power ofthe Force.   \n",
       "12                  No. Leave them to me. I will deal   \n",
       "13  The Empire will compensate you if he dies. Put...   \n",
       "14                                He will come to me?   \n",
       "15                        I have felt him, my Master.   \n",
       "\n",
       "                                               lbl_tk  \\\n",
       "0   [[23318, 3511, 284, 262, 3223, 1735, 13, 632, ...   \n",
       "1    [[11633, 345, 1064, 597, 3102, 2340, 30, 50256]]   \n",
       "2   [[2990, 1276, 1239, 757, 2666, 428, 19205, 13,...   \n",
       "3   [[3237, 284, 2562, 13, 8673, 345, 389, 407, 35...   \n",
       "4   [[50, 1694, 0, 1406, 986, 5832, 423, 257, 2034...   \n",
       "5   [[40, 1053, 587, 4953, 329, 345, 11, 46662, 12...   \n",
       "6       [[2990, 389, 1598, 11, 616, 5599, 13, 50256]]   \n",
       "7                    [[40, 423, 345, 783, 13, 50256]]   \n",
       "8   [[2504, 318, 3376, 11, 13353, 13, 843, 339, 27...   \n",
       "9   [[5944, 72, 12, 45681, 318, 994, 13, 383, 5221...   \n",
       "10  [[5944, 72, 12, 45681, 468, 7817, 345, 880, 13...   \n",
       "11  [[3987, 470, 34994, 262, 1176, 286, 1169, 5221...   \n",
       "12  [[2949, 13, 17446, 606, 284, 502, 13, 314, 481...   \n",
       "13  [[464, 8065, 481, 21392, 345, 611, 339, 10564,...   \n",
       "14           [[1544, 481, 1282, 284, 502, 30, 50256]]   \n",
       "15   [[40, 423, 2936, 683, 11, 616, 5599, 13, 50256]]   \n",
       "\n",
       "                                           prd_greedy  \\\n",
       "0                               I will not fight you.   \n",
       "1   Three, four,five,seven,seven,seven,seven,seven...   \n",
       "2                                       I don't know.   \n",
       "3   No!No!No!No!No!No!No!No!No!No!No!No!No!No!No!N...   \n",
       "4                                I have found my way.   \n",
       "5                                Open theblast doors!   \n",
       "6      I have not felt this way since I've been here.   \n",
       "7   Vader adjusts his control stick and adjusts hi...   \n",
       "8                          The Emperor's coming here!   \n",
       "9                               The princess is here!   \n",
       "10                                      I've felt it.   \n",
       "11                                         He's dead.   \n",
       "12                                      Yes, my lord.   \n",
       "13  If he survives this he will be the most powerf...   \n",
       "14        I want to know what happened to the others.   \n",
       "15                                          Yes, sir.   \n",
       "\n",
       "                                        prd_greedy_tk  \\\n",
       "0                [40, 481, 407, 1907, 345, 13, 50256]   \n",
       "1   [12510, 11, 1440, 11, 13261, 11, 26548, 11, 26...   \n",
       "2                      [40, 836, 470, 760, 13, 50256]   \n",
       "3   [2949, 0, 2949, 0, 2949, 0, 2949, 0, 2949, 0, ...   \n",
       "4                [40, 423, 1043, 616, 835, 13, 50256]   \n",
       "5                 [11505, 262, 39806, 8215, 0, 50256]   \n",
       "6   [40, 423, 407, 2936, 428, 835, 1201, 314, 1053...   \n",
       "7   [53, 5067, 46094, 465, 1630, 4859, 290, 46094,...   \n",
       "8              [464, 10851, 338, 2406, 994, 0, 50256]   \n",
       "9                    [464, 21752, 318, 994, 0, 50256]   \n",
       "10                   [40, 1053, 2936, 340, 13, 50256]   \n",
       "11                       [1544, 338, 2636, 13, 50256]   \n",
       "12                  [5297, 11, 616, 15876, 13, 50256]   \n",
       "13  [1532, 339, 36417, 428, 339, 481, 307, 262, 74...   \n",
       "14  [40, 765, 284, 760, 644, 3022, 284, 262, 1854,...   \n",
       "15                       [5297, 11, 15967, 13, 50256]   \n",
       "\n",
       "                                           prd_nbeams  \\\n",
       "0                               I will not fight you.   \n",
       "1   Three, four,five,seven,seven,seven,seven,seven...   \n",
       "2                                       I don't know.   \n",
       "3   No!No!No!No!No!No!No!No!No!No!No!No!No!No!No!N...   \n",
       "4                                I have found my way.   \n",
       "5                                Open theblast doors!   \n",
       "6      I have not felt this way since I've been here.   \n",
       "7   Vader adjusts his control stick and adjusts hi...   \n",
       "8                          The Emperor's coming here!   \n",
       "9                               The princess is here!   \n",
       "10                                      I've felt it.   \n",
       "11                                         He's dead.   \n",
       "12                                      Yes, my lord.   \n",
       "13  If he survives this he will be the most powerf...   \n",
       "14        I want to know what happened to the others.   \n",
       "15                                          Yes, sir.   \n",
       "\n",
       "                                        prd_nbeams_tk  \\\n",
       "0                [40, 481, 407, 1907, 345, 13, 50256]   \n",
       "1   [12510, 11, 1440, 11, 13261, 11, 26548, 11, 26...   \n",
       "2                      [40, 836, 470, 760, 13, 50256]   \n",
       "3   [2949, 0, 2949, 0, 2949, 0, 2949, 0, 2949, 0, ...   \n",
       "4                [40, 423, 1043, 616, 835, 13, 50256]   \n",
       "5                 [11505, 262, 39806, 8215, 0, 50256]   \n",
       "6   [40, 423, 407, 2936, 428, 835, 1201, 314, 1053...   \n",
       "7   [53, 5067, 46094, 465, 1630, 4859, 290, 46094,...   \n",
       "8              [464, 10851, 338, 2406, 994, 0, 50256]   \n",
       "9                    [464, 21752, 318, 994, 0, 50256]   \n",
       "10                   [40, 1053, 2936, 340, 13, 50256]   \n",
       "11                       [1544, 338, 2636, 13, 50256]   \n",
       "12                  [5297, 11, 616, 15876, 13, 50256]   \n",
       "13  [1532, 339, 36417, 428, 339, 481, 307, 262, 74...   \n",
       "14  [40, 765, 284, 760, 644, 3022, 284, 262, 1854,...   \n",
       "15                       [5297, 11, 15967, 13, 50256]   \n",
       "\n",
       "                                         prd_sampling  \\\n",
       "0                Prepare your ships, Imperial Master.   \n",
       "1   ... a disturbance in theForce, a disturbance i...   \n",
       "2                                       They're dead.   \n",
       "3          Oh, my Lord, why, sir!No, sir, no!Only me!   \n",
       "4   I've completed the training. You can't leave t...   \n",
       "5    We're trying to land on target, don't get crazy.   \n",
       "6   You know what? I'll make it clear. Lord Vader,...   \n",
       "7   The Emperor stands on the platform as the lase...   \n",
       "8                      The Senate will not be denied.   \n",
       "9   It was the last known location of the Rebellio...   \n",
       "10                     Yes, my lord. The end is near.   \n",
       "11                                        Lord Vader!   \n",
       "12  It would be much better, Lord Vader, if we cou...   \n",
       "13                                     If he survives   \n",
       "14                         Oh, I'll take you with me.   \n",
       "15                                      Are you sure?   \n",
       "\n",
       "                                      prd_sampling_tk  \n",
       "0   [37534, 533, 534, 7937, 11, 11773, 5599, 13, 5...  \n",
       "1   [986, 257, 30497, 287, 262, 10292, 11, 257, 30...  \n",
       "2                        [2990, 821, 2636, 13, 50256]  \n",
       "3   [5812, 11, 616, 4453, 11, 1521, 11, 15967, 0, ...  \n",
       "4   [40, 1053, 5668, 262, 3047, 13, 921, 460, 470,...  \n",
       "5   [1135, 821, 2111, 284, 1956, 319, 2496, 11, 83...  \n",
       "6   [1639, 760, 644, 30, 314, 1183, 787, 340, 1598...  \n",
       "7   [464, 10851, 6296, 319, 262, 3859, 355, 262, 1...  \n",
       "8         [464, 3845, 481, 407, 307, 6699, 13, 50256]  \n",
       "9   [1026, 373, 262, 938, 1900, 4067, 286, 262, 34...  \n",
       "10  [5297, 11, 616, 15876, 13, 383, 886, 318, 1474...  \n",
       "11                           [22438, 27403, 0, 50256]  \n",
       "12  [1026, 561, 307, 881, 1365, 11, 4453, 27403, 1...  \n",
       "13                          [1532, 339, 36417, 50256]  \n",
       "14  [5812, 11, 314, 1183, 1011, 345, 351, 502, 13,...  \n",
       "15                       [8491, 345, 1654, 30, 50256]  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_char = get_dataframe_for_metrics(character_hg['test'], predictions_greedy, predictions_nbeams, predictions_sampling)\n",
    "df_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics For Character 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ccl_sim(ctx_lbl, ctx_cht, lbl_cht):\n",
    "    return ((1 - abs(ctx_lbl - ctx_cht))**2 + lbl_cht**2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Lib.BBMetrics import BBMetric\n",
    "\n",
    "def compute_set_metrics(model, model_2, character, character_2, test_set_name,\n",
    "                        context_sentences, label_responses, chatbot_responses, encoded_test_set,\n",
    "                        classifier_n_sentences=50, label_chatbot_symmetry=False,\n",
    "                        include_qualitative_sentences=False, verbose=True):\n",
    "    scores = {}\n",
    "    \n",
    "    lbl_text = 'label' if not label_chatbot_symmetry else 'chatbota'\n",
    "    cht_text = 'chatbot' if not label_chatbot_symmetry else 'chatbotb'\n",
    "    \n",
    "    scores['metadata'] = {}\n",
    "    scores['metadata']['dataset name'] = test_set_name\n",
    "    scores['metadata']['names'] = {\n",
    "        'context':'context'\n",
    "    }\n",
    "    if label_chatbot_symmetry:\n",
    "        scores['metadata']['names'][lbl_text] = character\n",
    "        scores['metadata']['names'][cht_text] = character_2\n",
    "    else:\n",
    "        scores['metadata']['names'][lbl_text] = 'label'\n",
    "        scores['metadata']['names'][cht_text] = character\n",
    "    \n",
    "    # 0) computes metrics for perplexity\n",
    "    metric = BBMetric.load_metric(\"semantic similarity\")\n",
    "    scores['semantic similarity'] = [metric.compute(sentences_a=context_sentences,\n",
    "                                            sentences_b=label_responses)]\n",
    "    scores['semantic similarity'].append(metric.compute(sentences_a=context_sentences,\n",
    "                                            sentences_b=chatbot_responses)),\n",
    "    scores['semantic similarity'].append(metric.compute(sentences_a=label_responses,\n",
    "                                              sentences_b=chatbot_responses))\n",
    "    scores['semantic similarity'].append(ccl_sim(scores['semantic similarity'][0]['score'],\n",
    "                                                 scores['semantic similarity'][1]['score'],\n",
    "                                                 scores['semantic similarity'][2]['score']))\n",
    "    scores['metadata']['semantic similarity'] = {\n",
    "        'ordering': ['context-'+lbl_text, 'context-'+cht_text, cht_text+'-'+lbl_text, 'ccl']\n",
    "    }\n",
    "    if verbose:\n",
    "        print('=== SEMANTIC SIMILARITY ===')\n",
    "        print('context-'+lbl_text+' similarity:   ', scores['semantic similarity'][0])\n",
    "        print('context-'+cht_text+' similarity: ', scores['semantic similarity'][1])\n",
    "        print(cht_text+'-'+lbl_text+' similarity:   ', scores['semantic similarity'][2])\n",
    "        print('ccl-sim similarity:            ', scores['semantic similarity'][3])\n",
    "    # 1) computes metrics for perplexity\n",
    "    if encoded_test_set is not None:\n",
    "        metric = BBMetric.load_metric(\"perplexity\")\n",
    "        if not label_chatbot_symmetry:\n",
    "            scores['perplexity'] = metric.compute(model=model, encoded_test_set=encoded_test_set)['score']\n",
    "            scores['metadata']['perplexity'] = {\n",
    "                'ordering': cht_text\n",
    "            }\n",
    "        else:\n",
    "            scores['perplexity'] = [metric.compute(model=model, encoded_test_set=encoded_test_set)['score']]\n",
    "            scores['perplexity'].append(metric.compute(model=model_2, encoded_test_set=encoded_test_set)['score'])\n",
    "            scores['metadata']['perplexity'] = {\n",
    "                'ordering': [lbl_text, cht_text]\n",
    "            }\n",
    "        if verbose:\n",
    "            print('===       PERPLEXITY     ===')\n",
    "            if label_chatbot_symmetry:\n",
    "                print(lbl_text + ' perplexity:         ', scores['perplexity'][0])\n",
    "                print(cht_text + ' perplexity:         ', scores['perplexity'][1])\n",
    "            else:\n",
    "                print(cht_text + ' perplexity:         ', scores['perplexity'])\n",
    "    elif verbose:\n",
    "        print(\"encoded_test_set not provided, skipping Perplexity.\")\n",
    "    # 2) computes metrics for bleu\n",
    "    metric = BBMetric.load_metric(\"bleu\")\n",
    "    scores['bleu'] = [metric.compute(predictions=label_responses, references=context_sentences)]\n",
    "    scores['bleu'].append(metric.compute(predictions=chatbot_responses, references=context_sentences))\n",
    "    scores['bleu'].append(metric.compute(predictions=chatbot_responses, references=label_responses))\n",
    "    scores['bleu'].append(ccl_sim(scores['bleu'][0]['score'],\n",
    "                                  scores['bleu'][1]['score'],\n",
    "                                  scores['bleu'][2]['score']))\n",
    "    scores['metadata']['bleu'] = {\n",
    "        'ordering': ['context-'+lbl_text, 'context-'+cht_text, cht_text+'-'+lbl_text, 'ccl']\n",
    "    }\n",
    "    if verbose:\n",
    "        print('===         BLEU         ===')\n",
    "        print('context-to-'+lbl_text+' bleu:      ', scores['bleu'][0])\n",
    "        print('context-to-'+cht_text+' bleu:    ', scores['bleu'][1])\n",
    "        print(lbl_text+'-to-'+cht_text+' bleu:      ', scores['bleu'][2])\n",
    "        print('ccl-sim bleu:            ', scores['bleu'][3])\n",
    "    # 3) computes metrics for rouge-L\n",
    "    metric = BBMetric.load_metric(\"rouge l\")\n",
    "    scores['rouge l'] = [metric.compute(predictions=label_responses, references=context_sentences)]\n",
    "    scores['rouge l'].append(metric.compute(predictions=chatbot_responses, references=context_sentences))\n",
    "    scores['rouge l'].append(metric.compute(predictions=chatbot_responses, references=label_responses))\n",
    "    scores['rouge l'].append(ccl_sim(scores['rouge l'][0]['score'],\n",
    "                                     scores['rouge l'][1]['score'],\n",
    "                                     scores['rouge l'][2]['score']))\n",
    "    scores['metadata']['rouge l'] = {\n",
    "        'ordering': ['context-'+lbl_text, 'context-'+cht_text, cht_text+'-'+lbl_text, 'ccl']\n",
    "    }\n",
    "    if verbose:\n",
    "        print('===        ROUGE-L       ===')\n",
    "        print('context-to-'+lbl_text+' rouge:     ', scores['rouge l'][0])\n",
    "        print('context-to-'+cht_text+' rouge:   ', scores['rouge l'][1])\n",
    "        print(lbl_text+'-to-'+cht_text+' rouge:     ', scores['rouge l'][2])\n",
    "        print('ccl-sim rouge:            ', scores['rouge l'][3])\n",
    "    # 4) computes metrics for distinct\n",
    "    metric = BBMetric.load_metric(\"distinct\")\n",
    "    scores['distinct'] = [metric.compute(sentences=context_sentences)]\n",
    "    scores['distinct'].append(metric.compute(sentences=label_responses))\n",
    "    scores['distinct'].append(metric.compute(sentences=chatbot_responses))\n",
    "    scores['metadata']['distinct'] = {\n",
    "        'ordering': ['context', lbl_text, cht_text]\n",
    "    }\n",
    "    if verbose:\n",
    "        print('===       DISTINCT      ===')\n",
    "        print('context distinct:          ', scores['distinct'][0])\n",
    "        print(lbl_text+' distinct:          ', scores['distinct'][1])\n",
    "        print(cht_text+' distinct:          ', scores['distinct'][2])\n",
    "        \n",
    "    # 6) computes emotion metric\n",
    "    metric = BBMetric.load_metric(\"emotion\")\n",
    "    scores['emotion'] = [metric.compute(sentences=context_sentences)]\n",
    "    scores['emotion'].append(metric.compute(sentences=label_responses))\n",
    "    scores['emotion'].append(metric.compute(sentences=chatbot_responses))\n",
    "    scores['emotion'].append(sp.stats.stats.pearsonr(scores['emotion'][1]['score'],\n",
    "                                                     scores['emotion'][2]['score'])[0])\n",
    "    scores['metadata']['emotion'] = {\n",
    "        'ordering': ['context-'+lbl_text, 'context-'+cht_text, cht_text+'-'+lbl_text, cht_text+'-'+lbl_text+' correlation']\n",
    "    }\n",
    "    if verbose:\n",
    "        print('===       EMOTION       ===')\n",
    "        print('context emotions:            \\n', list(zip(scores['emotion'][0]['label'], scores['emotion'][0]['score'])))\n",
    "        print(lbl_text+' emotions:              \\n', list(zip(scores['emotion'][1]['label'], scores['emotion'][1]['score'])))\n",
    "        print(cht_text+' emotions:            \\n', list(zip(scores['emotion'][2]['label'], scores['emotion'][2]['score'])))\n",
    "        print(lbl_text+'-'+cht_text+'emotion corr:  \\n', scores['emotion'][3])\n",
    "    # 8) computes sas metric\n",
    "    metric = BBMetric.load_metric(\"semantic answer similarity\")\n",
    "    scores['semantic answer similarity'] = [metric.compute(predictions=context_sentences,\n",
    "                                                    references=label_responses)]\n",
    "    scores['semantic answer similarity'].append(metric.compute(predictions=context_sentences,\n",
    "                                                        references=chatbot_responses))\n",
    "    scores['semantic answer similarity'].append(metric.compute(predictions=label_responses,\n",
    "                                                        references=chatbot_responses))\n",
    "    scores['semantic answer similarity'].append(ccl_sim(scores['semantic answer similarity'][0]['score'],\n",
    "                                                        scores['semantic answer similarity'][1]['score'],\n",
    "                                                        scores['semantic answer similarity'][2]['score']))\n",
    "    scores['metadata']['semantic answer similarity'] = {\n",
    "        'ordering': ['context-'+lbl_text, 'context-'+cht_text, cht_text+'-'+lbl_text, 'ccl']\n",
    "    }\n",
    "    if verbose:\n",
    "        print('===         SAS         ===')\n",
    "        print('context-'+lbl_text+' sas:          ', scores['semantic answer similarity'][0])\n",
    "        print('context-'+cht_text+' sas:        ', scores['semantic answer similarity'][1])\n",
    "        print(lbl_text+'-'+cht_text+' sas:          ', scores['semantic answer similarity'][2])\n",
    "        print('ccl-sim sas:               ', scores['semantic answer similarity'][3])\n",
    "    # 9) computes metrics for semantic classifier\n",
    "    metric = BBMetric.load_metric(\"semantic classifier\")\n",
    "    start_time = time.time()\n",
    "    scores['semantic classifier'] = [metric.compute(character=character, character_dict=character_dict, \n",
    "                                                   base_folder=base_folder, sentences=label_responses,\n",
    "                                                   n_sentences=classifier_n_sentences)]\n",
    "    scores['semantic classifier'].append(metric.compute(character=character, character_dict=character_dict, \n",
    "                                                   base_folder=base_folder, sentences=chatbot_responses,\n",
    "                                                   n_sentences=classifier_n_sentences))\n",
    "    end_time = time.time()\n",
    "    scores['metadata']['semantic classifier'] = {\n",
    "        'ordering': [lbl_text, cht_text]\n",
    "    }\n",
    "    if verbose:\n",
    "        print('=== SEMANTIC CLASSIFIER ===')\n",
    "        print('sem-classifier '+lbl_text+':                ', scores['semantic classifier'][0])\n",
    "        print('sem-classifier '+cht_text+':                  ', scores['semantic classifier'][1])\n",
    "        print('time elapsed computing semantic classifier:  {:.2f} s'.format(end_time - start_time))\n",
    "    if not label_chatbot_symmetry and os.path.exists(os.path.join(os.getcwd(), \"Data\", \"Characters\", character, \"humancoherence.csv\")):\n",
    "        scores['human'] = {}\n",
    "        metric = BBMetric.load_metric(\"human - coherence\")\n",
    "        scores['human']['coherence'] = metric.compute(filepath=os.path.join(os.getcwd(), \"Data\", \"Characters\",\n",
    "                                                                            character, \"humancoherence.csv\"))\n",
    "        metric = BBMetric.load_metric(\"human - style\")\n",
    "        scores['human']['style'] = metric.compute(filepath=os.path.join(os.getcwd(), \"Data\", \"Characters\",\n",
    "                                                                        character, \"humanstyle.csv\"))\n",
    "        metric = BBMetric.load_metric(\"human - consistency\")\n",
    "        scores['human']['consistency'] = metric.compute(filepath=os.path.join(os.getcwd(), \"Data\", \"Characters\",\n",
    "                                                                              character, \"humanconsistency.csv\"))\n",
    "        scores['metadata']['human'] = {\n",
    "            'ordering': {\n",
    "                'coherence': cht_text,\n",
    "                'consistency': cht_text,\n",
    "                'style': cht_text\n",
    "            }\n",
    "        }\n",
    "        if verbose:\n",
    "            print('===    HUMAN METRICS    ===')\n",
    "            print('coherence:                 ', scores['human']['coherence'])\n",
    "            print('consistency:               ', scores['human']['consistency'])\n",
    "            print('style:                     ', scores['human']['style'])\n",
    "    elif verbose:\n",
    "        print(\"Symmetric mode, skipping Human metrics.\")\n",
    "    if include_qualitative_sentences:\n",
    "        sentences_df = {}\n",
    "        sentences_df['context'] = context_sentences\n",
    "        sentences_df[lbl_text] = label_responses\n",
    "        sentences_df[cht_text] = chatbot_responses\n",
    "        scores['sentences'] = sentences_df\n",
    "        if verbose:\n",
    "            print('===      SENTENCES      ===')\n",
    "            for i in range(len(context_sentences)):\n",
    "                print(\"* context: \", context_sentences[i])\n",
    "                print(\"* \" + lbl_text + \":\", label_responses[i])\n",
    "                print(\"* \" + cht_text + \":\", chatbot_responses[i])\n",
    "                print()\n",
    "    elif verbose:\n",
    "        print(\"Skipping sentence outputting.\")\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nset_size = 10\\ni = 30\\nprint(\"##### Set (Size \" + str(set_size) + \") #####\")\\ncontext_sentences = list(df_char[\\'ctx\\'][i:i+set_size])\\nchatbot_responses = list(df_char[\\'prd_greedy\\'][i:i+set_size])\\nlabel_responses   = list(df_char[\\'lbl\\'][i:i+set_size])\\ncompute_set_metrics(model, None,\\n                    context_sentences, label_responses, chatbot_responses, character, encoded_test_set)\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "set_size = 10\n",
    "i = 30\n",
    "print(\"##### Set (Size \" + str(set_size) + \") #####\")\n",
    "context_sentences = list(df_char['ctx'][i:i+set_size])\n",
    "chatbot_responses = list(df_char['prd_greedy'][i:i+set_size])\n",
    "label_responses   = list(df_char['lbl'][i:i+set_size])\n",
    "compute_set_metrics(model, None,\n",
    "                    context_sentences, label_responses, chatbot_responses, character, encoded_test_set)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Full Test Set #####\n",
      "=== SEMANTIC SIMILARITY ===\n",
      "context-label similarity:    {'score': 0.2628566026687622, 'std': 0.12330655008554459}\n",
      "context-chatbot similarity:  {'score': 0.5519678592681885, 'std': 0.2759278416633606}\n",
      "chatbot-label similarity:    {'score': 0.2198188602924347, 'std': 0.15027783811092377}\n",
      "ccl-sim similarity:             0.27684156841695584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:05<00:00,  2.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===       PERPLEXITY     ===\n",
      "chatbot perplexity:          20.615766981008772\n",
      "===         BLEU         ===\n",
      "context-to-label bleu:       {'score': 0.0}\n",
      "context-to-chatbot bleu:     {'score': 0.0752434444398826}\n",
      "label-to-chatbot bleu:       {'score': 0.0}\n",
      "ccl-sim bleu:             0.4275873435257062\n",
      "===        ROUGE-L       ===\n",
      "context-to-label rouge:      {'score': 0.07925918737060042, 'std': 0.07479345524211507}\n",
      "context-to-chatbot rouge:    {'score': 0.2783069336058467, 'std': 0.32252310998200084}\n",
      "label-to-chatbot rouge:      {'score': 0.05853002070393374, 'std': 0.0998893999786502}\n",
      "ccl-sim rouge:             0.3224751380672207\n",
      "===       DISTINCT      ===\n",
      "context distinct:           {'score': -1.8850444702591331, 'std': 0.04780861208267612}\n",
      "label distinct:           {'score': -1.860529383989936, 'std': 0.02676100873863563}\n",
      "chatbot distinct:           {'score': -1.9147244626716424, 'std': 0.061163652368743125}\n",
      "===       EMOTION       ===\n",
      "context emotions:            \n",
      " [('sadness', 0.03771194361979724), ('joy', 0.3086916868633125), ('love', 0.0028261127226869576), ('anger', 0.46144579228712246), ('fear', 0.17504403311977512), ('surprise', 0.01428043059331685)]\n",
      "label emotions:              \n",
      " [('sadness', 0.09321644622832537), ('joy', 0.4917986299897166), ('love', 0.005095473637993564), ('anger', 0.32870645810180577), ('fear', 0.07919873658102006), ('surprise', 0.0019842915035042097)]\n",
      "chatbot emotions:            \n",
      " [('sadness', 0.11372547360952012), ('joy', 0.3822097205265891), ('love', 0.03265330982321757), ('anger', 0.358829681732459), ('fear', 0.10793059322895715), ('surprise', 0.004651219400329865)]\n",
      "label-chatbotemotion corr:  \n",
      " 0.9735187569110957\n",
      "===         SAS         ===\n",
      "context-label sas:           {'score': 0.06030791625380516, 'std': 0.08577671647071838}\n",
      "context-chatbot sas:         {'score': 0.43145668506622314, 'std': 0.3515917956829071}\n",
      "label-chatbot sas:           {'score': 0.0975886881351471, 'std': 0.15125875174999237}\n",
      "ccl-sim sas:                0.20248871150903835\n",
      "=== SEMANTIC CLASSIFIER ===\n",
      "sem-classifier label:                 {'score': 0.9999648332595825, 'std': 1.0704990017984528e-05}\n",
      "sem-classifier chatbot:                   {'score': 0.4008733034133911, 'std': 0.4590854346752167}\n",
      "time elapsed computing semantic classifier:  11.78 s\n",
      "===    HUMAN METRICS    ===\n",
      "coherence:                  {'score': 0.26666666666666666, 'std': 0.3771236166328254}\n",
      "consistency:                {'score': 0.06666666666666667, 'std': 0.09428090415820635}\n",
      "style:                      {'score': 0.13333333333333333, 'std': 0.09428090415820635}\n",
      "Skipping sentence outputting.\n"
     ]
    }
   ],
   "source": [
    "print(\"##### Full Test Set #####\")\n",
    "context_sentences = list(df_char['ctx'])\n",
    "chatbot_responses = list(df_char['prd_greedy'])\n",
    "label_responses   = list(df_char['lbl'])\n",
    "scores = compute_set_metrics(model, None,\n",
    "                             character, None, character + \" dataset\",\n",
    "                             context_sentences, label_responses, chatbot_responses, encoded_test_set,\n",
    "                             classifier_n_sentences=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_json(metrics_folder, character+'_base_metrics', scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Between Different Sampling Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Greedy vs. N-Beams #####\n",
      "=== SEMANTIC SIMILARITY ===\n",
      "context-chatbota similarity:    {'score': 0.5519678592681885, 'std': 0.2759278416633606}\n",
      "context-chatbotb similarity:  {'score': 0.5519678592681885, 'std': 0.2759278416633606}\n",
      "chatbotb-chatbota similarity:    {'score': 1.0, 'std': 1.8066697293761536e-07}\n",
      "ccl-sim similarity:             1.0\n",
      "encoded_test_set not provided, skipping Perplexity.\n",
      "===         BLEU         ===\n",
      "context-to-chatbota bleu:       {'score': 0.0752434444398826}\n",
      "context-to-chatbotb bleu:     {'score': 0.0752434444398826}\n",
      "chatbota-to-chatbotb bleu:       {'score': 1.0}\n",
      "ccl-sim bleu:             1.0\n",
      "===        ROUGE-L       ===\n",
      "context-to-chatbota rouge:      {'score': 0.2783069336058467, 'std': 0.32252310998200084}\n",
      "context-to-chatbotb rouge:    {'score': 0.2783069336058467, 'std': 0.32252310998200084}\n",
      "chatbota-to-chatbotb rouge:      {'score': 1.0, 'std': 0.0}\n",
      "ccl-sim rouge:             1.0\n",
      "===       DISTINCT      ===\n",
      "context distinct:           {'score': -1.8850444702591331, 'std': 0.04780861208267612}\n",
      "chatbota distinct:           {'score': -1.9147244626716424, 'std': 0.061163652368743125}\n",
      "chatbotb distinct:           {'score': -1.9147244626716424, 'std': 0.061163652368743125}\n",
      "===       EMOTION       ===\n",
      "context emotions:            \n",
      " [('sadness', 0.03771194361979724), ('joy', 0.3086916868633125), ('love', 0.0028261127226869576), ('anger', 0.46144579228712246), ('fear', 0.17504403311977512), ('surprise', 0.01428043059331685)]\n",
      "chatbota emotions:              \n",
      " [('sadness', 0.11372547360952012), ('joy', 0.3822097205265891), ('love', 0.03265330982321757), ('anger', 0.358829681732459), ('fear', 0.10793059322895715), ('surprise', 0.004651219400329865)]\n",
      "chatbotb emotions:            \n",
      " [('sadness', 0.11372547360952012), ('joy', 0.3822097205265891), ('love', 0.03265330982321757), ('anger', 0.358829681732459), ('fear', 0.10793059322895715), ('surprise', 0.004651219400329865)]\n",
      "chatbota-chatbotbemotion corr:  \n",
      " 0.9999999999999998\n",
      "===         SAS         ===\n",
      "context-chatbota sas:           {'score': 0.43145668506622314, 'std': 0.3515917956829071}\n",
      "context-chatbotb sas:         {'score': 0.43145668506622314, 'std': 0.3515917956829071}\n",
      "chatbota-chatbotb sas:           {'score': 0.9619050025939941, 'std': 0.011696180328726768}\n",
      "ccl-sim sas:                0.9626306170076759\n",
      "=== SEMANTIC CLASSIFIER ===\n",
      "sem-classifier chatbota:                 {'score': 0.4008733034133911, 'std': 0.4590854346752167}\n",
      "sem-classifier chatbotb:                   {'score': 0.4008733034133911, 'std': 0.4590854346752167}\n",
      "time elapsed computing semantic classifier:  10.67 s\n",
      "Symmetric mode, skipping Human metrics.\n",
      "Skipping sentence outputting.\n"
     ]
    }
   ],
   "source": [
    "print(\"##### Greedy vs. N-Beams #####\")\n",
    "context_sentences = list(df_char['ctx'])\n",
    "greedy_responses  = list(df_char['prd_greedy'])\n",
    "nbeams_responses  = list(df_char['prd_nbeams'])\n",
    "scores['greedy_vs_nbeams'] = compute_set_metrics(None, None,\n",
    "                                                 character, character, character + \" dataset\",\n",
    "                                                 context_sentences,\n",
    "                                                 greedy_responses,\n",
    "                                                 nbeams_responses,\n",
    "                                                 None,\n",
    "                                                 classifier_n_sentences=75, label_chatbot_symmetry=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split == True:\n",
    "    save_as_json(metrics_folder, character+'_greedy_vs_nbeams_metrics', scores['greedy_vs_nbeams'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Greedy vs. Sampling #####\n",
      "=== SEMANTIC SIMILARITY ===\n",
      "context-chatbota similarity:    {'score': 0.5519678592681885, 'std': 0.2759278416633606}\n",
      "context-chatbotb similarity:  {'score': 0.3545387387275696, 'std': 0.21952050924301147}\n",
      "chatbotb-chatbota similarity:    {'score': 0.3163492679595947, 'std': 0.1475856751203537}\n",
      "ccl-sim similarity:             0.37209843794738795\n",
      "encoded_test_set not provided, skipping Perplexity.\n",
      "===         BLEU         ===\n",
      "context-to-chatbota bleu:       {'score': 0.0752434444398826}\n",
      "context-to-chatbotb bleu:     {'score': 0.0}\n",
      "chatbota-to-chatbotb bleu:       {'score': 0.0}\n",
      "ccl-sim bleu:             0.4275873435257062\n",
      "===        ROUGE-L       ===\n",
      "context-to-chatbota rouge:      {'score': 0.2783069336058467, 'std': 0.32252310998200084}\n",
      "context-to-chatbotb rouge:    {'score': 0.14650645744573537, 'std': 0.23855587781933377}\n",
      "chatbota-to-chatbotb rouge:      {'score': 0.06841869026172095, 'std': 0.09385208013151991}\n",
      "ccl-sim rouge:             0.3792257651864694\n",
      "===       DISTINCT      ===\n",
      "context distinct:           {'score': -1.8850444702591331, 'std': 0.04780861208267612}\n",
      "chatbota distinct:           {'score': -1.9147244626716424, 'std': 0.061163652368743125}\n",
      "chatbotb distinct:           {'score': -1.8858537217912217, 'std': 0.05431507639059038}\n",
      "===       EMOTION       ===\n",
      "context emotions:            \n",
      " [('sadness', 0.03771194361979724), ('joy', 0.3086916868633125), ('love', 0.0028261127226869576), ('anger', 0.46144579228712246), ('fear', 0.17504403311977512), ('surprise', 0.01428043059331685)]\n",
      "chatbota emotions:              \n",
      " [('sadness', 0.11372547360952012), ('joy', 0.3822097205265891), ('love', 0.03265330982321757), ('anger', 0.358829681732459), ('fear', 0.10793059322895715), ('surprise', 0.004651219400329865)]\n",
      "chatbotb emotions:            \n",
      " [('sadness', 0.07943609881112934), ('joy', 0.4079613431240432), ('love', 0.0034159159113187343), ('anger', 0.3681901668314822), ('fear', 0.13854427893602406), ('surprise', 0.0024521731611457653)]\n",
      "chatbota-chatbotbemotion corr:  \n",
      " 0.9915373910890306\n",
      "===         SAS         ===\n",
      "context-chatbota sas:           {'score': 0.43145668506622314, 'std': 0.3515917956829071}\n",
      "context-chatbotb sas:         {'score': 0.15687566995620728, 'std': 0.24979953467845917}\n",
      "chatbota-chatbotb sas:           {'score': 0.1758153885602951, 'std': 0.26951858401298523}\n",
      "ccl-sim sas:                0.2785718772467113\n",
      "=== SEMANTIC CLASSIFIER ===\n",
      "sem-classifier chatbota:                 {'score': 0.4008733034133911, 'std': 0.4590854346752167}\n",
      "sem-classifier chatbotb:                   {'score': 0.6420408487319946, 'std': 0.44049662351608276}\n",
      "time elapsed computing semantic classifier:  11.52 s\n",
      "Symmetric mode, skipping Human metrics.\n",
      "Skipping sentence outputting.\n"
     ]
    }
   ],
   "source": [
    "print(\"##### Greedy vs. Sampling #####\")\n",
    "context_sentences   = list(df_char['ctx'])\n",
    "greedy_responses    = list(df_char['prd_greedy'])\n",
    "sampling_responses  = list(df_char['prd_sampling'])\n",
    "scores['greedy_vs_sampling'] = compute_set_metrics(None, None,\n",
    "                                                   character, character, character + \" dataset\",\n",
    "                                                   context_sentences,\n",
    "                                                   greedy_responses,\n",
    "                                                   sampling_responses,\n",
    "                                                   None,\n",
    "                                                   classifier_n_sentences=75, label_chatbot_symmetry=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split == True:\n",
    "    save_as_json(metrics_folder, character+'_greedy_vs_sampling_metrics', scores['greedy_vs_sampling'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### N-Beams vs. Sampling #####\n",
      "=== SEMANTIC SIMILARITY ===\n",
      "context-chatbota similarity:    {'score': 0.5519678592681885, 'std': 0.2759278416633606}\n",
      "context-chatbotb similarity:  {'score': 0.3545387387275696, 'std': 0.21952050924301147}\n",
      "chatbotb-chatbota similarity:    {'score': 0.3163492679595947, 'std': 0.1475856751203537}\n",
      "ccl-sim similarity:             0.37209843794738795\n",
      "encoded_test_set not provided, skipping Perplexity.\n",
      "===         BLEU         ===\n",
      "context-to-chatbota bleu:       {'score': 0.0752434444398826}\n",
      "context-to-chatbotb bleu:     {'score': 0.0}\n",
      "chatbota-to-chatbotb bleu:       {'score': 0.0}\n",
      "ccl-sim bleu:             0.4275873435257062\n",
      "===        ROUGE-L       ===\n",
      "context-to-chatbota rouge:      {'score': 0.2783069336058467, 'std': 0.32252310998200084}\n",
      "context-to-chatbotb rouge:    {'score': 0.14650645744573537, 'std': 0.23855587781933377}\n",
      "chatbota-to-chatbotb rouge:      {'score': 0.06841869026172095, 'std': 0.09385208013151991}\n",
      "ccl-sim rouge:             0.3792257651864694\n",
      "===       DISTINCT      ===\n",
      "context distinct:           {'score': -1.8850444702591331, 'std': 0.04780861208267612}\n",
      "chatbota distinct:           {'score': -1.9147244626716424, 'std': 0.061163652368743125}\n",
      "chatbotb distinct:           {'score': -1.8858537217912217, 'std': 0.05431507639059038}\n",
      "===       EMOTION       ===\n",
      "context emotions:            \n",
      " [('sadness', 0.03771194361979724), ('joy', 0.3086916868633125), ('love', 0.0028261127226869576), ('anger', 0.46144579228712246), ('fear', 0.17504403311977512), ('surprise', 0.01428043059331685)]\n",
      "chatbota emotions:              \n",
      " [('sadness', 0.11372547360952012), ('joy', 0.3822097205265891), ('love', 0.03265330982321757), ('anger', 0.358829681732459), ('fear', 0.10793059322895715), ('surprise', 0.004651219400329865)]\n",
      "chatbotb emotions:            \n",
      " [('sadness', 0.07943609881112934), ('joy', 0.4079613431240432), ('love', 0.0034159159113187343), ('anger', 0.3681901668314822), ('fear', 0.13854427893602406), ('surprise', 0.0024521731611457653)]\n",
      "chatbota-chatbotbemotion corr:  \n",
      " 0.9915373910890306\n",
      "===         SAS         ===\n",
      "context-chatbota sas:           {'score': 0.43145668506622314, 'std': 0.3515917956829071}\n",
      "context-chatbotb sas:         {'score': 0.15687566995620728, 'std': 0.24979953467845917}\n",
      "chatbota-chatbotb sas:           {'score': 0.1758153885602951, 'std': 0.26951858401298523}\n",
      "ccl-sim sas:                0.2785718772467113\n",
      "=== SEMANTIC CLASSIFIER ===\n",
      "sem-classifier chatbota:                 {'score': 0.4008733034133911, 'std': 0.4590854346752167}\n",
      "sem-classifier chatbotb:                   {'score': 0.6420408487319946, 'std': 0.44049662351608276}\n",
      "time elapsed computing semantic classifier:  11.31 s\n",
      "Symmetric mode, skipping Human metrics.\n",
      "Skipping sentence outputting.\n"
     ]
    }
   ],
   "source": [
    "print(\"##### N-Beams vs. Sampling #####\")\n",
    "context_sentences   = list(df_char['ctx'])\n",
    "nbeams_responses    = list(df_char['prd_nbeams'])\n",
    "sampling_responses  = list(df_char['prd_sampling'])\n",
    "scores['nbeams_vs_sampling'] = compute_set_metrics(None, None,\n",
    "                                                   character, character, character + \" dataset\",\n",
    "                                                   context_sentences,\n",
    "                                                   nbeams_responses,\n",
    "                                                   sampling_responses,\n",
    "                                                   None,\n",
    "                                                   classifier_n_sentences=75, label_chatbot_symmetry=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split == True:\n",
    "    save_as_json(metrics_folder, character+'_nbeams_vs_sampling_metrics', scores['nbeams_vs_sampling'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split == True:    \n",
    "    scores = {}\n",
    "    scores['greedy_vs_nbeams'] = load_from_json(\n",
    "        filepath=metrics_folder,\n",
    "        filename=character+'_greedy_vs_nbeams_metrics'\n",
    "    )\n",
    "    scores['greedy_vs_sampling'] = load_from_json(\n",
    "        filepath=metrics_folder,\n",
    "        filename=character+'_greedy_vs_sampling_metrics'\n",
    "    )\n",
    "    scores['nbeams_vs_sampling'] = load_from_json(\n",
    "        filepath=metrics_folder,\n",
    "        filename=character+'_nbeams_vs_sampling_metrics'\n",
    "    )\n",
    "    \n",
    "    os.remove(os.path.join(\n",
    "        metrics_folder,\n",
    "        character+'_greedy_vs_nbeams_metrics.json'\n",
    "    ))\n",
    "    os.remove(os.path.join(\n",
    "        metrics_folder,\n",
    "        character+'_greedy_vs_sampling_metrics.json'\n",
    "    ))\n",
    "    os.remove(os.path.join(\n",
    "        metrics_folder,\n",
    "        character+'_nbeams_vs_sampling_metrics.json'\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_json(metrics_folder, character+'_sampling_comparison_metrics', scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Between Character vs Non-Finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading predictions from stored file\n",
      "Loaded predictions from stored file\n"
     ]
    }
   ],
   "source": [
    "predictions_def_sampling = get_predictions_cached(sample_questions, model_def,\n",
    "                                                  os.path.join(in_folder_def, 'from_' + character + '_df_' + '_sampling.json'),\n",
    "                                                  \"Sampling\", override_predictions=override_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 1776.07it/s]\n"
     ]
    }
   ],
   "source": [
    "df_char_def = get_dataframe_for_metrics(character_hg['test'], None, None, predictions_def_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor i in range(1):\\n    print(\"##### Sample \" + str(i+1) + \" #####\")\\n    context_sentence   = df_char[\\'ctx\\'][i]\\n    character_response = df_char[\\'prd_sampling\\'][i]\\n    default_response   = df_char_def[\\'prd_sampling\\'][i]\\n    compute_sample_metrics(context_sentence, default_response, character_response, label_chatbot_symmetry=True)\\n    print()\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for i in range(1):\n",
    "    print(\"##### Sample \" + str(i+1) + \" #####\")\n",
    "    context_sentence   = df_char['ctx'][i]\n",
    "    character_response = df_char['prd_sampling'][i]\n",
    "    default_response   = df_char_def['prd_sampling'][i]\n",
    "    compute_sample_metrics(context_sentence, default_response, character_response, label_chatbot_symmetry=True)\n",
    "    print()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nset_size = 50\\ni = 30\\nprint(\"##### Set (Size \" + str(set_size) + \") #####\")\\ncontext_sentences   = list(df_char[\\'ctx\\'][i:i+set_size])\\ncharacter_responses = list(df_char[\\'prd_sampling\\'][i:i+set_size])\\ndefault_responses   = list(df_char_def[\\'prd_sampling\\'][i:i+set_size])\\ncompute_set_metrics(None, None,\\n                    context_sentences, default_responses, character_responses, character, label_chatbot_symmetry=True)\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "set_size = 50\n",
    "i = 30\n",
    "print(\"##### Set (Size \" + str(set_size) + \") #####\")\n",
    "context_sentences   = list(df_char['ctx'][i:i+set_size])\n",
    "character_responses = list(df_char['prd_sampling'][i:i+set_size])\n",
    "default_responses   = list(df_char_def['prd_sampling'][i:i+set_size])\n",
    "compute_set_metrics(None, None,\n",
    "                    context_sentences, default_responses, character_responses, character, label_chatbot_symmetry=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Full Test Set #####\n",
      "=== SEMANTIC SIMILARITY ===\n",
      "context-chatbota similarity:    {'score': 0.3545387387275696, 'std': 0.21952050924301147}\n",
      "context-chatbotb similarity:  {'score': 0.33069825172424316, 'std': 0.18615958094596863}\n",
      "chatbotb-chatbota similarity:    {'score': 0.13831575214862823, 'std': 0.07600176334381104}\n",
      "ccl-sim similarity:             0.48600932105317185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.78it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===       PERPLEXITY     ===\n",
      "chatbota perplexity:          20.615766981008772\n",
      "chatbotb perplexity:          618.6860775727047\n",
      "===         BLEU         ===\n",
      "context-to-chatbota bleu:       {'score': 0.0}\n",
      "context-to-chatbotb bleu:     {'score': 0.0}\n",
      "chatbota-to-chatbotb bleu:       {'score': 0.0}\n",
      "ccl-sim bleu:             0.5\n",
      "===        ROUGE-L       ===\n",
      "context-to-chatbota rouge:      {'score': 0.14650645744573537, 'std': 0.23855587781933377}\n",
      "context-to-chatbotb rouge:    {'score': 0.11526831253756564, 'std': 0.1510622681644724}\n",
      "chatbota-to-chatbotb rouge:      {'score': 0.04968081435472739, 'std': 0.0746656093146303}\n",
      "ccl-sim rouge:             0.4704838575979566\n",
      "===       DISTINCT      ===\n",
      "context distinct:           {'score': -1.8850444702591331, 'std': 0.04780861208267612}\n",
      "chatbota distinct:           {'score': -1.8858537217912217, 'std': 0.05431507639059038}\n",
      "chatbotb distinct:           {'score': -1.84711077546383, 'std': 0.04474986771806541}\n",
      "===       EMOTION       ===\n",
      "context emotions:            \n",
      " [('sadness', 0.03771194361979724), ('joy', 0.3086916868633125), ('love', 0.0028261127226869576), ('anger', 0.46144579228712246), ('fear', 0.17504403311977512), ('surprise', 0.01428043059331685)]\n",
      "chatbota emotions:              \n",
      " [('sadness', 0.07943609881112934), ('joy', 0.4079613431240432), ('love', 0.0034159159113187343), ('anger', 0.3681901668314822), ('fear', 0.13854427893602406), ('surprise', 0.0024521731611457653)]\n",
      "chatbotb emotions:            \n",
      " [('sadness', 0.10721138260123553), ('joy', 0.4747975275386125), ('love', 0.010618916036037263), ('anger', 0.33580759711549035), ('fear', 0.06870650521705102), ('surprise', 0.00285805671592243)]\n",
      "chatbota-chatbotbemotion corr:  \n",
      " 0.9707528398997248\n",
      "===         SAS         ===\n",
      "context-chatbota sas:           {'score': 0.15687566995620728, 'std': 0.24979953467845917}\n",
      "context-chatbotb sas:         {'score': 0.0847223550081253, 'std': 0.14002537727355957}\n",
      "chatbota-chatbotb sas:           {'score': 0.01616792380809784, 'std': 0.014025779440999031}\n",
      "ccl-sim sas:                0.4305804363610488\n",
      "=== SEMANTIC CLASSIFIER ===\n",
      "sem-classifier chatbota:                 {'score': 0.6420408487319946, 'std': 0.44049662351608276}\n",
      "sem-classifier chatbotb:                   {'score': 0.1775670349597931, 'std': 0.34092846512794495}\n",
      "time elapsed computing semantic classifier:  11.03 s\n",
      "Symmetric mode, skipping Human metrics.\n",
      "Skipping sentence outputting.\n"
     ]
    }
   ],
   "source": [
    "print(\"##### Full Test Set #####\")\n",
    "context_sentences   = list(df_char['ctx'])\n",
    "character_responses = list(df_char['prd_sampling'])\n",
    "default_responses   = list(df_char_def['prd_sampling'])\n",
    "scores = compute_set_metrics(model, model_def, character, 'Default', character + \" dataset\",\n",
    "                             context_sentences, \n",
    "                             character_responses, \n",
    "                             default_responses,\n",
    "                             encoded_test_set,\n",
    "                             classifier_n_sentences=75,\n",
    "                             label_chatbot_symmetry=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_json(metrics_folder, character+'_vs_nonfinetuned_metrics', scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Between Character 1 & Character 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_small(sample_questions, model, generation_method):\n",
    "    print(\"Creating predictions\")\n",
    "    predictions = list()\n",
    "    for x in tqdm(sample_questions):\n",
    "        tokenized_question = tokenizer.encode(x + tokenizer.eos_token, return_tensors='tf')\n",
    "        max_length = 128 + tokenized_question.shape[1]\n",
    "        if generation_method == \"Greedy\":\n",
    "            generated_answer = model.generate(tokenized_question,\n",
    "                                pad_token_id=tokenizer.eos_token_id, max_length=max_length)[0].numpy().tolist()\n",
    "        elif generation_method == \"Beam Search\":\n",
    "            generated_answer = model.generate(tokenized_question,\n",
    "                                         pad_token_id=tokenizer.eos_token_id, max_length=max_length,\n",
    "                                         n_beams=n_beams)[0].numpy().tolist()\n",
    "        elif generation_method == \"Sampling\":\n",
    "                b = True\n",
    "                c = 0\n",
    "                while b:\n",
    "                    generated_answer = model.generate(tokenized_question,\n",
    "                                                 pad_token_id=tokenizer.eos_token_id, max_length=max_length,\n",
    "                                                 do_sample=True, top_k=top_k, top_p=top_p)[0].numpy().tolist()\n",
    "                    \n",
    "                    c+= 1\n",
    "                    if len(generated_answer[len(tokenized_question[0]):])>1:\n",
    "                        b = False         \n",
    "                    if c>100: \n",
    "                        generated_answer[len(tokenized_question[0]):] = tokenizer.encode('hi') + [tokenizer.eos_token_id]\n",
    "                        break \n",
    "                        \n",
    "        predictions.append(generated_answer[len(tokenized_question[0]):])\n",
    "        \n",
    "        assert all([len(p)>1 for p in predictions])\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-2caa0e9478f457ce\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\cache\\csv\\default-2caa0e9478f457ce\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd05bd17a5064f959da36a78359aaff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e30b5a01c694d6c95ca9e07e88775f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\cache\\csv\\default-2caa0e9478f457ce\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12dbeb83d08c4156aa9c9a13d892d7af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa5a36214c644b68b8a940b3e19be23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_common = load_dataset('csv',\n",
    "                         data_files=os.path.join(base_folder, 'Data', 'common_dataset.csv'), \n",
    "                         cache_dir=os.path.join(base_folder, \"cache\"))\n",
    "\n",
    "df_common = df_common.remove_columns(['source'])\n",
    "tokenized_common_hg = df_common['train'].map(preprocess_function, batched=False)\n",
    "\n",
    "encoded_common_set = tokenized_common_hg.to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'context/0'],\n",
       "        num_rows: 35\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec={'input_ids': TensorSpec(shape=(None, None), dtype=tf.int64, name=None), 'attention_mask': TensorSpec(shape=(None, None), dtype=tf.int64, name=None), 'labels': TensorSpec(shape=(None, None), dtype=tf.int64, name=None)}>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_common_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 35/35 [00:46<00:00,  1.33s/it]\n"
     ]
    }
   ],
   "source": [
    "predictions_1_sampling = get_predictions_small(df_common['train']['context/0'], model, \"Sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 35/35 [00:26<00:00,  1.31it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions_2_sampling = get_predictions_small(df_common['train']['context/0'], model_2, \"Sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 35/35 [00:00<00:00, 3496.67it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 35/35 [00:00<00:00, 3496.92it/s]\n"
     ]
    }
   ],
   "source": [
    "df_common_char_1 = get_dataframe_for_metrics(df_common['train'], None, None, predictions_1_sampling)\n",
    "df_common_char_2 = get_dataframe_for_metrics(df_common['train'], None, None, predictions_2_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Vader  Vs. Harry #####\n",
      "=== SEMANTIC SIMILARITY ===\n",
      "context-chatbota similarity:    {'score': 0.22928862273693085, 'std': 0.19413906335830688}\n",
      "context-chatbotb similarity:  {'score': 0.26278918981552124, 'std': 0.13143767416477203}\n",
      "chatbotb-chatbota similarity:    {'score': 0.265611469745636, 'std': 0.13768655061721802}\n",
      "ccl-sim similarity:             0.5023353033489216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  5.40it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===       PERPLEXITY     ===\n",
      "chatbota perplexity:          57.95086184069064\n",
      "chatbotb perplexity:          43.05399126504585\n",
      "===         BLEU         ===\n",
      "context-to-chatbota bleu:       {'score': 0.0}\n",
      "context-to-chatbotb bleu:     {'score': 0.0}\n",
      "chatbota-to-chatbotb bleu:       {'score': 0.0}\n",
      "ccl-sim bleu:             0.5\n",
      "===        ROUGE-L       ===\n",
      "context-to-chatbota rouge:      {'score': 0.07366340634319406, 'std': 0.17719224809401155}\n",
      "context-to-chatbotb rouge:    {'score': 0.054179676999004736, 'std': 0.08987718399234672}\n",
      "chatbota-to-chatbotb rouge:      {'score': 0.06389015467962837, 'std': 0.09993900923630875}\n",
      "ccl-sim rouge:             0.48274705444288296\n",
      "===       DISTINCT      ===\n",
      "context distinct:           {'score': -1.890156719517119, 'std': 0.061888269489336664}\n",
      "chatbota distinct:           {'score': -1.8925175597818058, 'std': 0.05512533105673859}\n",
      "chatbotb distinct:           {'score': -1.9070860012552835, 'std': 0.06746374618886293}\n",
      "===       EMOTION       ===\n",
      "context emotions:            \n",
      " [('sadness', 0.014541776718605043), ('joy', 0.34016804504208265), ('love', 0.11487390943678162), ('anger', 0.37266245127040226), ('fear', 0.15484204553899222), ('surprise', 0.002911792164585287)]\n",
      "chatbota emotions:              \n",
      " [('sadness', 0.059613082312612926), ('joy', 0.40625777527706564), ('love', 0.004188759525589246), ('anger', 0.41203260560619775), ('fear', 0.11248617501946033), ('surprise', 0.005421601514016012)]\n",
      "chatbotb emotions:            \n",
      " [('sadness', 0.1300904471776448), ('joy', 0.39918279103668675), ('love', 0.006212823160707818), ('anger', 0.3733503845495371), ('fear', 0.08858192077688208), ('surprise', 0.002581644834052505)]\n",
      "chatbota-chatbotbemotion corr:  \n",
      " 0.9824036885010332\n",
      "===         SAS         ===\n",
      "context-chatbota sas:           {'score': 0.06959118694067001, 'std': 0.16765445470809937}\n",
      "context-chatbotb sas:         {'score': 0.05564865842461586, 'std': 0.09564653784036636}\n",
      "chatbota-chatbotb sas:           {'score': 0.05021628737449646, 'std': 0.06134150177240372}\n",
      "ccl-sim sas:                0.48741550629349534\n",
      "=== SEMANTIC CLASSIFIER ===\n",
      "sem-classifier chatbota:                 {'score': 0.49807772040367126, 'std': 0.4654912054538727}\n",
      "sem-classifier chatbotb:                   {'score': 0.08739948272705078, 'std': 0.24489933252334595}\n",
      "time elapsed computing semantic classifier:  11.89 s\n",
      "Symmetric mode, skipping Human metrics.\n",
      "===      SENTENCES      ===\n",
      "* context:  Barney, this is about the building.\n",
      "* chatbota: ... no?\n",
      "* chatbotb: I hope she's right.\n",
      "\n",
      "* context:  All right. I'll be right there. Stay where you are.\n",
      "* chatbota: Look at your\n",
      "* chatbotb: You heard it.\n",
      "\n",
      "* context:  I think there's a pretty girl smiling at me there.\n",
      "* chatbota: Chewie, what's going on?\n",
      "* chatbotb: Hagrid, what're you doing?\n",
      "\n",
      "* context:  I love you, man.\n",
      "* chatbota: Yes, my Lord.\n",
      "* chatbotb: Oh, sweet! You're going to be our friend!\n",
      "\n",
      "* context:  Not even if she's hot?\n",
      "* chatbota: I feel the faith.\n",
      "* chatbotb: The real one would never have made it to the toilet.\n",
      "\n",
      "* context:  Soft kitty, warm kitty Little ball of fur\n",
      "* chatbota: He's alive, and now he has a new home, a new father.\n",
      "* chatbotb: What's this?\n",
      "\n",
      "* context:  Penny.\n",
      "* chatbota: You are a powerful ally.\n",
      "* chatbotb: You're a fraud.\n",
      "\n",
      "* context:  Oh. Sheldon, thank you. Thats so romantic. But what about Rajesh? He was okay with you choosing the name?\n",
      "* chatbota: Perhaps you should continue to deal with it, Artoo.\n",
      "* chatbotb: It was a really unfortunate decision.\n",
      "\n",
      "* context:  I didnt break it. I, I guess Stuart sold it to me like this.\n",
      "* chatbota: I think I've been inebriated since you've broken the station.\n",
      "* chatbotb: I'm sorry.\n",
      "\n",
      "* context:  Be careful.\n",
      "* chatbota: The firetide lies in the sky\n",
      "* chatbotb: Ron, you are the fool that put the lamp on the fire.\n",
      "\n",
      "* context:  But why would anyone go near that dog?\n",
      "* chatbota: Oh.\n",
      "* chatbotb: It is not worth the time.\n",
      "\n",
      "* context:  Expecto Patronum!\n",
      "* chatbota: We've been expecting you!\n",
      "* chatbotb: That's a great trick!\n",
      "\n",
      "* context:  Ron Weasley.\n",
      "* chatbota: ... you've taken the fall, young Skywalker.\n",
      "* chatbotb: Hermione?\n",
      "\n",
      "* context:  I spoke a different language?\n",
      "* chatbota: They will not leave you alone!\n",
      "* chatbotb: That's not normal!\n",
      "\n",
      "* context:  Harry?\n",
      "* chatbota: No, sir.You're here for the wrong information.\n",
      "* chatbotb: I see he's lost a few of his old ones.\n",
      "\n",
      "* context:  OK. First Bender, then Flexo, then Fry.\n",
      "* chatbota: The Imperial Senate has accepted the terms to our new treaty. We will make the necessary preparations to keep this deal from the Rebel Alliance for the next three years!\n",
      "* chatbotb: The one who gave you the keys to the tower.\n",
      "\n",
      "* context:  Just relax, Bender. Tomorrow we'll pry you down, have a nice breakfast and then go hunt down and slaughter that ancient evil.\n",
      "* chatbota: The Force is strong with this one. We're going to destroy this galaxy with this\n",
      "* chatbotb: I'm not afraid, professor.\n",
      "\n",
      "* context:  I'm too scared.\n",
      "* chatbota: She's a princess, she'll keep me forever.\n",
      "* chatbotb: All right.\n",
      "\n",
      "* context:  Dr. Zoidberg? Are you OK?\n",
      "* chatbota: Yes, we do.\n",
      "* chatbotb: Well, thank you.\n",
      "\n",
      "* context:  Fry, thank God we found you.\n",
      "* chatbota: Lord Vader, no, not this again! We've been waiting for you, father.\n",
      "* chatbotb: Harry, I think we're safe.\n",
      "\n",
      "* context:  I will not fight you.\n",
      "* chatbota: Yes, sir, it is you who are hurt by the Rebels. The Rebels have used up their energy of hatred.\n",
      "* chatbotb: You'll fight me.\n",
      "\n",
      "* context:  Lord Vader, what about Leia and theWookiee?\n",
      "* chatbota: The Emperor is here, and he'll fix this mess.\n",
      "* chatbotb: Don't worry, your parents will come.\n",
      "\n",
      "* context:  The Emperor's coming here?\n",
      "* chatbota: Look what you've done, kid.Look on this. I can't be stopped, let me tell you.\n",
      "* chatbotb: He's not going to be there, are he?\n",
      "\n",
      "* context:  Shall I hold them?\n",
      "* chatbota: Yes, my master.\n",
      "* chatbotb: No.\n",
      "\n",
      "* context:  Lord Vader, what about Leia and theWookiee?\n",
      "* chatbota: ... are you on a technicality?\n",
      "* chatbotb: You should have talked to them.\n",
      "\n",
      "* context:  Oh! Joey uh, were you in our room last night?\n",
      "* chatbota: Where were you?\n",
      "* chatbotb: Yeah, I was in your room.\n",
      "\n",
      "* context:  Hey.\n",
      "* chatbota: Hey.\n",
      "* chatbotb: Welcome home.\n",
      "\n",
      "* context:  Joey... are you sure? I mean, I know how much you love him!\n",
      "* chatbota: It's just a dream, my friend. You can't believe this.\n",
      "* chatbotb: Come on.\n",
      "\n",
      "* context:  Ok, ten.\n",
      "* chatbota: TheVampire, you've outsmarted me.\n",
      "* chatbotb: We are sorry.\n",
      "\n",
      "* context:  Joey, Ross is gonna be here any second, would you mind watching Ben for me while I use the ladies' room?\n",
      "* chatbota: I don't know about that.\n",
      "* chatbotb: It's not a very nice house you have there, professor.\n",
      "\n",
      "* context:  What are you doing for a living?\n",
      "* chatbota: You must be a real hit with your friend, Commander.\n",
      "* chatbotb: Oh.\n",
      "\n",
      "* context:  How are you doing?\n",
      "* chatbota: They found a little piece of human flesh on the surface of the surface of the solar system.\n",
      "* chatbotb: My parents made me do it.\n",
      "\n",
      "* context:  Where are you going to?\n",
      "* chatbota: Comeon, hurry!\n",
      "* chatbotb: The U SG is a little bit more powerful than the Harry Potter School of Witchcraft and Wizardry.\n",
      "\n",
      "* context:  What are you wearing?\n",
      "* chatbota: Darth Vader calmly adjusts his control stick\n",
      "* chatbotb: Hagrid's robes.\n",
      "\n",
      "* context:  What do you want to do tonight?\n",
      "* chatbota: Yes, sir.\n",
      "* chatbotb: I just want to say thanks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"##### \" + character + \"  Vs. \" + character_2 + \" #####\")\n",
    "context_sentences   = list(df_common_char_1['ctx'])\n",
    "chatbot_responses   = list(df_common_char_1['prd_sampling'])\n",
    "chatbot_2_responses = list(df_common_char_2['prd_sampling'])\n",
    "scores = compute_set_metrics(model, model_2, character, character_2, \"common small dataset\",\n",
    "                             context_sentences, chatbot_responses, chatbot_2_responses, encoded_common_set,\n",
    "                             include_qualitative_sentences=True, label_chatbot_symmetry=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_json(metrics_folder, character+'_vs_'+character_2+'_metrics', scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

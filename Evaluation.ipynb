{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data.data_dicts import character_dict, source_dict, random_state\n",
    "\n",
    "model_name = 'microsoft/DialoGPT-small'\n",
    "character = 'Sheldon' # 'Barney' | 'Sheldon' | 'Harry' | 'Fry' | 'Vader' | 'Joey' | 'Phoebe' | 'Bender' | Default'\n",
    "character_2 = 'Barney'\n",
    "override_predictions = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount google drive\n",
    "import os\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive',force_remount=True)\n",
    "    base_folder = '/content/drive/My Drive/unibo/NLP_project/BarneyBot'\n",
    "    os.system(\"pip install datasets\")\n",
    "    os.system(\"pip install transformers\")\n",
    "    os.system(\"pip install rouge_score\")\n",
    "    os.system(\"pip install -U sentence-transformers\")\n",
    "else:\n",
    "    base_folder = os.getcwd()\n",
    "    \n",
    "in_folder = os.path.join(base_folder, 'Data', 'Characters', character)\n",
    "if not os.path.exists(in_folder):\n",
    "    os.makedirs(in_folder)\n",
    "out_folder = os.path.join(base_folder, 'Data', 'Characters', character)\n",
    "if not os.path.exists(out_folder):\n",
    "    os.makedirs(out_folder)\n",
    "    \n",
    "in_folder_2 = os.path.join(base_folder, 'Data', 'Characters', character_2)\n",
    "if not os.path.exists(in_folder_2):\n",
    "    os.makedirs(in_folder_2)\n",
    "out_folder_2 = os.path.join(base_folder, 'Data', 'Characters', character_2)\n",
    "if not os.path.exists(out_folder_2):\n",
    "    os.makedirs(out_folder_2)\n",
    "    \n",
    "in_folder_def = os.path.join(base_folder, 'Data', 'Characters', 'Default')\n",
    "if not os.path.exists(in_folder_def):\n",
    "    os.makedirs(in_folder_def)\n",
    "out_folder_def = os.path.join(base_folder, 'Data', 'Characters', 'Default')\n",
    "if not os.path.exists(out_folder_def):\n",
    "    os.makedirs(out_folder_def)\n",
    "    \n",
    "metrics_folder = os.path.join(base_folder, 'Metrics')\n",
    "if not os.path.exists(metrics_folder):\n",
    "    os.makedirs(metrics_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_json(filepath, filename, data):\n",
    "    if not os.path.exists(filepath):\n",
    "        os.makedirs(filepath, exist_ok=True)\n",
    "    with open(os.path.join(filepath, filename + \".json\"), 'w') as f:\n",
    "        f.write(json.dumps(data, indent=4))\n",
    "\n",
    "def load_from_json(filepath, filename):\n",
    "    if not os.path.exists(os.path.join(filepath, filename + '.json')):\n",
    "        return dict()\n",
    "    with open(os.path.join(filepath, filename + '.json'), 'r') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "def load_df(character):\n",
    "    dataset_path = os.path.join(base_folder, \"Data\", \"Characters\", character, character+'.csv')\n",
    "    \n",
    "    character_hg = load_dataset('csv', \n",
    "                                data_files=dataset_path, \n",
    "                                cache_dir=os.path.join(base_folder, \"cache\"))\n",
    "    \n",
    "    # 85% train / 10% test / 5% validation\n",
    "    train_test_hg = character_hg['train'].train_test_split(test_size=0.15, seed=random_state)\n",
    "    test_val = train_test_hg['test'].train_test_split(test_size=0.33, seed=random_state)\n",
    "    \n",
    "    \n",
    "    character_hg = DatasetDict({\n",
    "        'train': train_test_hg['train'],\n",
    "        'test': test_val['train'],\n",
    "        'val': test_val['test']\n",
    "    })\n",
    "    \n",
    "    return character_hg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_conv(row, tokenizer):\n",
    "    MAX_LENGTH = 512\n",
    "    row = list(reversed(list(row.values())))\n",
    "    model_inputs = tokenizer(row)\n",
    "    tokenizer_pad_token_id = tokenizer.encode('#')[0]\n",
    "    for i in range(len(model_inputs['input_ids'])):\n",
    "        model_inputs['input_ids'][i].append(tokenizer.eos_token_id)\n",
    "        model_inputs['attention_mask'][i].append(1)\n",
    "    model_inputs['input_ids'] = [item for sublist in model_inputs['input_ids'] for item in sublist]\n",
    "    model_inputs['attention_mask'] = [item for sublist in model_inputs['attention_mask'] for item in sublist]\n",
    "    if MAX_LENGTH > len(model_inputs['input_ids']):\n",
    "        model_inputs['input_ids'] += [tokenizer_pad_token_id] * (MAX_LENGTH - len(model_inputs['input_ids']))\n",
    "        model_inputs['attention_mask'] += [0] * (MAX_LENGTH - len(model_inputs['attention_mask']))\n",
    "    elif MAX_LENGTH < len(model_inputs['input_ids']):\n",
    "        model_inputs['input_ids'] = model_inputs['input_ids'][:MAX_LENGTH-1]\n",
    "        model_inputs['input_ids'][-1] = tokenizer.eos_token_id\n",
    "        model_inputs['attention_mask'] = model_inputs['attention_mask'][:MAX_LENGTH-1]\n",
    "        model_inputs['attention_mask'][-1] = 1\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    tokenizer.pad_token = '#'\n",
    "    model_inputs = construct_conv(examples, tokenizer)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-fbb5da8195eeafdf\n",
      "Reusing dataset csv (C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\cache\\csv\\default-fbb5da8195eeafdf\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1853b495e90e49cfbb5b1a3e8faac2ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\cache\\csv\\default-fbb5da8195eeafdf\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-ed070a17f87583b0.arrow and C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\cache\\csv\\default-fbb5da8195eeafdf\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-8b13a1828c5f655c.arrow\n",
      "Loading cached split indices for dataset at C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\cache\\csv\\default-fbb5da8195eeafdf\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-2b4e70bae7b548bf.arrow and C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\cache\\csv\\default-fbb5da8195eeafdf\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-9730e803e8838750.arrow\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"HF_DATASETS_CACHE\"] = os.path.join(base_folder, \"cache\")\n",
    "character_hg = load_df(character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_folder = os.path.join(out_folder, character_dict[character]['checkpoint_folder'])\n",
    "checkpoint_folder_2 = os.path.join(out_folder_2, character_dict[character_2]['checkpoint_folder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\Data\\Characters\\Sheldon\\sheldon_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! Please ensure your labels are passed as keys in the input dict so that they are accessible to the model during the forward pass. To disable this behaviour, please pass a loss argument, or explicitly pass loss=None if you do not want your model to compute a loss.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\Data\\Characters\\Barney\\barney_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! Please ensure your labels are passed as keys in the input dict so that they are accessible to the model during the forward pass. To disable this behaviour, please pass a loss argument, or explicitly pass loss=None if you do not want your model to compute a loss.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at microsoft/DialoGPT-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! Please ensure your labels are passed as keys in the input dict so that they are accessible to the model during the forward pass. To disable this behaviour, please pass a loss argument, or explicitly pass loss=None if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForCausalLM, AutoTokenizer, AdamWeightDecay\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=os.path.join(base_folder, \"cache\"))\n",
    "tokenizer.pad_token = '#'\n",
    "\n",
    "model = TFAutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=checkpoint_folder)\n",
    "model.compile(optimizer=AdamWeightDecay(learning_rate=2e-5))\n",
    "\n",
    "model_2 = TFAutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=checkpoint_folder_2)\n",
    "model_2.compile(optimizer=AdamWeightDecay(learning_rate=2e-5))\n",
    "\n",
    "model_def = TFAutoModelForCausalLM.from_pretrained(model_name, cache_dir=os.path.join(base_folder, \"cache\"))\n",
    "model_def.compile(optimizer=AdamWeightDecay(learning_rate=2e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\cache\\csv\\default-fbb5da8195eeafdf\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-57294622ea31622c.arrow\n",
      "Loading cached processed dataset at C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\cache\\csv\\default-fbb5da8195eeafdf\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-9abd6d9399deb390.arrow\n",
      "Loading cached processed dataset at C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\cache\\csv\\default-fbb5da8195eeafdf\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-b16222792f5f5613.arrow\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import AdamWeightDecay\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(mlm=False, tokenizer=tokenizer, return_tensors='tf')\n",
    "\n",
    "tokenized_character_hg = character_hg.map(preprocess_function, batched=False)\n",
    "\n",
    "encoded_test_set = tokenized_character_hg[\"test\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_questions = character_hg['test']['context/0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_beams = 3\n",
    "top_k = 50\n",
    "top_p = 0.92\n",
    "\n",
    "def get_predictions_cached(sample_questions, model, filename, generation_method, override_predictions=False):\n",
    "    prediction_path = os.path.join(in_folder, filename)\n",
    "    if os.path.exists(prediction_path) and not override_predictions:\n",
    "        print(\"Loading predictions from stored file\")\n",
    "        with open(prediction_path, 'r') as file:\n",
    "            json_string = file.read()\n",
    "        predictions = json.loads(json_string)\n",
    "        print(\"Loaded predictions from stored file\")\n",
    "\n",
    "    else:\n",
    "        print(\"Creating predictions\")\n",
    "        predictions = list()\n",
    "        for x in tqdm(sample_questions):\n",
    "            tokenized_question = tokenizer.encode(x + tokenizer.eos_token, return_tensors='tf')\n",
    "            max_length = 128 + tokenized_question.shape[1]\n",
    "            if generation_method == \"Greedy\":\n",
    "                generated_answer = model.generate(tokenized_question,\n",
    "                                    pad_token_id=tokenizer.eos_token_id, max_length=max_length)[0].numpy().tolist()\n",
    "            elif generation_method == \"Beam Search\":\n",
    "                generated_answer = model.generate(tokenized_question,\n",
    "                                             pad_token_id=tokenizer.eos_token_id, max_length=max_length,\n",
    "                                             n_beams=n_beams)[0].numpy().tolist()\n",
    "            elif generation_method == \"Sampling\":\n",
    "                b = True\n",
    "                c = 0\n",
    "                while b:\n",
    "                    generated_answer = model.generate(tokenized_question,\n",
    "                                                 pad_token_id=tokenizer.eos_token_id, max_length=max_length,\n",
    "                                                 do_sample=True, top_k=top_k, top_p=top_p)[0].numpy().tolist()\n",
    "                    c += 1\n",
    "                    if len(generated_answer[len(tokenized_question[0]):])>1:\n",
    "                        b = False       \n",
    "                    if c>100: \n",
    "                        generated_answer[len(tokenized_question[0]):] = tokenizer.encode('hi') + [tokenizer.eos_token_id]\n",
    "                        break\n",
    "            \n",
    "            predictions.append(generated_answer[len(tokenized_question[0]):])\n",
    "\n",
    "        # Save predictions as a JSON file\n",
    "        output_string = json.dumps(predictions)\n",
    "        with open(prediction_path, 'w') as file:\n",
    "            file.write(output_string)\n",
    "        \n",
    "        assert all([len(p)>1 for p in predictions])\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading predictions from stored file\n",
      "Loaded predictions from stored file\n",
      "Loading predictions from stored file\n",
      "Loaded predictions from stored file\n",
      "Loading predictions from stored file\n",
      "Loaded predictions from stored file\n"
     ]
    }
   ],
   "source": [
    "predictions_greedy = get_predictions_cached(sample_questions, model,\n",
    "                                            character_dict[character]['prediction_filename'] + '_greedy.json',\n",
    "                                            \"Greedy\",\n",
    "                                            override_predictions=override_predictions)\n",
    "predictions_nbeams = get_predictions_cached(sample_questions, model,\n",
    "                                            character_dict[character]['prediction_filename'] + '_nbeams.json',\n",
    "                                            \"Beam Search\",\n",
    "                                            override_predictions=override_predictions)\n",
    "predictions_sampling = get_predictions_cached(sample_questions, model,\n",
    "                                              character_dict[character]['prediction_filename'] + '_sampling.json',\n",
    "                                              \"Sampling\",\n",
    "                                              override_predictions=override_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe_for_metrics(data_test, predictions_greedy, predictions_nbeams, predictions_sampling):\n",
    "    i = 0\n",
    "    df = {'ctx':[], 'ctx_tk':[]}\n",
    "    has_labels = 'response' in data_test.features\n",
    "    if has_labels:\n",
    "        df['lbl'] = []\n",
    "        df['lbl_tk'] = []\n",
    "    if predictions_greedy:\n",
    "        df['prd_greedy'] = []\n",
    "        df['prd_greedy_tk'] = []\n",
    "    if predictions_nbeams:\n",
    "        df['prd_nbeams'] = []\n",
    "        df['prd_nbeams_tk'] = [] \n",
    "    if predictions_sampling:\n",
    "        df['prd_sampling'] = []\n",
    "        df['prd_sampling_tk'] = []\n",
    "    for sample in tqdm(data_test):\n",
    "        # encode the context and label sentences, add the eos_token and return a tensor\n",
    "        ctx_tk = tokenizer.encode(sample['context/0'] + tokenizer.eos_token, return_tensors='tf').numpy().tolist()\n",
    "        ctx = sample['context/0']\n",
    "        df['ctx_tk'].append(ctx_tk)\n",
    "        df['ctx'].append(ctx)\n",
    "        if has_labels:\n",
    "            lbl_tk = tokenizer.encode(sample['response'] + tokenizer.eos_token, return_tensors='tf').numpy().tolist()\n",
    "            lbl = sample['response']\n",
    "            df['lbl'].append(lbl)\n",
    "            df['lbl_tk'].append(lbl_tk)\n",
    "        if predictions_greedy:\n",
    "            prd_greedy_tk = predictions_greedy[i]\n",
    "            prd_greedy = tokenizer.decode(prd_greedy_tk, skip_special_tokens=True)\n",
    "            df['prd_greedy'].append(prd_greedy)\n",
    "            df['prd_greedy_tk'].append(prd_greedy_tk)\n",
    "        if predictions_nbeams:\n",
    "            prd_nbeams_tk = predictions_nbeams[i]\n",
    "            prd_nbeams = tokenizer.decode(prd_nbeams_tk, skip_special_tokens=True)\n",
    "            df['prd_nbeams'].append(prd_nbeams)\n",
    "            df['prd_nbeams_tk'].append(prd_nbeams_tk)\n",
    "        if predictions_sampling:\n",
    "            prd_sampling_tk = predictions_sampling[i]\n",
    "            prd_sampling = tokenizer.decode(prd_sampling_tk, skip_special_tokens=True)\n",
    "            df['prd_sampling'].append(prd_sampling)\n",
    "            df['prd_sampling_tk'].append(prd_sampling_tk)\n",
    "        i += 1\n",
    "    return pd.DataFrame(data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1170/1170 [00:00<00:00, 1724.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ctx</th>\n",
       "      <th>ctx_tk</th>\n",
       "      <th>lbl</th>\n",
       "      <th>lbl_tk</th>\n",
       "      <th>prd_greedy</th>\n",
       "      <th>prd_greedy_tk</th>\n",
       "      <th>prd_nbeams</th>\n",
       "      <th>prd_nbeams_tk</th>\n",
       "      <th>prd_sampling</th>\n",
       "      <th>prd_sampling_tk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oh, sweetie, Im sorry.</td>\n",
       "      <td>[[5812, 11, 6029, 494, 11, 1846, 7926, 13, 502...</td>\n",
       "      <td>She called me dumbass.</td>\n",
       "      <td>[[3347, 1444, 502, 13526, 562, 13, 50256]]</td>\n",
       "      <td>No, its okay.</td>\n",
       "      <td>[2949, 11, 663, 8788, 13, 50256]</td>\n",
       "      <td>No, its okay.</td>\n",
       "      <td>[2949, 11, 663, 8788, 13, 50256]</td>\n",
       "      <td>No. Its okay.</td>\n",
       "      <td>[2949, 13, 6363, 8788, 13, 50256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oh. Wow. Good job. Okay, um, can you do this?</td>\n",
       "      <td>[[5812, 13, 24755, 13, 4599, 1693, 13, 16805, ...</td>\n",
       "      <td>Well never know.</td>\n",
       "      <td>[[5779, 1239, 760, 13, 50256]]</td>\n",
       "      <td>I dont know.</td>\n",
       "      <td>[40, 17666, 760, 13, 50256]</td>\n",
       "      <td>I dont know.</td>\n",
       "      <td>[40, 17666, 760, 13, 50256]</td>\n",
       "      <td>No.</td>\n",
       "      <td>[2949, 13, 50256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thanks. And, Sheldon, I know tonights the nigh...</td>\n",
       "      <td>[[9690, 13, 843, 11, 34536, 11, 314, 760, 5680...</td>\n",
       "      <td>Oh, you shouldnt have.</td>\n",
       "      <td>[[5812, 11, 345, 815, 429, 423, 13, 50256]]</td>\n",
       "      <td>Thats very impressive.</td>\n",
       "      <td>[817, 1381, 845, 8036, 13, 50256]</td>\n",
       "      <td>Thats very impressive.</td>\n",
       "      <td>[817, 1381, 845, 8036, 13, 50256]</td>\n",
       "      <td>Did you even ask for the ingredients?</td>\n",
       "      <td>[11633, 345, 772, 1265, 329, 262, 9391, 30, 50...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Too bad.</td>\n",
       "      <td>[[23307, 2089, 13, 50256]]</td>\n",
       "      <td>Youre going up Euclid Avenue?</td>\n",
       "      <td>[[1639, 260, 1016, 510, 48862, 312, 8878, 30, ...</td>\n",
       "      <td>You know, I, I, I think I, I, I think I, I thi...</td>\n",
       "      <td>[1639, 760, 11, 314, 11, 314, 11, 314, 892, 31...</td>\n",
       "      <td>You know, I, I, I think I, I, I think I, I thi...</td>\n",
       "      <td>[1639, 760, 11, 314, 11, 314, 11, 314, 892, 31...</td>\n",
       "      <td>Im coming, Penny.  Oh, look who it is.</td>\n",
       "      <td>[3546, 2406, 11, 25965, 13, 220, 3966, 11, 804...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Okay, so youd lose most of Jar Jar, all the tr...</td>\n",
       "      <td>[[16454, 11, 523, 345, 67, 4425, 749, 286, 153...</td>\n",
       "      <td>Get rid of the trade route part? Then how woul...</td>\n",
       "      <td>[[3855, 5755, 286, 262, 3292, 6339, 636, 30, 3...</td>\n",
       "      <td>I dont think I can go through with this.</td>\n",
       "      <td>[40, 17666, 892, 314, 460, 467, 832, 351, 428,...</td>\n",
       "      <td>I dont think I can go through with this.</td>\n",
       "      <td>[40, 17666, 892, 314, 460, 467, 832, 351, 428,...</td>\n",
       "      <td>Oh yeah, thats not bad. I love it.</td>\n",
       "      <td>[5812, 10194, 11, 29294, 407, 2089, 13, 314, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1165</th>\n",
       "      <td>Im available for experimentation.</td>\n",
       "      <td>[[3546, 1695, 329, 29315, 13, 50256]]</td>\n",
       "      <td>Thank you. Not necessary. We know everything t...</td>\n",
       "      <td>[[10449, 345, 13, 1892, 3306, 13, 775, 760, 22...</td>\n",
       "      <td>Oh, great. Well, good luck.</td>\n",
       "      <td>[5812, 11, 1049, 13, 3894, 11, 922, 8458, 13, ...</td>\n",
       "      <td>Oh, great. Well, good luck.</td>\n",
       "      <td>[5812, 11, 1049, 13, 3894, 11, 922, 8458, 13, ...</td>\n",
       "      <td>Oh. Well, see you in a while.</td>\n",
       "      <td>[5812, 13, 3894, 11, 766, 345, 287, 257, 981, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166</th>\n",
       "      <td>Sheldon, why are you ignoring your sister?</td>\n",
       "      <td>[[3347, 25900, 11, 1521, 389, 345, 15482, 534,...</td>\n",
       "      <td>Im not ignoring my sister. Im ignoring all of ...</td>\n",
       "      <td>[[3546, 407, 15482, 616, 6621, 13, 1846, 15482...</td>\n",
       "      <td>I dont follow.</td>\n",
       "      <td>[40, 17666, 1061, 13, 50256]</td>\n",
       "      <td>I dont follow.</td>\n",
       "      <td>[40, 17666, 1061, 13, 50256]</td>\n",
       "      <td>I dont think Penny was ignoring you.</td>\n",
       "      <td>[40, 17666, 892, 25965, 373, 15482, 345, 13, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1167</th>\n",
       "      <td>Look, Sheldon, before you race off to the fert...</td>\n",
       "      <td>[[8567, 11, 34536, 11, 878, 345, 3234, 572, 28...</td>\n",
       "      <td>You mean dating?</td>\n",
       "      <td>[[1639, 1612, 10691, 30, 50256]]</td>\n",
       "      <td>Oh, gee, I dont know. Maybe.</td>\n",
       "      <td>[5812, 11, 308, 1453, 11, 314, 17666, 760, 13,...</td>\n",
       "      <td>Oh, gee, I dont know. Maybe.</td>\n",
       "      <td>[5812, 11, 308, 1453, 11, 314, 17666, 760, 13,...</td>\n",
       "      <td>Oh, that is a good idea.</td>\n",
       "      <td>[5812, 11, 326, 318, 257, 922, 2126, 13, 50256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168</th>\n",
       "      <td>Sheldon, Im sorry. Ill be happy to reimburse y...</td>\n",
       "      <td>[[3347, 25900, 11, 1846, 7926, 13, 5821, 307, ...</td>\n",
       "      <td>You dont need to reimburse me because Im not p...</td>\n",
       "      <td>[[1639, 17666, 761, 284, 25903, 502, 780, 1846...</td>\n",
       "      <td>Oh, I dont mind. I just dont want to be respon...</td>\n",
       "      <td>[5812, 11, 314, 17666, 2000, 13, 314, 655, 176...</td>\n",
       "      <td>Oh, I dont mind. I just dont want to be respon...</td>\n",
       "      <td>[5812, 11, 314, 17666, 2000, 13, 314, 655, 176...</td>\n",
       "      <td>Its not fair.</td>\n",
       "      <td>[20459, 407, 3148, 13, 50256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1169</th>\n",
       "      <td>Oh, God, I feel terrible.</td>\n",
       "      <td>[[5812, 11, 1793, 11, 314, 1254, 7818, 13, 502...</td>\n",
       "      <td>Do you have a stomach ache, too?</td>\n",
       "      <td>[[5211, 345, 423, 257, 11384, 936, 258, 11, 11...</td>\n",
       "      <td>I know. I feel terrible.</td>\n",
       "      <td>[40, 760, 13, 314, 1254, 7818, 13, 50256]</td>\n",
       "      <td>I know. I feel terrible.</td>\n",
       "      <td>[40, 760, 13, 314, 1254, 7818, 13, 50256]</td>\n",
       "      <td>What?</td>\n",
       "      <td>[2061, 30, 50256]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1170 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    ctx  \\\n",
       "0                                Oh, sweetie, Im sorry.   \n",
       "1         Oh. Wow. Good job. Okay, um, can you do this?   \n",
       "2     Thanks. And, Sheldon, I know tonights the nigh...   \n",
       "3                                              Too bad.   \n",
       "4     Okay, so youd lose most of Jar Jar, all the tr...   \n",
       "...                                                 ...   \n",
       "1165                  Im available for experimentation.   \n",
       "1166         Sheldon, why are you ignoring your sister?   \n",
       "1167  Look, Sheldon, before you race off to the fert...   \n",
       "1168  Sheldon, Im sorry. Ill be happy to reimburse y...   \n",
       "1169                          Oh, God, I feel terrible.   \n",
       "\n",
       "                                                 ctx_tk  \\\n",
       "0     [[5812, 11, 6029, 494, 11, 1846, 7926, 13, 502...   \n",
       "1     [[5812, 13, 24755, 13, 4599, 1693, 13, 16805, ...   \n",
       "2     [[9690, 13, 843, 11, 34536, 11, 314, 760, 5680...   \n",
       "3                            [[23307, 2089, 13, 50256]]   \n",
       "4     [[16454, 11, 523, 345, 67, 4425, 749, 286, 153...   \n",
       "...                                                 ...   \n",
       "1165              [[3546, 1695, 329, 29315, 13, 50256]]   \n",
       "1166  [[3347, 25900, 11, 1521, 389, 345, 15482, 534,...   \n",
       "1167  [[8567, 11, 34536, 11, 878, 345, 3234, 572, 28...   \n",
       "1168  [[3347, 25900, 11, 1846, 7926, 13, 5821, 307, ...   \n",
       "1169  [[5812, 11, 1793, 11, 314, 1254, 7818, 13, 502...   \n",
       "\n",
       "                                                    lbl  \\\n",
       "0                                She called me dumbass.   \n",
       "1                                      Well never know.   \n",
       "2                                Oh, you shouldnt have.   \n",
       "3                         Youre going up Euclid Avenue?   \n",
       "4     Get rid of the trade route part? Then how woul...   \n",
       "...                                                 ...   \n",
       "1165  Thank you. Not necessary. We know everything t...   \n",
       "1166  Im not ignoring my sister. Im ignoring all of ...   \n",
       "1167                                   You mean dating?   \n",
       "1168  You dont need to reimburse me because Im not p...   \n",
       "1169                   Do you have a stomach ache, too?   \n",
       "\n",
       "                                                 lbl_tk  \\\n",
       "0            [[3347, 1444, 502, 13526, 562, 13, 50256]]   \n",
       "1                        [[5779, 1239, 760, 13, 50256]]   \n",
       "2           [[5812, 11, 345, 815, 429, 423, 13, 50256]]   \n",
       "3     [[1639, 260, 1016, 510, 48862, 312, 8878, 30, ...   \n",
       "4     [[3855, 5755, 286, 262, 3292, 6339, 636, 30, 3...   \n",
       "...                                                 ...   \n",
       "1165  [[10449, 345, 13, 1892, 3306, 13, 775, 760, 22...   \n",
       "1166  [[3546, 407, 15482, 616, 6621, 13, 1846, 15482...   \n",
       "1167                   [[1639, 1612, 10691, 30, 50256]]   \n",
       "1168  [[1639, 17666, 761, 284, 25903, 502, 780, 1846...   \n",
       "1169  [[5211, 345, 423, 257, 11384, 936, 258, 11, 11...   \n",
       "\n",
       "                                             prd_greedy  \\\n",
       "0                                         No, its okay.   \n",
       "1                                          I dont know.   \n",
       "2                                Thats very impressive.   \n",
       "3     You know, I, I, I think I, I, I think I, I thi...   \n",
       "4              I dont think I can go through with this.   \n",
       "...                                                 ...   \n",
       "1165                        Oh, great. Well, good luck.   \n",
       "1166                                     I dont follow.   \n",
       "1167                       Oh, gee, I dont know. Maybe.   \n",
       "1168  Oh, I dont mind. I just dont want to be respon...   \n",
       "1169                           I know. I feel terrible.   \n",
       "\n",
       "                                          prd_greedy_tk  \\\n",
       "0                      [2949, 11, 663, 8788, 13, 50256]   \n",
       "1                           [40, 17666, 760, 13, 50256]   \n",
       "2                     [817, 1381, 845, 8036, 13, 50256]   \n",
       "3     [1639, 760, 11, 314, 11, 314, 11, 314, 892, 31...   \n",
       "4     [40, 17666, 892, 314, 460, 467, 832, 351, 428,...   \n",
       "...                                                 ...   \n",
       "1165  [5812, 11, 1049, 13, 3894, 11, 922, 8458, 13, ...   \n",
       "1166                       [40, 17666, 1061, 13, 50256]   \n",
       "1167  [5812, 11, 308, 1453, 11, 314, 17666, 760, 13,...   \n",
       "1168  [5812, 11, 314, 17666, 2000, 13, 314, 655, 176...   \n",
       "1169          [40, 760, 13, 314, 1254, 7818, 13, 50256]   \n",
       "\n",
       "                                             prd_nbeams  \\\n",
       "0                                         No, its okay.   \n",
       "1                                          I dont know.   \n",
       "2                                Thats very impressive.   \n",
       "3     You know, I, I, I think I, I, I think I, I thi...   \n",
       "4              I dont think I can go through with this.   \n",
       "...                                                 ...   \n",
       "1165                        Oh, great. Well, good luck.   \n",
       "1166                                     I dont follow.   \n",
       "1167                       Oh, gee, I dont know. Maybe.   \n",
       "1168  Oh, I dont mind. I just dont want to be respon...   \n",
       "1169                           I know. I feel terrible.   \n",
       "\n",
       "                                          prd_nbeams_tk  \\\n",
       "0                      [2949, 11, 663, 8788, 13, 50256]   \n",
       "1                           [40, 17666, 760, 13, 50256]   \n",
       "2                     [817, 1381, 845, 8036, 13, 50256]   \n",
       "3     [1639, 760, 11, 314, 11, 314, 11, 314, 892, 31...   \n",
       "4     [40, 17666, 892, 314, 460, 467, 832, 351, 428,...   \n",
       "...                                                 ...   \n",
       "1165  [5812, 11, 1049, 13, 3894, 11, 922, 8458, 13, ...   \n",
       "1166                       [40, 17666, 1061, 13, 50256]   \n",
       "1167  [5812, 11, 308, 1453, 11, 314, 17666, 760, 13,...   \n",
       "1168  [5812, 11, 314, 17666, 2000, 13, 314, 655, 176...   \n",
       "1169          [40, 760, 13, 314, 1254, 7818, 13, 50256]   \n",
       "\n",
       "                                prd_sampling  \\\n",
       "0                              No. Its okay.   \n",
       "1                                        No.   \n",
       "2      Did you even ask for the ingredients?   \n",
       "3     Im coming, Penny.  Oh, look who it is.   \n",
       "4         Oh yeah, thats not bad. I love it.   \n",
       "...                                      ...   \n",
       "1165           Oh. Well, see you in a while.   \n",
       "1166    I dont think Penny was ignoring you.   \n",
       "1167                Oh, that is a good idea.   \n",
       "1168                           Its not fair.   \n",
       "1169                                   What?   \n",
       "\n",
       "                                        prd_sampling_tk  \n",
       "0                     [2949, 13, 6363, 8788, 13, 50256]  \n",
       "1                                     [2949, 13, 50256]  \n",
       "2     [11633, 345, 772, 1265, 329, 262, 9391, 30, 50...  \n",
       "3     [3546, 2406, 11, 25965, 13, 220, 3966, 11, 804...  \n",
       "4     [5812, 10194, 11, 29294, 407, 2089, 13, 314, 1...  \n",
       "...                                                 ...  \n",
       "1165  [5812, 13, 3894, 11, 766, 345, 287, 257, 981, ...  \n",
       "1166  [40, 17666, 892, 25965, 373, 15482, 345, 13, 5...  \n",
       "1167    [5812, 11, 326, 318, 257, 922, 2126, 13, 50256]  \n",
       "1168                      [20459, 407, 3148, 13, 50256]  \n",
       "1169                                  [2061, 30, 50256]  \n",
       "\n",
       "[1170 rows x 10 columns]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_char = get_dataframe_for_metrics(character_hg['test'], predictions_greedy, predictions_nbeams, predictions_sampling)\n",
    "df_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics For Character 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ccl_sim(ctx_lbl, ctx_cht, lbl_cht):\n",
    "    return ((1 - abs(ctx_lbl - ctx_cht))**2 + lbl_cht**2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Lib.BBMetrics import BBMetric\n",
    "\n",
    "def compute_set_metrics(model, model_2, character, character_2, test_set_name,\n",
    "                        context_sentences, label_responses, chatbot_responses, encoded_test_set,\n",
    "                        classifier_n_sentences=50, label_chatbot_symmetry=False,\n",
    "                        include_qualitative_sentences=False, verbose=True):\n",
    "    scores = {}\n",
    "    \n",
    "    lbl_text = 'label' if not label_chatbot_symmetry else 'chatbota'\n",
    "    cht_text = 'chatbot' if not label_chatbot_symmetry else 'chatbotb'\n",
    "    \n",
    "    scores['metadata'] = {}\n",
    "    scores['metadata']['dataset name'] = test_set_name\n",
    "    scores['metadata']['names'] = {\n",
    "        'context':'context'\n",
    "    }\n",
    "    if label_chatbot_symmetry:\n",
    "        scores['metadata']['names'][lbl_text] = character\n",
    "        scores['metadata']['names'][cht_text] = character_2\n",
    "    else:\n",
    "        scores['metadata']['names'][lbl_text] = 'label'\n",
    "        scores['metadata']['names'][cht_text] = character\n",
    "    \n",
    "    # 0) computes metrics for perplexity\n",
    "    metric = BBMetric.load_metric(\"semantic similarity\")\n",
    "    scores['semantic similarity'] = [metric.compute(sentences_a=context_sentences,\n",
    "                                            sentences_b=label_responses)]\n",
    "    scores['semantic similarity'].append(metric.compute(sentences_a=context_sentences,\n",
    "                                            sentences_b=chatbot_responses)),\n",
    "    scores['semantic similarity'].append(metric.compute(sentences_a=label_responses,\n",
    "                                              sentences_b=chatbot_responses))\n",
    "    scores['semantic similarity'].append(ccl_sim(scores['semantic similarity'][0]['score'],\n",
    "                                                 scores['semantic similarity'][1]['score'],\n",
    "                                                 scores['semantic similarity'][2]['score']))\n",
    "    scores['metadata']['semantic similarity'] = {\n",
    "        'ordering': ['context-'+lbl_text, 'context-'+cht_text, cht_text+'-'+lbl_text, 'ccl']\n",
    "    }\n",
    "    if verbose:\n",
    "        print('=== SEMANTIC SIMILARITY ===')\n",
    "        print('context-'+lbl_text+' similarity:   ', scores['semantic similarity'][0])\n",
    "        print('context-'+cht_text+' similarity: ', scores['semantic similarity'][1])\n",
    "        print(cht_text+'-'+lbl_text+' similarity:   ', scores['semantic similarity'][2])\n",
    "        print('ccl-sim similarity:            ', scores['semantic similarity'][3])\n",
    "    # 1) computes metrics for perplexity\n",
    "    if encoded_test_set is not None:\n",
    "        metric = BBMetric.load_metric(\"perplexity\")\n",
    "        if not label_chatbot_symmetry:\n",
    "            scores['perplexity'] = metric.compute(model=model, encoded_test_set=encoded_test_set)['score']\n",
    "            scores['metadata']['perplexity'] = {\n",
    "                'ordering': cht_text\n",
    "            }\n",
    "        else:\n",
    "            scores['perplexity'] = [metric.compute(model=model, encoded_test_set=encoded_test_set)['score']]\n",
    "            scores['perplexity'].append(metric.compute(model=model_2, encoded_test_set=encoded_test_set)['score'])\n",
    "            scores['metadata']['perplexity'] = {\n",
    "                'ordering': [lbl_text, cht_text]\n",
    "            }\n",
    "        if verbose:\n",
    "            print('===       PERPLEXITY     ===')\n",
    "            if label_chatbot_symmetry:\n",
    "                print(lbl_text + ' perplexity:         ', scores['perplexity'][0])\n",
    "                print(cht_text + ' perplexity:         ', scores['perplexity'][1])\n",
    "            else:\n",
    "                print(cht_text + ' perplexity:         ', scores['perplexity'])\n",
    "    elif verbose:\n",
    "        print(\"encoded_test_set not provided, skipping Perplexity.\")\n",
    "    # 2) computes metrics for bleu\n",
    "    metric = BBMetric.load_metric(\"bleu\")\n",
    "    scores['bleu'] = [metric.compute(predictions=label_responses, references=context_sentences)]\n",
    "    scores['bleu'].append(metric.compute(predictions=chatbot_responses, references=context_sentences))\n",
    "    scores['bleu'].append(metric.compute(predictions=chatbot_responses, references=label_responses))\n",
    "    scores['bleu'].append(ccl_sim(scores['bleu'][0]['score'],\n",
    "                                  scores['bleu'][1]['score'],\n",
    "                                  scores['bleu'][2]['score']))\n",
    "    scores['metadata']['bleu'] = {\n",
    "        'ordering': ['context-'+lbl_text, 'context-'+cht_text, cht_text+'-'+lbl_text, 'ccl']\n",
    "    }\n",
    "    if verbose:\n",
    "        print('===         BLEU         ===')\n",
    "        print('context-to-'+lbl_text+' bleu:      ', scores['bleu'][0])\n",
    "        print('context-to-'+cht_text+' bleu:    ', scores['bleu'][1])\n",
    "        print(lbl_text+'-to-'+cht_text+' bleu:      ', scores['bleu'][2])\n",
    "        print('ccl-sim bleu:            ', scores['bleu'][3])\n",
    "    # 3) computes metrics for rouge-L\n",
    "    metric = BBMetric.load_metric(\"rouge l\")\n",
    "    scores['rouge l'] = [metric.compute(predictions=label_responses, references=context_sentences)]\n",
    "    scores['rouge l'].append(metric.compute(predictions=chatbot_responses, references=context_sentences))\n",
    "    scores['rouge l'].append(metric.compute(predictions=chatbot_responses, references=label_responses))\n",
    "    scores['rouge l'].append(ccl_sim(scores['rouge l'][0]['score'],\n",
    "                                     scores['rouge l'][1]['score'],\n",
    "                                     scores['rouge l'][2]['score']))\n",
    "    scores['metadata']['rouge l'] = {\n",
    "        'ordering': ['context-'+lbl_text, 'context-'+cht_text, cht_text+'-'+lbl_text, 'ccl']\n",
    "    }\n",
    "    if verbose:\n",
    "        print('===        ROUGE-L       ===')\n",
    "        print('context-to-'+lbl_text+' rouge:     ', scores['rouge l'][0])\n",
    "        print('context-to-'+cht_text+' rouge:   ', scores['rouge l'][1])\n",
    "        print(lbl_text+'-to-'+cht_text+' rouge:     ', scores['rouge l'][2])\n",
    "        print('ccl-sim rouge:            ', scores['rouge l'][3])\n",
    "    # 4) computes metrics for distinct\n",
    "    metric = BBMetric.load_metric(\"distinct\")\n",
    "    scores['distinct'] = [metric.compute(sentences=context_sentences)]\n",
    "    scores['distinct'].append(metric.compute(sentences=label_responses))\n",
    "    scores['distinct'].append(metric.compute(sentences=chatbot_responses))\n",
    "    scores['metadata']['distinct'] = {\n",
    "        'ordering': ['context', lbl_text, cht_text]\n",
    "    }\n",
    "    if verbose:\n",
    "        print('===       DISTINCT      ===')\n",
    "        print('context distinct:          ', scores['distinct'][0])\n",
    "        print(lbl_text+' distinct:          ', scores['distinct'][1])\n",
    "        print(cht_text+' distinct:          ', scores['distinct'][2])\n",
    "        \n",
    "    # 6) computes emotion metric\n",
    "    metric = BBMetric.load_metric(\"emotion\")\n",
    "    scores['emotion'] = [metric.compute(sentences=context_sentences)]\n",
    "    scores['emotion'].append(metric.compute(sentences=label_responses))\n",
    "    scores['emotion'].append(metric.compute(sentences=chatbot_responses))\n",
    "    scores['emotion'].append(sp.stats.stats.pearsonr(scores['emotion'][1]['score'],\n",
    "                                                     scores['emotion'][2]['score'])[0])\n",
    "    scores['metadata']['emotion'] = {\n",
    "        'ordering': ['context-'+lbl_text, 'context-'+cht_text, cht_text+'-'+lbl_text, cht_text+'-'+lbl_text+' correlation']\n",
    "    }\n",
    "    if verbose:\n",
    "        print('===       EMOTION       ===')\n",
    "        print('context emotions:            \\n', list(zip(scores['emotion'][0]['label'], scores['emotion'][0]['score'])))\n",
    "        print(lbl_text+' emotions:              \\n', list(zip(scores['emotion'][1]['label'], scores['emotion'][1]['score'])))\n",
    "        print(cht_text+' emotions:            \\n', list(zip(scores['emotion'][2]['label'], scores['emotion'][2]['score'])))\n",
    "        print(lbl_text+'-'+cht_text+'emotion corr:  \\n', scores['emotion'][3])\n",
    "    # 8) computes sas metric\n",
    "    metric = BBMetric.load_metric(\"semantic answer similarity\")\n",
    "    scores['semantic answer similarity'] = [metric.compute(predictions=context_sentences,\n",
    "                                                    references=label_responses)]\n",
    "    scores['semantic answer similarity'].append(metric.compute(predictions=context_sentences,\n",
    "                                                        references=chatbot_responses))\n",
    "    scores['semantic answer similarity'].append(metric.compute(predictions=label_responses,\n",
    "                                                        references=chatbot_responses))\n",
    "    scores['semantic answer similarity'].append(ccl_sim(scores['semantic answer similarity'][0]['score'],\n",
    "                                                        scores['semantic answer similarity'][1]['score'],\n",
    "                                                        scores['semantic answer similarity'][2]['score']))\n",
    "    scores['metadata']['semantic answer similarity'] = {\n",
    "        'ordering': ['context-'+lbl_text, 'context-'+cht_text, cht_text+'-'+lbl_text, 'ccl']\n",
    "    }\n",
    "    if verbose:\n",
    "        print('===         SAS         ===')\n",
    "        print('context-'+lbl_text+' sas:          ', scores['semantic answer similarity'][0])\n",
    "        print('context-'+cht_text+' sas:        ', scores['semantic answer similarity'][1])\n",
    "        print(lbl_text+'-'+cht_text+' sas:          ', scores['semantic answer similarity'][2])\n",
    "        print('ccl-sim sas:               ', scores['semantic answer similarity'][3])\n",
    "    # 9) computes metrics for semantic classifier\n",
    "    metric = BBMetric.load_metric(\"semantic classifier\")\n",
    "    start_time = time.time()\n",
    "    scores['semantic classifier'] = [metric.compute(character=character, character_dict=character_dict, \n",
    "                                                   base_folder=base_folder, sentences=label_responses,\n",
    "                                                   n_sentences=classifier_n_sentences)]\n",
    "    scores['semantic classifier'].append(metric.compute(character=character, character_dict=character_dict, \n",
    "                                                   base_folder=base_folder, sentences=chatbot_responses,\n",
    "                                                   n_sentences=classifier_n_sentences))\n",
    "    end_time = time.time()\n",
    "    scores['metadata']['semantic classifier'] = {\n",
    "        'ordering': [lbl_text, cht_text]\n",
    "    }\n",
    "    if verbose:\n",
    "        print('=== SEMANTIC CLASSIFIER ===')\n",
    "        print('sem-classifier '+lbl_text+':                ', scores['semantic classifier'][0])\n",
    "        print('sem-classifier '+cht_text+':                  ', scores['semantic classifier'][1])\n",
    "        print('time elapsed computing semantic classifier:  {:.2f} s'.format(end_time - start_time))\n",
    "    if not label_chatbot_symmetry and os.path.exists(os.path.join(os.getcwd(), \"Data\", \"Characters\", character, \"humancoherence.csv\")):\n",
    "        scores['human'] = {}\n",
    "        metric = BBMetric.load_metric(\"human - coherence\")\n",
    "        scores['human']['coherence'] = metric.compute(filepath=os.path.join(os.getcwd(), \"Data\", \"Characters\",\n",
    "                                                                            character, \"humancoherence.csv\"))\n",
    "        metric = BBMetric.load_metric(\"human - style\")\n",
    "        scores['human']['style'] = metric.compute(filepath=os.path.join(os.getcwd(), \"Data\", \"Characters\",\n",
    "                                                                        character, \"humanstyle.csv\"))\n",
    "        metric = BBMetric.load_metric(\"human - consistency\")\n",
    "        scores['human']['consistency'] = metric.compute(filepath=os.path.join(os.getcwd(), \"Data\", \"Characters\",\n",
    "                                                                              character, \"humanconsistency.csv\"))\n",
    "        scores['metadata']['human'] = {\n",
    "            'ordering': {\n",
    "                'coherence': cht_text,\n",
    "                'consistency': cht_text,\n",
    "                'style': cht_text\n",
    "            }\n",
    "        }\n",
    "        if verbose:\n",
    "            print('===    HUMAN METRICS    ===')\n",
    "            print('coherence:                 ', scores['human']['coherence'])\n",
    "            print('consistency:               ', scores['human']['consistency'])\n",
    "            print('style:                     ', scores['human']['style'])\n",
    "    elif verbose:\n",
    "        print(\"Symmetric mode, skipping Human metrics.\")\n",
    "    if include_qualitative_sentences:\n",
    "        sentences_df = {}\n",
    "        sentences_df['context'] = context_sentences\n",
    "        sentences_df[lbl_text] = label_responses\n",
    "        sentences_df[cht_text] = chatbot_responses\n",
    "        scores['sentences'] = sentences_df\n",
    "        if verbose:\n",
    "            print('===      SENTENCES      ===')\n",
    "            for i in range(len(context_sentences)):\n",
    "                print(\"* context: \", context_sentences[i])\n",
    "                print(\"* \" + lbl_text + \":\", label_responses[i])\n",
    "                print(\"* \" + cht_text + \":\", chatbot_responses[i])\n",
    "                print()\n",
    "    elif verbose:\n",
    "        print(\"Skipping sentence outputting.\")\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "set_size = 10\n",
    "i = 30\n",
    "print(\"##### Set (Size \" + str(set_size) + \") #####\")\n",
    "context_sentences = list(df_char['ctx'][i:i+set_size])\n",
    "chatbot_responses = list(df_char['prd_greedy'][i:i+set_size])\n",
    "label_responses   = list(df_char['lbl'][i:i+set_size])\n",
    "compute_set_metrics(model, None,\n",
    "                    context_sentences, label_responses, chatbot_responses, character, encoded_test_set)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"##### Full Test Set #####\")\n",
    "context_sentences = list(df_char['ctx'])\n",
    "chatbot_responses = list(df_char['prd_greedy'])\n",
    "label_responses   = list(df_char['lbl'])\n",
    "scores = compute_set_metrics(model, None,\n",
    "                             character, None, character + \" dataset\",\n",
    "                             context_sentences, label_responses, chatbot_responses, encoded_test_set,\n",
    "                             classifier_n_sentences=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_json(metrics_folder, character+'_base_metrics', scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Between Different Sampling Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"##### Greedy vs. N-Beams #####\")\n",
    "context_sentences = list(df_char['ctx'])\n",
    "greedy_responses  = list(df_char['prd_greedy'])\n",
    "nbeams_responses  = list(df_char['prd_nbeams'])\n",
    "scores['greedy_vs_nbeams'] = compute_set_metrics(None, None,\n",
    "                                                 character, character, character + \" dataset\",\n",
    "                                                 context_sentences,\n",
    "                                                 greedy_responses,\n",
    "                                                 nbeams_responses,\n",
    "                                                 None,\n",
    "                                                 classifier_n_sentences=75, label_chatbot_symmetry=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split == True:\n",
    "    save_as_json(metrics_folder, character+'_greedy_vs_nbeams_metrics', scores['greedy_vs_nbeams'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"##### Greedy vs. Sampling #####\")\n",
    "context_sentences   = list(df_char['ctx'])\n",
    "greedy_responses    = list(df_char['prd_greedy'])\n",
    "sampling_responses  = list(df_char['prd_sampling'])\n",
    "scores['greedy_vs_sampling'] = compute_set_metrics(None, None,\n",
    "                                                   character, character, character + \" dataset\",\n",
    "                                                   context_sentences,\n",
    "                                                   greedy_responses,\n",
    "                                                   sampling_responses,\n",
    "                                                   None,\n",
    "                                                   classifier_n_sentences=75, label_chatbot_symmetry=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split == True:\n",
    "    save_as_json(metrics_folder, character+'_greedy_vs_sampling_metrics', scores['greedy_vs_sampling'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"##### N-Beams vs. Sampling #####\")\n",
    "context_sentences   = list(df_char['ctx'])\n",
    "nbeams_responses    = list(df_char['prd_nbeams'])\n",
    "sampling_responses  = list(df_char['prd_sampling'])\n",
    "scores['nbeams_vs_sampling'] = compute_set_metrics(None, None,\n",
    "                                                   character, character, character + \" dataset\",\n",
    "                                                   context_sentences,\n",
    "                                                   nbeams_responses,\n",
    "                                                   sampling_responses,\n",
    "                                                   None,\n",
    "                                                   classifier_n_sentences=75, label_chatbot_symmetry=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split == True:\n",
    "    save_as_json(metrics_folder, character+'_nbeams_vs_sampling_metrics', scores['nbeams_vs_sampling'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split == True:    \n",
    "    scores = {}\n",
    "    scores['greedy_vs_nbeams'] = load_from_json(\n",
    "        filepath=metrics_folder,\n",
    "        filename=character+'_greedy_vs_nbeams_metrics'\n",
    "    )\n",
    "    scores['greedy_vs_sampling'] = load_from_json(\n",
    "        filepath=metrics_folder,\n",
    "        filename=character+'_greedy_vs_sampling_metrics'\n",
    "    )\n",
    "    scores['nbeams_vs_sampling'] = load_from_json(\n",
    "        filepath=metrics_folder,\n",
    "        filename=character+'_nbeams_vs_sampling_metrics'\n",
    "    )\n",
    "    \n",
    "    os.remove(os.path.join(\n",
    "        metrics_folder,\n",
    "        character+'_greedy_vs_nbeams_metrics.json'\n",
    "    ))\n",
    "    os.remove(os.path.join(\n",
    "        metrics_folder,\n",
    "        character+'_greedy_vs_sampling_metrics.json'\n",
    "    ))\n",
    "    os.remove(os.path.join(\n",
    "        metrics_folder,\n",
    "        character+'_nbeams_vs_sampling_metrics.json'\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_json(metrics_folder, character+'_sampling_comparison_metrics', scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Between Character vs Non-Finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_def_sampling = get_predictions_cached(sample_questions, model_def,\n",
    "                                                  os.path.join(in_folder_def, 'from_' + character + '_df_' + '_sampling.json'),\n",
    "                                                  \"Sampling\", override_predictions=override_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_char_def = get_dataframe_for_metrics(character_hg['test'], None, None, predictions_def_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for i in range(1):\n",
    "    print(\"##### Sample \" + str(i+1) + \" #####\")\n",
    "    context_sentence   = df_char['ctx'][i]\n",
    "    character_response = df_char['prd_sampling'][i]\n",
    "    default_response   = df_char_def['prd_sampling'][i]\n",
    "    compute_sample_metrics(context_sentence, default_response, character_response, label_chatbot_symmetry=True)\n",
    "    print()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "set_size = 50\n",
    "i = 30\n",
    "print(\"##### Set (Size \" + str(set_size) + \") #####\")\n",
    "context_sentences   = list(df_char['ctx'][i:i+set_size])\n",
    "character_responses = list(df_char['prd_sampling'][i:i+set_size])\n",
    "default_responses   = list(df_char_def['prd_sampling'][i:i+set_size])\n",
    "compute_set_metrics(None, None,\n",
    "                    context_sentences, default_responses, character_responses, character, label_chatbot_symmetry=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"##### Full Test Set #####\")\n",
    "context_sentences   = list(df_char['ctx'])\n",
    "character_responses = list(df_char['prd_sampling'])\n",
    "default_responses   = list(df_char_def['prd_sampling'])\n",
    "scores = compute_set_metrics(model, model_def, character, 'Default', character + \" dataset\",\n",
    "                             context_sentences, \n",
    "                             character_responses, \n",
    "                             default_responses,\n",
    "                             encoded_test_set,\n",
    "                             classifier_n_sentences=75,\n",
    "                             label_chatbot_symmetry=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_json(metrics_folder, character+'_vs_nonfinetuned_metrics', scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Between Character 1 & Character 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_small(sample_questions, model, generation_method):\n",
    "    print(\"Creating predictions\")\n",
    "    predictions = list()\n",
    "    for x in tqdm(sample_questions):\n",
    "        tokenized_question = tokenizer.encode(x + tokenizer.eos_token, return_tensors='tf')\n",
    "        max_length = 128 + tokenized_question.shape[1]\n",
    "        if generation_method == \"Greedy\":\n",
    "            generated_answer = model.generate(tokenized_question,\n",
    "                                pad_token_id=tokenizer.eos_token_id, max_length=max_length)[0].numpy().tolist()\n",
    "        elif generation_method == \"Beam Search\":\n",
    "            generated_answer = model.generate(tokenized_question,\n",
    "                                         pad_token_id=tokenizer.eos_token_id, max_length=max_length,\n",
    "                                         n_beams=n_beams)[0].numpy().tolist()\n",
    "        elif generation_method == \"Sampling\":\n",
    "                b = True\n",
    "                c = 0\n",
    "                while b:\n",
    "                    generated_answer = model.generate(tokenized_question,\n",
    "                                                 pad_token_id=tokenizer.eos_token_id, max_length=max_length,\n",
    "                                                 do_sample=True, top_k=top_k, top_p=top_p)[0].numpy().tolist()\n",
    "                    \n",
    "                    c+= 1\n",
    "                    if len(generated_answer[len(tokenized_question[0]):])>1:\n",
    "                        b = False         \n",
    "                    if c>100: \n",
    "                        generated_answer[len(tokenized_question[0]):] = tokenizer.encode('hi') + [tokenizer.eos_token_id]\n",
    "                        break \n",
    "                        \n",
    "        predictions.append(generated_answer[len(tokenized_question[0]):])\n",
    "        \n",
    "        assert all([len(p)>1 for p in predictions])\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-4624fb5057215352\n",
      "Reusing dataset csv (C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\cache\\csv\\default-4624fb5057215352\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee6f85aa977478b9f544560694bb39a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\cache\\csv\\default-4624fb5057215352\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-0024dfc524fce5dc.arrow\n"
     ]
    }
   ],
   "source": [
    "df_common = load_dataset('csv',\n",
    "                         data_files=os.path.join(base_folder, 'Data', 'common_dataset.csv'), \n",
    "                         cache_dir=os.path.join(base_folder, \"cache\"))\n",
    "\n",
    "df_common = df_common.remove_columns(['source'])\n",
    "tokenized_common_hg = df_common['train'].map(preprocess_function, batched=False)\n",
    "\n",
    "encoded_common_set = tokenized_common_hg.to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'context/0'],\n",
       "        num_rows: 35\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec={'input_ids': TensorSpec(shape=(None, None), dtype=tf.int64, name=None), 'attention_mask': TensorSpec(shape=(None, None), dtype=tf.int64, name=None), 'labels': TensorSpec(shape=(None, None), dtype=tf.int64, name=None)}>"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_common_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 35/35 [00:51<00:00,  1.47s/it]\n"
     ]
    }
   ],
   "source": [
    "predictions_1_sampling = get_predictions_small(df_common['train']['context/0'], model, \"Sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 35/35 [00:55<00:00,  1.58s/it]\n"
     ]
    }
   ],
   "source": [
    "predictions_2_sampling = get_predictions_small(df_common['train']['context/0'], model_2, \"Sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 35/35 [00:00<00:00, 3179.22it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 35/35 [00:00<00:00, 2914.10it/s]\n"
     ]
    }
   ],
   "source": [
    "df_common_char_1 = get_dataframe_for_metrics(df_common['train'], None, None, predictions_1_sampling)\n",
    "df_common_char_2 = get_dataframe_for_metrics(df_common['train'], None, None, predictions_2_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Sheldon  Vs. Barney #####\n",
      "=== SEMANTIC SIMILARITY ===\n",
      "context-chatbota similarity:    {'score': 0.3438683748245239, 'std': 0.20891159772872925}\n",
      "context-chatbotb similarity:  {'score': 0.3270496428012848, 'std': 0.15701977908611298}\n",
      "chatbotb-chatbota similarity:    {'score': 0.2786218225955963, 'std': 0.1434633731842041}\n",
      "ccl-sim similarity:             0.5221377628634416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.34it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===       PERPLEXITY     ===\n",
      "chatbota perplexity:          67.96138565177583\n",
      "chatbotb perplexity:          60.62977505345756\n",
      "===         BLEU         ===\n",
      "context-to-chatbota bleu:       {'score': 0.0}\n",
      "context-to-chatbotb bleu:     {'score': 0.010537053648441798}\n",
      "chatbota-to-chatbotb bleu:       {'score': 0.0}\n",
      "ccl-sim bleu:             0.4895184611013533\n",
      "===        ROUGE-L       ===\n",
      "context-to-chatbota rouge:      {'score': 0.14642306237639446, 'std': 0.2228028221728049}\n",
      "context-to-chatbotb rouge:    {'score': 0.0682674913445552, 'std': 0.11311735470852449}\n",
      "chatbota-to-chatbotb rouge:      {'score': 0.05256667602067202, 'std': 0.06728318343210075}\n",
      "ccl-sim rouge:             0.42628020332374833\n",
      "===       DISTINCT      ===\n",
      "context distinct:           {'score': 0.10984328048288133, 'std': 0.06188826948933667}\n",
      "chatbota distinct:           {'score': 0.13308285410006637, 'std': 0.06287864172395086}\n",
      "chatbotb distinct:           {'score': 0.12777674962949512, 'std': 0.056132893489592085}\n",
      "===       EMOTION       ===\n",
      "context emotions:            \n",
      " [('sadness', 0.014541776718605043), ('joy', 0.34016804504208265), ('love', 0.11487390943678162), ('anger', 0.37266245127040226), ('fear', 0.15484204553899222), ('surprise', 0.002911792164585287)]\n",
      "chatbota emotions:              \n",
      " [('sadness', 0.12347597471837486), ('joy', 0.43841626886361545), ('love', 0.05793287606627148), ('anger', 0.31529313305925044), ('fear', 0.061718685196163796), ('surprise', 0.0031630753905379345)]\n",
      "chatbotb emotions:            \n",
      " [('sadness', 0.051894612326785654), ('joy', 0.5174019124791293), ('love', 0.03459250985428558), ('anger', 0.2805034483317286), ('fear', 0.11204961632278615), ('surprise', 0.0035578881789531027)]\n",
      "chatbota-chatbotbemotion corr:  \n",
      " 0.9642220309098339\n",
      "===         SAS         ===\n",
      "context-chatbota sas:           {'score': 0.1494990885257721, 'std': 0.22014522552490234}\n",
      "context-chatbotb sas:         {'score': 0.08029940724372864, 'std': 0.12609511613845825}\n",
      "chatbota-chatbotb sas:           {'score': 0.08741191774606705, 'std': 0.1250864863395691}\n",
      "ccl-sim sas:                0.4370150383447473\n",
      "=== SEMANTIC CLASSIFIER ===\n",
      "sem-classifier chatbota:                 {'score': 0.5653219819068909, 'std': 0.45856615900993347}\n",
      "sem-classifier chatbotb:                   {'score': 0.20315369963645935, 'std': 0.37240856885910034}\n",
      "time elapsed computing semantic classifier:  11.28 s\n",
      "Symmetric mode, skipping Human metrics.\n",
      "===      SENTENCES      ===\n",
      "* context:  Barney, this is about the building.\n",
      "* chatbota: Are you sure? I see no building on H-O.\n",
      "* chatbotb: I don't know, Barney, I know about the NRDC, and they're pretty cool about it. I mean, even if they were a corporation, they'd be a recognized charitable org, which would probably be a lot better than the crap out of the Empire State Building.\n",
      "\n",
      "* context:  All right. I'll be right there. Stay where you are.\n",
      "* chatbota: All right.\n",
      "* chatbotb: No! I'll be right there.\n",
      "\n",
      "* context:  I think there's a pretty girl smiling at me there.\n",
      "* chatbota: You do?\n",
      "* chatbotb: Oh, yeah, there she is.\n",
      "\n",
      "* context:  I love you, man.\n",
      "* chatbota: Yeah, me, too. Just think how you would feel if you were referred to as your husband and wife and you worked on such a high school graduate-level physics research project.\n",
      "* chatbotb: You're a jerk.\n",
      "\n",
      "* context:  Not even if she's hot?\n",
      "* chatbota: No, its okay, she wont be a huge pain in the ass.\n",
      "* chatbotb: You have my full support.\n",
      "\n",
      "* context:  Soft kitty, warm kitty Little ball of fur\n",
      "* chatbota: Soft kitty, little ball of fur\n",
      "* chatbotb: We love you.\n",
      "\n",
      "* context:  Penny.\n",
      "* chatbota: Heres a fun Postal fact. The inner side of our mailbox is under federal jurisdiction. So if I was under the impression that UPS was under federal jurisdiction, Id be under the impression that I was, in fact, under federal jurisdiction.\n",
      "* chatbotb: Hey, you know, it's just...\n",
      "\n",
      "* context:  Oh. Sheldon, thank you. Thats so romantic. But what about Rajesh? He was okay with you choosing the name?\n",
      "* chatbota: Thank you for making me tea. Its time I teach him to make good decisions.\n",
      "* chatbotb: Okay. He was a little more mellow. He was also a bit resentful. But it wasn't too bad.\n",
      "\n",
      "* context:  I didnt break it. I, I guess Stuart sold it to me like this.\n",
      "* chatbota: No. Im sorry.\n",
      "* chatbotb: Oh. I'm sorry. I thought you broke your ass.\n",
      "\n",
      "* context:  Be careful.\n",
      "* chatbota: This is going to be a long night.\n",
      "* chatbotb: What? No!\n",
      "\n",
      "* context:  But why would anyone go near that dog?\n",
      "* chatbota: Well, I dont know, its a dog-infested fire hazard.\n",
      "* chatbotb: Did you know that about 50 years ago, when a group of environmentalists tried to put a liveable kller in a cage?\n",
      "\n",
      "* context:  Expecto Patronum!\n",
      "* chatbota: Excuse me. I dont see how you missed that.\n",
      "* chatbotb: Barney, what is my job? It is so clear, it's hard to see who is the boss.\n",
      "\n",
      "* context:  Ron Weasley.\n",
      "* chatbota: Oh. I was thinking about that show earlier. Dumbledore had that look in his suit. Thats nice.\n",
      "* chatbotb: Ted Mosby.\n",
      "\n",
      "* context:  I spoke a different language?\n",
      "* chatbota: Oh, gee, Raj, you did.\n",
      "* chatbotb: No, I said what?\n",
      "\n",
      "* context:  Harry?\n",
      "* chatbota: Its not a big deal, Sheldon. Its just, I was thinking that seeing as youre my girlfriend and youre my friend, I should be able to have our share of the time.\n",
      "* chatbotb: It's Robin. You're Ted's best friend.\n",
      "\n",
      "* context:  OK. First Bender, then Flexo, then Fry.\n",
      "* chatbota: I dont know. Any sufficiently advanced technology is indistinguishable from ordinary man. I bet I can handle the thought of a man with a fridge full of pickled jello.\n",
      "* chatbotb: Hey, so you know how I love you? Well, okay. I never pictured Barney waking up at 4am in the morning, knowing that the Central Park Zoo is located only a few blocks away from the Smithsonian.\n",
      "\n",
      "* context:  Just relax, Bender. Tomorrow we'll pry you down, have a nice breakfast and then go hunt down and slaughter that ancient evil.\n",
      "* chatbota: We will not pry you so hard, I doubt you have anything better to do.\n",
      "* chatbotb: That's the spirit!\n",
      "\n",
      "* context:  I'm too scared.\n",
      "* chatbota: If it makes you feel any better, I found a Korean mans business card tucked into my cleavage.\n",
      "* chatbotb: It's okay. It's time for bed.\n",
      "\n",
      "* context:  Dr. Zoidberg? Are you OK?\n",
      "* chatbota: Im fine.\n",
      "* chatbotb: My mother is, yes.\n",
      "\n",
      "* context:  Fry, thank God we found you.\n",
      "* chatbota: I was having a good time, and you ruined it.\n",
      "* chatbotb: No, no, no. You do not understand. This is a group decision, I know. We will not be fooled by the lies you have told to put your name in that box.\n",
      "\n",
      "* context:  I will not fight you.\n",
      "* chatbota: I cant. Im lactose intolerant.\n",
      "* chatbotb: Come on, guys. If you can't fight us...then... You'll never be able to look the guy in the eye. Oh, really?\n",
      "\n",
      "* context:  Lord Vader, what about Leia and theWookiee?\n",
      "* chatbota: They were on the Clone Wars TV series.\n",
      "* chatbotb: Oh, there's only one explanation for that. She's a Star Wars fan?\n",
      "\n",
      "* context:  The Emperor's coming here?\n",
      "* chatbota: Of course youre aware.\n",
      "* chatbotb: Yeah, we knew that.\n",
      "\n",
      "* context:  Shall I hold them?\n",
      "* chatbota: I dont know. I thought you said you were keeping them.\n",
      "* chatbotb: Yes, I was just about to give you an even bigger hug.\n",
      "\n",
      "* context:  Lord Vader, what about Leia and theWookiee?\n",
      "* chatbota: Thats a different story.\n",
      "* chatbotb: All right, I'm done with that, okay? I mean, I don't mean The Frozen Snowshoe. I mean, I know not what Frozen Snowshoe looks like, but... yeah, come on. I mean, seriously, I can't remember who sings that song,000 years in. Who sings that song?\n",
      "\n",
      "* context:  Oh! Joey uh, were you in our room last night?\n",
      "* chatbota: No, Im just in the shower.\n",
      "* chatbotb: Lily, we found a porno starring Ted Mosby.\n",
      "\n",
      "* context:  Hey.\n",
      "* chatbota: Hey.\n",
      "* chatbotb: Oh, come on. That was great.\n",
      "\n",
      "* context:  Joey... are you sure? I mean, I know how much you love him!\n",
      "* chatbota: I love him more than anybody.\n",
      "* chatbotb: I'm going to go wash my hair.\n",
      "\n",
      "* context:  Ok, ten.\n",
      "* chatbota: You know, if youre not going to help, at least take a hint. I was 13 years old at the zoo.\n",
      "* chatbotb: You are, uh, looking pretty young.\n",
      "\n",
      "* context:  Joey, Ross is gonna be here any second, would you mind watching Ben for me while I use the ladies' room?\n",
      "* chatbota: Sure.\n",
      "* chatbotb: Sure. Just don't look as good as you think you do!\n",
      "\n",
      "* context:  What are you doing for a living?\n",
      "* chatbota: Well, Im a theoretical physicist whose work has demonstrated the need for permanent mental warfare.\n",
      "* chatbotb: I'm an architect.\n",
      "\n",
      "* context:  How are you doing?\n",
      "* chatbota: You must be doing something wrong.\n",
      "* chatbotb: Oh, okay, let me grab my wife.\n",
      "\n",
      "* context:  Where are you going to?\n",
      "* chatbota: Youre going to come with me to your mother.\n",
      "* chatbotb: Philadelphia.\n",
      "\n",
      "* context:  What are you wearing?\n",
      "* chatbota: Ive decided not to make fun of Leonard. He makes fun of himself.\n",
      "* chatbotb: A suede jacket.\n",
      "\n",
      "* context:  What do you want to do tonight?\n",
      "* chatbota: We were going out, we were building a bunch of voltages and then when we ran out we didnt want to take it outside.\n",
      "* chatbotb: I was thinking either go out or party number three. Or maybe party number three. Or... what?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"##### \" + character + \"  Vs. \" + character_2 + \" #####\")\n",
    "context_sentences   = list(df_common_char_1['ctx'])\n",
    "chatbot_responses   = list(df_common_char_1['prd_sampling'])\n",
    "chatbot_2_responses = list(df_common_char_2['prd_sampling'])\n",
    "scores = compute_set_metrics(model, model_2, character, character_2, \"common small dataset\",\n",
    "                             context_sentences, chatbot_responses, chatbot_2_responses, encoded_common_set,\n",
    "                             include_qualitative_sentences=True, label_chatbot_symmetry=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_json(metrics_folder, character+'_vs_'+character_2+'_metrics', scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

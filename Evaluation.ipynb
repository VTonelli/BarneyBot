{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data.data_dicts import character_dict, source_dict, random_state\n",
    "\n",
    "model_name = 'microsoft/DialoGPT-small'\n",
    "character = 'Barney' # 'Barney' | 'Sheldon' | 'Harry' | 'Fry' | 'Vader' | 'Joey' | 'Phoebe' | 'Bender' | Default'\n",
    "character_2 = 'Sheldon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount google drive\n",
    "import os\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive',force_remount=True)\n",
    "    base_folder = '/content/drive/My Drive/unibo/NLP_project/BarneyBot'\n",
    "    os.system(\"pip install datasets\")\n",
    "    os.system(\"pip install transformers\")\n",
    "    os.system(\"pip install rouge_score\")\n",
    "    os.system(\"pip install -U sentence-transformers\")\n",
    "else:\n",
    "    base_folder = os.getcwd()\n",
    "    \n",
    "in_folder = os.path.join(base_folder, 'Data', 'Characters', character)\n",
    "if not os.path.exists(in_folder):\n",
    "    os.makedirs(in_folder)\n",
    "out_folder = os.path.join(base_folder, 'Data', 'Characters', character)\n",
    "if not os.path.exists(out_folder):\n",
    "    os.makedirs(out_folder)\n",
    "    \n",
    "in_folder_2 = os.path.join(base_folder, 'Data', 'Characters', character_2)\n",
    "if not os.path.exists(in_folder_2):\n",
    "    os.makedirs(in_folder_2)\n",
    "out_folder_2 = os.path.join(base_folder, 'Data', 'Characters', character_2)\n",
    "if not os.path.exists(out_folder_2):\n",
    "    os.makedirs(out_folder_2)\n",
    "    \n",
    "in_folder_def = os.path.join(base_folder, 'Data', 'Characters', 'Default')\n",
    "if not os.path.exists(in_folder_def):\n",
    "    os.makedirs(in_folder_def)\n",
    "out_folder_def = os.path.join(base_folder, 'Data', 'Characters', 'Default')\n",
    "if not os.path.exists(out_folder_def):\n",
    "    os.makedirs(out_folder_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "def load_df(character):\n",
    "    dataset_path = os.path.join(base_folder, \"Data\", \"Characters\", character, character+'.csv')\n",
    "    \n",
    "    character_hg = load_dataset('csv', \n",
    "                                data_files=dataset_path, \n",
    "                                cache_dir=os.path.join(base_folder, \"cache\"))\n",
    "    \n",
    "    # 85% train / 10% test / 5% validation\n",
    "    train_test_hg = character_hg['train'].train_test_split(test_size=0.15, seed=random_state)\n",
    "    test_val = train_test_hg['test'].train_test_split(test_size=0.33, seed=random_state)\n",
    "    \n",
    "    \n",
    "    character_hg = DatasetDict({\n",
    "        'train': train_test_hg['train'],\n",
    "        'test': test_val['train'],\n",
    "        'val': test_val['test']\n",
    "    })\n",
    "    \n",
    "    return character_hg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_conv(row, tokenizer):\n",
    "    MAX_LENGTH = 512\n",
    "    row = list(reversed(list(row.values())))\n",
    "    model_inputs = tokenizer(row)\n",
    "    tokenizer_pad_token_id = tokenizer.encode('#')[0]\n",
    "    for i in range(len(model_inputs['input_ids'])):\n",
    "        model_inputs['input_ids'][i].append(tokenizer.eos_token_id)\n",
    "        model_inputs['attention_mask'][i].append(1)\n",
    "    model_inputs['input_ids'] = [item for sublist in model_inputs['input_ids'] for item in sublist]\n",
    "    model_inputs['attention_mask'] = [item for sublist in model_inputs['attention_mask'] for item in sublist]\n",
    "    if MAX_LENGTH > len(model_inputs['input_ids']):\n",
    "        model_inputs['input_ids'] += [tokenizer_pad_token_id] * (MAX_LENGTH - len(model_inputs['input_ids']))\n",
    "        model_inputs['attention_mask'] += [0] * (MAX_LENGTH - len(model_inputs['attention_mask']))\n",
    "    elif MAX_LENGTH < len(model_inputs['input_ids']):\n",
    "        model_inputs['input_ids'] = model_inputs['input_ids'][:MAX_LENGTH-1]\n",
    "        model_inputs['input_ids'][-1] = tokenizer.eos_token_id\n",
    "        model_inputs['attention_mask'] = model_inputs['attention_mask'][:MAX_LENGTH-1]\n",
    "        model_inputs['attention_mask'][-1] = 1\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    tokenizer.pad_token = '#'\n",
    "    model_inputs = construct_conv(examples, tokenizer)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-0c3b759885fd8d5f\n",
      "Reusing dataset csv (D:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBot\\BarneyBot\\cache\\csv\\default-0c3b759885fd8d5f\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "237cedc8cfca45a8a1073444ef32b4e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at D:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBot\\BarneyBot\\cache\\csv\\default-0c3b759885fd8d5f\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-c79122a57f55334a.arrow and D:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBot\\BarneyBot\\cache\\csv\\default-0c3b759885fd8d5f\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-4faecb1c641ab4b9.arrow\n",
      "Loading cached split indices for dataset at D:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBot\\BarneyBot\\cache\\csv\\default-0c3b759885fd8d5f\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-7b672756e3cf7bb2.arrow and D:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBot\\BarneyBot\\cache\\csv\\default-0c3b759885fd8d5f\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-bb39fb19125a9103.arrow\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"HF_DATASETS_CACHE\"] = os.path.join(base_folder, \"cache\")\n",
    "character_hg = load_df(character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_folder = os.path.join(out_folder, character_dict[character]['checkpoint_folder'])\n",
    "checkpoint_folder_2 = os.path.join(out_folder_2, character_dict[character_2]['checkpoint_folder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at D:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBot\\BarneyBot\\Data\\Characters\\Barney\\barney_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at D:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBot\\BarneyBot\\Data\\Characters\\Sheldon\\sheldon_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at microsoft/DialoGPT-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=os.path.join(base_folder, \"cache\"))\n",
    "tokenizer.pad_token = '#'\n",
    "\n",
    "model = TFAutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=checkpoint_folder)\n",
    "model_2 = TFAutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=checkpoint_folder_2)\n",
    "model_def = TFAutoModelForCausalLM.from_pretrained(model_name, cache_dir=os.path.join(base_folder, \"cache\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_questions = character_hg['test']['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_beams = 3\n",
    "top_k = 50\n",
    "top_p = 0.92\n",
    "\n",
    "def get_predictions_cached(sample_questions, model, filename, generation_method, override_predictions=False):\n",
    "    prediction_path = os.path.join(in_folder, filename)\n",
    "    if os.path.exists(prediction_path) and not override_predictions:\n",
    "        print(\"Loading predictions from stored file\")\n",
    "        with open(prediction_path, 'r') as file:\n",
    "            json_string = file.read()\n",
    "        predictions = json.loads(json_string)\n",
    "        print(\"Loaded predictions from stored file\")\n",
    "\n",
    "    else:\n",
    "        print(\"Creating predictions\")\n",
    "        predictions = list()\n",
    "        for x in tqdm(sample_questions):\n",
    "            tokenized_question = tokenizer.encode(x + tokenizer.eos_token, return_tensors='tf')\n",
    "            max_length = 128 + tokenized_question.shape[1]\n",
    "            if generation_method == \"Greedy\":\n",
    "                generated_answer = model.generate(tokenized_question,\n",
    "                                    pad_token_id=tokenizer.eos_token_id, max_length=max_length)[0].numpy().tolist()\n",
    "            elif generation_method == \"Beam Search\":\n",
    "                generated_answer = model.generate(tokenized_question,\n",
    "                                             pad_token_id=tokenizer.eos_token_id, max_length=max_length,\n",
    "                                             n_beams=n_beams)[0].numpy().tolist()\n",
    "            elif generation_method == \"Sampling\":\n",
    "                generated_answer = model.generate(tokenized_question,\n",
    "                                             pad_token_id=tokenizer.eos_token_id, max_length=max_length,\n",
    "                                             do_sample=True, top_k=top_k, top_p=top_p)[0].numpy().tolist()\n",
    "            predictions.append(generated_answer[len(tokenized_question[0]):])\n",
    "\n",
    "        # Save predictions as a JSON file\n",
    "        output_string = json.dumps(predictions)\n",
    "        with open(prediction_path, 'w') as file:\n",
    "            file.write(output_string)\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading predictions from stored file\n",
      "Loaded predictions from stored file\n",
      "Loading predictions from stored file\n",
      "Loaded predictions from stored file\n",
      "Loading predictions from stored file\n",
      "Loaded predictions from stored file\n"
     ]
    }
   ],
   "source": [
    "predictions_greedy = get_predictions_cached(sample_questions, model,\n",
    "                                            character_dict[character]['prediction_filename'] + '_greedy.json',\n",
    "                                            \"Greedy\")\n",
    "predictions_nbeams = get_predictions_cached(sample_questions, model,\n",
    "                                            character_dict[character]['prediction_filename'] + '_nbeams.json',\n",
    "                                            \"Beam Search\")\n",
    "predictions_sampling = get_predictions_cached(sample_questions, model,\n",
    "                                              character_dict[character]['prediction_filename'] + '_sampling.json',\n",
    "                                              \"Sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe_for_metrics(data_test, predictions_greedy, predictions_nbeams, predictions_sampling):\n",
    "    i = 0\n",
    "    df = {'ctx':[], 'ctx_tk':[]}\n",
    "    has_labels = 'response' in data_test.features\n",
    "    if has_labels:\n",
    "        df['lbl'] = []\n",
    "        df['lbl_tk'] = []\n",
    "    if predictions_greedy:\n",
    "        df['prd_greedy'] = []\n",
    "        df['prd_greedy_tk'] = []\n",
    "    if predictions_nbeams:\n",
    "        df['prd_nbeams'] = []\n",
    "        df['prd_nbeams_tk'] = [] \n",
    "    if predictions_sampling:\n",
    "        df['prd_sampling'] = []\n",
    "        df['prd_sampling_tk'] = []\n",
    "    for sample in tqdm(data_test):\n",
    "        # encode the context and label sentences, add the eos_token and return a tensor\n",
    "        ctx_tk = tokenizer.encode(sample['context'] + tokenizer.eos_token, return_tensors='tf').numpy().tolist()\n",
    "        ctx = sample['context']\n",
    "        df['ctx_tk'].append(ctx_tk)\n",
    "        df['ctx'].append(ctx)\n",
    "        if has_labels:\n",
    "            lbl_tk = tokenizer.encode(sample['response'] + tokenizer.eos_token, return_tensors='tf').numpy().tolist()\n",
    "            lbl = sample['response']\n",
    "            df['lbl'].append(lbl)\n",
    "            df['lbl_tk'].append(lbl_tk)\n",
    "        if predictions_greedy:\n",
    "            prd_greedy_tk = predictions_greedy[i]\n",
    "            prd_greedy = tokenizer.decode(prd_greedy_tk, skip_special_tokens=True)\n",
    "            df['prd_greedy'].append(prd_greedy)\n",
    "            df['prd_greedy_tk'].append(prd_greedy_tk)\n",
    "        if predictions_nbeams:\n",
    "            prd_nbeams_tk = predictions_nbeams[i]\n",
    "            prd_nbeams = tokenizer.decode(prd_nbeams_tk, skip_special_tokens=True)\n",
    "            df['prd_nbeams'].append(prd_nbeams)\n",
    "            df['prd_nbeams_tk'].append(prd_nbeams_tk)\n",
    "        if predictions_sampling:\n",
    "            prd_sampling_tk = predictions_sampling[i]\n",
    "            prd_sampling = tokenizer.decode(prd_sampling_tk, skip_special_tokens=True)\n",
    "            df['prd_sampling'].append(prd_sampling)\n",
    "            df['prd_sampling_tk'].append(prd_sampling_tk)\n",
    "        i += 1\n",
    "    return pd.DataFrame(data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 522/522 [00:00<00:00, 594.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ctx</th>\n",
       "      <th>ctx_tk</th>\n",
       "      <th>lbl</th>\n",
       "      <th>lbl_tk</th>\n",
       "      <th>prd_greedy</th>\n",
       "      <th>prd_greedy_tk</th>\n",
       "      <th>prd_nbeams</th>\n",
       "      <th>prd_nbeams_tk</th>\n",
       "      <th>prd_sampling</th>\n",
       "      <th>prd_sampling_tk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I know, it's two years of my life I'm never ge...</td>\n",
       "      <td>[[40, 760, 11, 340, 338, 734, 812, 286, 616, 1...</td>\n",
       "      <td>Daddy's home.</td>\n",
       "      <td>[[48280, 338, 1363, 13, 50256]]</td>\n",
       "      <td>Oh, God!</td>\n",
       "      <td>[5812, 11, 1793, 0, 50256]</td>\n",
       "      <td>Oh, God!</td>\n",
       "      <td>[5812, 11, 1793, 0, 50256]</td>\n",
       "      <td>Oh, of course.</td>\n",
       "      <td>[5812, 11, 286, 1781, 13, 50256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wh-Where'd you get a meatball...</td>\n",
       "      <td>[[1199, 12, 8496, 1549, 345, 651, 257, 6174, 1...</td>\n",
       "      <td>I don't have much time!</td>\n",
       "      <td>[[40, 836, 470, 423, 881, 640, 0, 50256]]</td>\n",
       "      <td>I don't know. I just saw a meatball sub.</td>\n",
       "      <td>[40, 836, 470, 760, 13, 314, 655, 2497, 257, 6...</td>\n",
       "      <td>I don't know. I just saw a meatball sub.</td>\n",
       "      <td>[40, 836, 470, 760, 13, 314, 655, 2497, 257, 6...</td>\n",
       "      <td>You don't remember?</td>\n",
       "      <td>[1639, 836, 470, 3505, 30, 50256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Okay, what is so urgent that you called me and...</td>\n",
       "      <td>[[16454, 11, 644, 318, 523, 18039, 326, 345, 1...</td>\n",
       "      <td>I could tell you knew something was up with me...</td>\n",
       "      <td>[[40, 714, 1560, 345, 2993, 1223, 373, 510, 35...</td>\n",
       "      <td>I'm sorry, I don't follow you.</td>\n",
       "      <td>[40, 1101, 7926, 11, 314, 836, 470, 1061, 345,...</td>\n",
       "      <td>I'm sorry, I don't follow you.</td>\n",
       "      <td>[40, 1101, 7926, 11, 314, 836, 470, 1061, 345,...</td>\n",
       "      <td>All right. It's time to start?</td>\n",
       "      <td>[3237, 826, 13, 632, 338, 640, 284, 923, 30, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How much?</td>\n",
       "      <td>[[2437, 881, 30, 50256]]</td>\n",
       "      <td>A little.</td>\n",
       "      <td>[[32, 1310, 13, 50256]]</td>\n",
       "      <td>I have not decided. I want to get married in a...</td>\n",
       "      <td>[40, 423, 407, 3066, 13, 314, 765, 284, 651, 6...</td>\n",
       "      <td>I have not decided. I want to get married in a...</td>\n",
       "      <td>[40, 423, 407, 3066, 13, 314, 765, 284, 651, 6...</td>\n",
       "      <td>He said he had like 3,000 in suits.</td>\n",
       "      <td>[1544, 531, 339, 550, 588, 513, 11, 830, 287, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You're being super nice. It's... freaking me o...</td>\n",
       "      <td>[[1639, 821, 852, 2208, 3621, 13, 632, 338, 98...</td>\n",
       "      <td>I'm being Barney, and I think tonight's going ...</td>\n",
       "      <td>[[40, 1101, 852, 41921, 11, 290, 314, 892, 997...</td>\n",
       "      <td>I'm not gross. I'm just... gross.</td>\n",
       "      <td>[40, 1101, 407, 10319, 13, 314, 1101, 655, 986...</td>\n",
       "      <td>I'm not gross. I'm just... gross.</td>\n",
       "      <td>[40, 1101, 407, 10319, 13, 314, 1101, 655, 986...</td>\n",
       "      <td>Buckminster Fuller?</td>\n",
       "      <td>[33, 1347, 18462, 31863, 30, 50256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>Okay, I want to lay down some ground rules for...</td>\n",
       "      <td>[[16454, 11, 314, 765, 284, 3830, 866, 617, 23...</td>\n",
       "      <td>Well, well, well. How rich. You make me promis...</td>\n",
       "      <td>[[5779, 11, 880, 11, 880, 13, 1374, 5527, 13, ...</td>\n",
       "      <td>Oh, I know. I just want to be as awesome as sh...</td>\n",
       "      <td>[5812, 11, 314, 760, 13, 314, 655, 765, 284, 3...</td>\n",
       "      <td>Oh, I know. I just want to be as awesome as sh...</td>\n",
       "      <td>[5812, 11, 314, 760, 13, 314, 655, 765, 284, 3...</td>\n",
       "      <td>No, I'll be right over.</td>\n",
       "      <td>[2949, 11, 314, 1183, 307, 826, 625, 13, 50256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>It looks to be a... sacred... spa.</td>\n",
       "      <td>[[1026, 3073, 284, 307, 257, 986, 13626, 986, ...</td>\n",
       "      <td>Owl. How do we go? We will do what? Jump?</td>\n",
       "      <td>[[46, 40989, 13, 1374, 466, 356, 467, 30, 775,...</td>\n",
       "      <td>I'm sorry, I don't follow you.</td>\n",
       "      <td>[40, 1101, 7926, 11, 314, 836, 470, 1061, 345,...</td>\n",
       "      <td>I'm sorry, I don't follow you.</td>\n",
       "      <td>[40, 1101, 7926, 11, 314, 836, 470, 1061, 345,...</td>\n",
       "      <td>It's called a spa.</td>\n",
       "      <td>[1026, 338, 1444, 257, 41900, 13, 50256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>That's putting it a bit strongly.</td>\n",
       "      <td>[[2504, 338, 5137, 340, 257, 1643, 7634, 13, 5...</td>\n",
       "      <td>A bit strongly. She's not my girlfriend.</td>\n",
       "      <td>[[32, 1643, 7634, 13, 1375, 338, 407, 616, 110...</td>\n",
       "      <td>I'm not going to put it in a little strong.</td>\n",
       "      <td>[40, 1101, 407, 1016, 284, 1234, 340, 287, 257...</td>\n",
       "      <td>I'm not going to put it in a little strong.</td>\n",
       "      <td>[40, 1101, 407, 1016, 284, 1234, 340, 287, 257...</td>\n",
       "      <td>And here comes the fun part.</td>\n",
       "      <td>[1870, 994, 2058, 262, 1257, 636, 13, 50256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>I do.</td>\n",
       "      <td>[[40, 466, 13, 50256]]</td>\n",
       "      <td>I'm gonna head out to a reggae concert. I'm a ...</td>\n",
       "      <td>[[40, 1101, 8066, 1182, 503, 284, 257, 842, 25...</td>\n",
       "      <td>You're a good man.</td>\n",
       "      <td>[1639, 821, 257, 922, 582, 13, 50256]</td>\n",
       "      <td>You're a good man.</td>\n",
       "      <td>[1639, 821, 257, 922, 582, 13, 50256]</td>\n",
       "      <td>Hey.</td>\n",
       "      <td>[10814, 13, 50256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>It's great. Drinking at work.</td>\n",
       "      <td>[[1026, 338, 1049, 13, 43963, 379, 670, 13, 50...</td>\n",
       "      <td>Basically, it is of Mad Men.</td>\n",
       "      <td>[[31524, 11, 340, 318, 286, 4627, 6065, 13, 50...</td>\n",
       "      <td>I'm not drinking at work.</td>\n",
       "      <td>[40, 1101, 407, 7722, 379, 670, 13, 50256]</td>\n",
       "      <td>I'm not drinking at work.</td>\n",
       "      <td>[40, 1101, 407, 7722, 379, 670, 13, 50256]</td>\n",
       "      <td>Barney, I didn't mean any personal attacks!</td>\n",
       "      <td>[10374, 1681, 11, 314, 1422, 470, 1612, 597, 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>522 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   ctx  \\\n",
       "0    I know, it's two years of my life I'm never ge...   \n",
       "1                     Wh-Where'd you get a meatball...   \n",
       "2    Okay, what is so urgent that you called me and...   \n",
       "3                                            How much?   \n",
       "4    You're being super nice. It's... freaking me o...   \n",
       "..                                                 ...   \n",
       "517  Okay, I want to lay down some ground rules for...   \n",
       "518                 It looks to be a... sacred... spa.   \n",
       "519                  That's putting it a bit strongly.   \n",
       "520                                              I do.   \n",
       "521                      It's great. Drinking at work.   \n",
       "\n",
       "                                                ctx_tk  \\\n",
       "0    [[40, 760, 11, 340, 338, 734, 812, 286, 616, 1...   \n",
       "1    [[1199, 12, 8496, 1549, 345, 651, 257, 6174, 1...   \n",
       "2    [[16454, 11, 644, 318, 523, 18039, 326, 345, 1...   \n",
       "3                             [[2437, 881, 30, 50256]]   \n",
       "4    [[1639, 821, 852, 2208, 3621, 13, 632, 338, 98...   \n",
       "..                                                 ...   \n",
       "517  [[16454, 11, 314, 765, 284, 3830, 866, 617, 23...   \n",
       "518  [[1026, 3073, 284, 307, 257, 986, 13626, 986, ...   \n",
       "519  [[2504, 338, 5137, 340, 257, 1643, 7634, 13, 5...   \n",
       "520                             [[40, 466, 13, 50256]]   \n",
       "521  [[1026, 338, 1049, 13, 43963, 379, 670, 13, 50...   \n",
       "\n",
       "                                                   lbl  \\\n",
       "0                                        Daddy's home.   \n",
       "1                              I don't have much time!   \n",
       "2    I could tell you knew something was up with me...   \n",
       "3                                            A little.   \n",
       "4    I'm being Barney, and I think tonight's going ...   \n",
       "..                                                 ...   \n",
       "517  Well, well, well. How rich. You make me promis...   \n",
       "518          Owl. How do we go? We will do what? Jump?   \n",
       "519           A bit strongly. She's not my girlfriend.   \n",
       "520  I'm gonna head out to a reggae concert. I'm a ...   \n",
       "521                       Basically, it is of Mad Men.   \n",
       "\n",
       "                                                lbl_tk  \\\n",
       "0                      [[48280, 338, 1363, 13, 50256]]   \n",
       "1            [[40, 836, 470, 423, 881, 640, 0, 50256]]   \n",
       "2    [[40, 714, 1560, 345, 2993, 1223, 373, 510, 35...   \n",
       "3                              [[32, 1310, 13, 50256]]   \n",
       "4    [[40, 1101, 852, 41921, 11, 290, 314, 892, 997...   \n",
       "..                                                 ...   \n",
       "517  [[5779, 11, 880, 11, 880, 13, 1374, 5527, 13, ...   \n",
       "518  [[46, 40989, 13, 1374, 466, 356, 467, 30, 775,...   \n",
       "519  [[32, 1643, 7634, 13, 1375, 338, 407, 616, 110...   \n",
       "520  [[40, 1101, 8066, 1182, 503, 284, 257, 842, 25...   \n",
       "521  [[31524, 11, 340, 318, 286, 4627, 6065, 13, 50...   \n",
       "\n",
       "                                            prd_greedy  \\\n",
       "0                                             Oh, God!   \n",
       "1             I don't know. I just saw a meatball sub.   \n",
       "2                       I'm sorry, I don't follow you.   \n",
       "3    I have not decided. I want to get married in a...   \n",
       "4                    I'm not gross. I'm just... gross.   \n",
       "..                                                 ...   \n",
       "517  Oh, I know. I just want to be as awesome as sh...   \n",
       "518                     I'm sorry, I don't follow you.   \n",
       "519        I'm not going to put it in a little strong.   \n",
       "520                                 You're a good man.   \n",
       "521                          I'm not drinking at work.   \n",
       "\n",
       "                                         prd_greedy_tk  \\\n",
       "0                           [5812, 11, 1793, 0, 50256]   \n",
       "1    [40, 836, 470, 760, 13, 314, 655, 2497, 257, 6...   \n",
       "2    [40, 1101, 7926, 11, 314, 836, 470, 1061, 345,...   \n",
       "3    [40, 423, 407, 3066, 13, 314, 765, 284, 651, 6...   \n",
       "4    [40, 1101, 407, 10319, 13, 314, 1101, 655, 986...   \n",
       "..                                                 ...   \n",
       "517  [5812, 11, 314, 760, 13, 314, 655, 765, 284, 3...   \n",
       "518  [40, 1101, 7926, 11, 314, 836, 470, 1061, 345,...   \n",
       "519  [40, 1101, 407, 1016, 284, 1234, 340, 287, 257...   \n",
       "520              [1639, 821, 257, 922, 582, 13, 50256]   \n",
       "521         [40, 1101, 407, 7722, 379, 670, 13, 50256]   \n",
       "\n",
       "                                            prd_nbeams  \\\n",
       "0                                             Oh, God!   \n",
       "1             I don't know. I just saw a meatball sub.   \n",
       "2                       I'm sorry, I don't follow you.   \n",
       "3    I have not decided. I want to get married in a...   \n",
       "4                    I'm not gross. I'm just... gross.   \n",
       "..                                                 ...   \n",
       "517  Oh, I know. I just want to be as awesome as sh...   \n",
       "518                     I'm sorry, I don't follow you.   \n",
       "519        I'm not going to put it in a little strong.   \n",
       "520                                 You're a good man.   \n",
       "521                          I'm not drinking at work.   \n",
       "\n",
       "                                         prd_nbeams_tk  \\\n",
       "0                           [5812, 11, 1793, 0, 50256]   \n",
       "1    [40, 836, 470, 760, 13, 314, 655, 2497, 257, 6...   \n",
       "2    [40, 1101, 7926, 11, 314, 836, 470, 1061, 345,...   \n",
       "3    [40, 423, 407, 3066, 13, 314, 765, 284, 651, 6...   \n",
       "4    [40, 1101, 407, 10319, 13, 314, 1101, 655, 986...   \n",
       "..                                                 ...   \n",
       "517  [5812, 11, 314, 760, 13, 314, 655, 765, 284, 3...   \n",
       "518  [40, 1101, 7926, 11, 314, 836, 470, 1061, 345,...   \n",
       "519  [40, 1101, 407, 1016, 284, 1234, 340, 287, 257...   \n",
       "520              [1639, 821, 257, 922, 582, 13, 50256]   \n",
       "521         [40, 1101, 407, 7722, 379, 670, 13, 50256]   \n",
       "\n",
       "                                    prd_sampling  \\\n",
       "0                                 Oh, of course.   \n",
       "1                            You don't remember?   \n",
       "2                 All right. It's time to start?   \n",
       "3            He said he had like 3,000 in suits.   \n",
       "4                            Buckminster Fuller?   \n",
       "..                                           ...   \n",
       "517                      No, I'll be right over.   \n",
       "518                           It's called a spa.   \n",
       "519                 And here comes the fun part.   \n",
       "520                                         Hey.   \n",
       "521  Barney, I didn't mean any personal attacks!   \n",
       "\n",
       "                                       prd_sampling_tk  \n",
       "0                     [5812, 11, 286, 1781, 13, 50256]  \n",
       "1                    [1639, 836, 470, 3505, 30, 50256]  \n",
       "2    [3237, 826, 13, 632, 338, 640, 284, 923, 30, 5...  \n",
       "3    [1544, 531, 339, 550, 588, 513, 11, 830, 287, ...  \n",
       "4                  [33, 1347, 18462, 31863, 30, 50256]  \n",
       "..                                                 ...  \n",
       "517    [2949, 11, 314, 1183, 307, 826, 625, 13, 50256]  \n",
       "518           [1026, 338, 1444, 257, 41900, 13, 50256]  \n",
       "519       [1870, 994, 2058, 262, 1257, 636, 13, 50256]  \n",
       "520                                 [10814, 13, 50256]  \n",
       "521  [10374, 1681, 11, 314, 1422, 470, 1612, 597, 2...  \n",
       "\n",
       "[522 rows x 10 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_char = get_dataframe_for_metrics(character_hg['test'], predictions_greedy, predictions_nbeams, predictions_sampling)\n",
    "df_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics For Character 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ccl_sim(ctx_lbl, ctx_cht, lbl_cht):\n",
    "    return ((1 - abs(ctx_lbl - ctx_cht))**2 + lbl_cht**2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Lib.BBMetrics import BBMetric\n",
    "\n",
    "def compute_sample_metrics(context_sentence, label_response, chatbot_response, verbose=True,\n",
    "                           label_chatbot_symmetry=False):\n",
    "    scores = {}\n",
    "    lbl_text = 'label' if not label_chatbot_symmetry else 'chatbota'\n",
    "    cht_text = 'chatbot' if not label_chatbot_symmetry else 'chatbotb'\n",
    "    if verbose:\n",
    "        # prints the sentences\n",
    "        print('* context:', context_sentence) \n",
    "        print('* ' + lbl_text  + ':  ', label_response)\n",
    "        print('* ' + cht_text  + ':', chatbot_response) \n",
    "    # 1) computes metrics for semantic similarity\n",
    "    metric = BBMetric.load_metric(\"semantic similarity\")\n",
    "    scores['semantic similarity'] = [metric.compute(sentences_a=context_sentence,\n",
    "                                                      sentences_b=label_response)['score']]\n",
    "    scores['semantic similarity'].append(metric.compute(sentences_a=context_sentence,\n",
    "                                                      sentences_b=chatbot_response)['score'])\n",
    "    scores['semantic similarity'].append(metric.compute(sentences_a=label_response,\n",
    "                                                      sentences_b=chatbot_response)['score'])\n",
    "    scores['semantic similarity'].append(ccl_sim(scores['semantic similarity'][0],\n",
    "                                                 scores['semantic similarity'][1],\n",
    "                                                 scores['semantic similarity'][2]))\n",
    "    if verbose:\n",
    "        print('=== SEMANTIC SIMILARITY ===')\n",
    "        print('context-'+lbl_text+' similarity:   ', scores['semantic similarity'][0])\n",
    "        print('context-'+cht_text+' similarity: ', scores['semantic similarity'][1])\n",
    "        print(cht_text+'-'+lbl_text+' similarity:   ', scores['semantic similarity'][2])\n",
    "        print('ccl-sim similarity:            ', scores['semantic similarity'][3])\n",
    "    # 2) computes metrics for bleu\n",
    "    metric = BBMetric.load_metric(\"bleu\")\n",
    "    scores['bleu'] = [metric.compute(predictions=label_response, references=context_sentence)['score']]\n",
    "    scores['bleu'].append(metric.compute(predictions=chatbot_response, references=context_sentence)['score'])\n",
    "    scores['bleu'].append(metric.compute(predictions=chatbot_response, references=label_response)['score'])\n",
    "    scores['bleu'].append(ccl_sim(scores['bleu'][0],\n",
    "                                  scores['bleu'][1],\n",
    "                                  scores['bleu'][2]))\n",
    "    if verbose:\n",
    "        print('===         BLEU         ===')\n",
    "        print('context-to-'+lbl_text+' bleu:      ', scores['bleu'][0])\n",
    "        print('context-to-'+cht_text+' bleu:    ', scores['bleu'][1])\n",
    "        print(lbl_text+'-to-'+cht_text+' bleu:      ', scores['bleu'][2])\n",
    "        print('ccl-sim bleu:            ', scores['bleu'][3])\n",
    "    # 3) computes metrics for rouge-L\n",
    "    metric = BBMetric.load_metric(\"rouge l\")\n",
    "    scores['rouge l'] = [metric.compute(predictions=label_response, references=context_sentence)['score']]\n",
    "    scores['rouge l'].append(metric.compute(predictions=chatbot_response, references=context_sentence)['score'])\n",
    "    scores['rouge l'].append(metric.compute(predictions=chatbot_response, references=label_response)['score'])\n",
    "    scores['rouge l'].append(ccl_sim(scores['rouge l'][0],\n",
    "                                     scores['rouge l'][1],\n",
    "                                     scores['rouge l'][2]))\n",
    "    if verbose:\n",
    "        print('===        ROUGE-L       ===')\n",
    "        print('context-to-'+lbl_text+' rouge:     ', scores['rouge l'][0])\n",
    "        print('context-to-'+cht_text+' rouge:   ', scores['rouge l'][1])\n",
    "        print(lbl_text+'-to-'+cht_text+' rouge:     ', scores['rouge l'][2])\n",
    "        print('ccl-sim rouge:            ', scores['rouge l'][3])\n",
    "    # 6) computes metrics for distinct\n",
    "    metric = BBMetric.load_metric(\"distinct\")\n",
    "    scores['distinct'] = [metric.compute(sentences=context_sentence)['score']]\n",
    "    scores['distinct'].append(metric.compute(sentences=label_response)['score'])\n",
    "    scores['distinct'].append(metric.compute(sentences=chatbot_response)['score'])\n",
    "    scores['distinct'].append(ccl_sim(scores['distinct'][0],\n",
    "                                      scores['distinct'][1],\n",
    "                                      scores['distinct'][2]))\n",
    "    if verbose:\n",
    "        print('===       DISTINCT      ===')\n",
    "        print('context distinct:          ', scores['distinct'][0])\n",
    "        print(lbl_text+' distinct:          ', scores['distinct'][1])\n",
    "        print(cht_text+' distinct:          ', scores['distinct'][2])\n",
    "        print('ccl-sim distinct:          ', scores['distinct'][3])\n",
    "    # 4) computes sas metric\n",
    "    metric = BBMetric.load_metric(\"semantic answer similarity\")\n",
    "    scores['semantic answer similarity'] = [metric.compute(predictions=context_sentence,\n",
    "                                                    references=label_response)['score']]\n",
    "    scores['semantic answer similarity'].append(metric.compute(predictions=context_sentence,\n",
    "                                                        references=chatbot_response)['score'])\n",
    "    scores['semantic answer similarity'].append(metric.compute(predictions=label_response,\n",
    "                                                        references=chatbot_response)['score'])\n",
    "    scores['semantic answer similarity'].append(ccl_sim(scores['semantic answer similarity'][0],\n",
    "                                                        scores['semantic answer similarity'][1],\n",
    "                                                        scores['semantic answer similarity'][2]))\n",
    "    if verbose:\n",
    "        print('===         SAS         ===')\n",
    "        print('context-'+lbl_text+' sas:          ', scores['semantic answer similarity'][0])\n",
    "        print('context-'+cht_text+' sas:        ', scores['semantic answer similarity'][1])\n",
    "        print(lbl_text+'-'+cht_text+' sas:          ', scores['semantic answer similarity'][2])\n",
    "        print('ccl-sim sas:               ', scores['semantic answer similarity'][3])\n",
    "    # 5) computes emotion metric\n",
    "    metric = BBMetric.load_metric(\"emotion\")\n",
    "    scores['emotion'] = [metric.compute(sentences=context_sentence)['score']]\n",
    "    scores['emotion'].append(metric.compute(sentences=label_response)['score'])\n",
    "    scores['emotion'].append(metric.compute(sentences=chatbot_response)['score'])\n",
    "    scores['emotion'].append(sp.stats.stats.pearsonr(scores['emotion'][1],\n",
    "                                                     scores['emotion'][2]))\n",
    "    if verbose:\n",
    "        print('===       EMOTION       ===')\n",
    "        print('context emotions:            \\n', list(zip(scores['emotion'][0]['label'], scores['emotion'][0]['score'])))\n",
    "        print(lbl_text+' emotions:              \\n', list(zip(scores['emotion'][1]['label'], scores['emotion'][1]['score'])))\n",
    "        print(cht_text+' emotions:            \\n', list(zip(scores['emotion'][2]['label'], scores['emotion'][2]['score'])))\n",
    "        print(lbl_text+'-'+cht_text+'emotion corr:  \\n', scores['emotion'][3])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_set_metrics(model, tokenizer, context_sentences, label_responses, chatbot_responses, verbose=True,\n",
    "                        classifier_n_sentences=50, include_qualitative=False, label_chatbot_symmetry=False):\n",
    "    scores = {}\n",
    "    \n",
    "    lbl_text = 'label' if not label_chatbot_symmetry else 'chatbota'\n",
    "    cht_text = 'chatbot' if not label_chatbot_symmetry else 'chatbotb'\n",
    "    \n",
    "    # 0) computes metrics for perplexity\n",
    "    metric = BBMetric.load_metric(\"semantic similarity\")\n",
    "    scores['semantic similarity'] = [metric.compute(sentences_a=context_sentences,\n",
    "                                                      sentences_b=label_responses)]\n",
    "    scores['semantic similarity'].append(metric.compute(sentences_a=context_sentences,\n",
    "                                                      sentences_b=chatbot_responses))\n",
    "    scores['semantic similarity'].append(metric.compute(sentences_a=label_responses,\n",
    "                                                      sentences_b=chatbot_responses))\n",
    "    scores['semantic similarity'].append(ccl_sim(scores['semantic similarity'][0]['score'],\n",
    "                                                 scores['semantic similarity'][1]['score'],\n",
    "                                                 scores['semantic similarity'][2]['score']))\n",
    "    if verbose:\n",
    "        print('=== SEMANTIC SIMILARITY ===')\n",
    "        print('context-'+lbl_text+' similarity:   ', scores['semantic similarity'][0])\n",
    "        print('context-'+cht_text+' similarity: ', scores['semantic similarity'][1])\n",
    "        print(cht_text+'-'+lbl_text+' similarity:   ', scores['semantic similarity'][2])\n",
    "        print('ccl-sim similarity:            ', scores['semantic similarity'][3])\n",
    "    # 1) computes metrics for perplexity\n",
    "    if not label_chatbot_symmetry:\n",
    "        metric = BBMetric.load_metric(\"perplexity\")\n",
    "        scores['perplexity'] = metric.compute(model=model, tokenizer=tokenizer, sentences=chatbot_responses)['score_concat']\n",
    "        if verbose:\n",
    "            print('===       PERPLEXITY     ===')\n",
    "            print('chatbot perplexity:         ', scores['perplexity'])\n",
    "    elif verbose:\n",
    "        print(\"Symmetric mode, skipping Perplexity.\")\n",
    "    # 2) computes metrics for bleu\n",
    "    metric = BBMetric.load_metric(\"bleu\")\n",
    "    scores['bleu'] = [metric.compute(predictions=label_responses, references=context_sentences)]\n",
    "    scores['bleu'].append(metric.compute(predictions=chatbot_responses, references=context_sentences))\n",
    "    scores['bleu'].append(metric.compute(predictions=chatbot_responses, references=label_responses))\n",
    "    scores['bleu'].append(ccl_sim(scores['bleu'][0]['score'],\n",
    "                                  scores['bleu'][1]['score'],\n",
    "                                  scores['bleu'][2]['score']))\n",
    "    if verbose:\n",
    "        print('===         BLEU         ===')\n",
    "        print('context-to-'+lbl_text+' bleu:      ', scores['bleu'][0])\n",
    "        print('context-to-'+cht_text+' bleu:    ', scores['bleu'][1])\n",
    "        print(lbl_text+'-to-'+cht_text+' bleu:      ', scores['bleu'][2])\n",
    "        print('ccl-sim bleu:            ', scores['bleu'][3])\n",
    "    # 3) computes metrics for rouge-L\n",
    "    metric = BBMetric.load_metric(\"rouge l\")\n",
    "    scores['rouge l'] = [metric.compute(predictions=label_responses, references=context_sentences)]\n",
    "    scores['rouge l'].append(metric.compute(predictions=chatbot_responses, references=context_sentences))\n",
    "    scores['rouge l'].append(metric.compute(predictions=chatbot_responses, references=label_responses))\n",
    "    scores['rouge l'].append(ccl_sim(scores['rouge l'][0]['score'],\n",
    "                                     scores['rouge l'][1]['score'],\n",
    "                                     scores['rouge l'][2]['score']))\n",
    "    if verbose:\n",
    "        print('===        ROUGE-L       ===')\n",
    "        print('context-to-'+lbl_text+' rouge:     ', scores['rouge l'][0])\n",
    "        print('context-to-'+cht_text+' rouge:   ', scores['rouge l'][1])\n",
    "        print(lbl_text+'-to-'+cht_text+' rouge:     ', scores['rouge l'][2])\n",
    "        print('ccl-sim rouge:            ', scores['rouge l'][3])\n",
    "    # 4) computes metrics for distinct\n",
    "    metric = BBMetric.load_metric(\"distinct\")\n",
    "    scores['distinct'] = [metric.compute(sentences=context_sentences)]\n",
    "    scores['distinct'].append(metric.compute(sentences=label_responses))\n",
    "    scores['distinct'].append(metric.compute(sentences=chatbot_responses))\n",
    "    scores['distinct'].append(ccl_sim(scores['distinct'][0]['score'],\n",
    "                                      scores['distinct'][1]['score'],\n",
    "                                      scores['distinct'][2]['score']))\n",
    "    if verbose:\n",
    "        print('===       DISTINCT      ===')\n",
    "        print('context distinct:          ', scores['distinct'][0])\n",
    "        print(lbl_text+' distinct:          ', scores['distinct'][1])\n",
    "        print(cht_text+' distinct:          ', scores['distinct'][2])\n",
    "        print('ccl-sim distinct:          ', scores['distinct'][3])\n",
    "    # 6) computes emotion metric\n",
    "    metric = BBMetric.load_metric(\"emotion\")\n",
    "    scores['emotion'] = [metric.compute(sentences=context_sentence)]\n",
    "    scores['emotion'].append(metric.compute(sentences=label_response))\n",
    "    scores['emotion'].append(metric.compute(sentences=chatbot_response))\n",
    "    scores['emotion'].append(sp.stats.stats.pearsonr(scores['emotion'][1]['score'],\n",
    "                                                     scores['emotion'][2]['score']))\n",
    "    if verbose:\n",
    "        print('===       EMOTION       ===')\n",
    "        print('context emotions:            \\n', list(zip(scores['emotion'][0]['label'], scores['emotion'][0]['score'])))\n",
    "        print(lbl_text+' emotions:              \\n', list(zip(scores['emotion'][1]['label'], scores['emotion'][1]['score'])))\n",
    "        print(cht_text+' emotions:            \\n', list(zip(scores['emotion'][2]['label'], scores['emotion'][2]['score'])))\n",
    "        print(lbl_text+'-'+cht_text+'emotion corr:  \\n', scores['emotion'][3])\n",
    "    # 8) computes sas metric\n",
    "    metric = BBMetric.load_metric(\"semantic answer similarity\")\n",
    "    scores['semantic answer similarity'] = [metric.compute(predictions=context_sentences,\n",
    "                                                    references=label_responses)]\n",
    "    scores['semantic answer similarity'].append(metric.compute(predictions=context_sentences,\n",
    "                                                        references=chatbot_responses))\n",
    "    scores['semantic answer similarity'].append(metric.compute(predictions=label_responses,\n",
    "                                                        references=chatbot_responses))\n",
    "    scores['semantic answer similarity'].append(ccl_sim(scores['semantic answer similarity'][0]['score'],\n",
    "                                                        scores['semantic answer similarity'][1]['score'],\n",
    "                                                        scores['semantic answer similarity'][2]['score']))\n",
    "    if verbose:\n",
    "        print('===         SAS         ===')\n",
    "        print('context-'+lbl_text+' sas:          ', scores['semantic answer similarity'][0])\n",
    "        print('context-'+cht_text+' sas:        ', scores['semantic answer similarity'][1])\n",
    "        print(lbl_text+'-'+cht_text+' sas:          ', scores['semantic answer similarity'][2])\n",
    "        print('ccl-sim sas:               ', scores['semantic answer similarity'][3])\n",
    "    # 9) computes metrics for semantic classifier\n",
    "    metric = BBMetric.load_metric(\"semantic classifier\")\n",
    "    start_time = time.time()\n",
    "    scores['semantic classifier'] = [metric.compute(character=character, character_dict=character_dict, \n",
    "                                                   base_folder=base_folder, sentences=label_responses,\n",
    "                                                   n_sentences=classifier_n_sentences)]\n",
    "    scores['semantic classifier'].append(metric.compute(character=character, character_dict=character_dict, \n",
    "                                                   base_folder=base_folder, sentences=chatbot_responses,\n",
    "                                                   n_sentences=classifier_n_sentences))\n",
    "    end_time = time.time()\n",
    "    if verbose:\n",
    "        print('=== SEMANTIC CLASSIFIER ===')\n",
    "        print('sem-classifier '+lbl_text+':                ', scores['semantic classifier'][0])\n",
    "        print('sem-classifier '+cht_text+':                  ', scores['semantic classifier'][1])\n",
    "        print('time elapsed computing semantic classifier:  {:.2f} s'.format(end_time - start_time))\n",
    "        \n",
    "    if include_qualitative:\n",
    "        # Do stuff with human metrics and print sentences\n",
    "        pass\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Sample 1 #####\n",
      "* context: I know, it's two years of my life I'm never getting back. A little part of me just wants to jump the bones of the next guy I see.\n",
      "* chatbot_a:   Daddy's home.\n",
      "* chatbot_b: Oh, God!\n",
      "=== SEMANTIC SIMILARITY ===\n",
      "context-chatbot_a similarity:    0.009417437016963959\n",
      "context-chatbot_b similarity:  0.017228586599230766\n",
      "chatbot_b-chatbot_a similarity:    0.008852758444845676\n",
      "ccl-sim similarity:             0.49225854311267286\n",
      "===         BLEU         ===\n",
      "context-to-chatbot_a bleu:       0.0\n",
      "context-to-chatbot_b bleu:     0.0\n",
      "chatbot_a-to-chatbot_b bleu:       0.0\n",
      "ccl-sim bleu:             0.5\n",
      "===        ROUGE-L       ===\n",
      "context-to-chatbot_a rouge:      0.0588235294117647\n",
      "context-to-chatbot_b rouge:    0.0\n",
      "chatbot_a-to-chatbot_b rouge:      0.0\n",
      "ccl-sim rouge:             0.4429065743944637\n",
      "===       DISTINCT      ===\n",
      "context distinct:           0.20930232558139536\n",
      "chatbot_a distinct:           0.0\n",
      "chatbot_b distinct:           0.0\n",
      "ccl-sim distinct:           0.3126014061654948\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-70a7eb557de2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mchatbot_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_char\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'prd_greedy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mlabel_response\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mdf_char\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lbl'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mcompute_sample_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext_sentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_response\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchatbot_response\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-4e1c7525179b>\u001b[0m in \u001b[0;36mcompute_sample_metrics\u001b[1;34m(context_sentence, label_response, chatbot_response, verbose, label_chatbot_symmetry)\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ccl-sim distinct:          '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'distinct'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;31m# 4) computes sas metric\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m     \u001b[0mmetric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBBMetric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"semantic answer similarity\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     scores['semantic answer similarity'] = [metric.compute(predictions=context_sentence,\n\u001b[0;32m     75\u001b[0m                                                     references=label_response)['score']]\n",
      "\u001b[1;32mD:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBot\\BarneyBot\\Lib\\BBMetrics.py\u001b[0m in \u001b[0;36mload_metric\u001b[1;34m(name, **kwargs)\u001b[0m\n\u001b[0;32m    432\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"semantic answer similarity\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m             metric = BBMetric(name,\n\u001b[1;32m--> 434\u001b[1;33m                               SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"))\n\u001b[0m\u001b[0;32m    435\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"distinct\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m             metric = BBMetric(name,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model_name_or_path, modules, device, cache_folder, use_auth_token)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'modules.json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m#Load as SentenceTransformer model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m                 \u001b[0mmodules\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_sbert_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m   \u001b[1;31m#Load with AutoModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m                 \u001b[0mmodules\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_auto_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py\u001b[0m in \u001b[0;36m_load_sbert_model\u001b[1;34m(self, model_path)\u001b[0m\n\u001b[0;32m    829\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule_config\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodules_config\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m             \u001b[0mmodule_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimport_from_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule_config\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'type'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 831\u001b[1;33m             \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule_config\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'path'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    832\u001b[0m             \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodule_config\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    833\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(input_path)\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msbert_config_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfIn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfIn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model_name_or_path, max_seq_length, model_args, cache_dir, tokenizer_args, do_lower_case, tokenizer_name_or_path)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer_name_or_path\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtokenizer_name_or_path\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmodel_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mtokenizer_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py\u001b[0m in \u001b[0;36m_load_model\u001b[1;34m(self, model_name_or_path, config, cache_dir)\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_t5_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_load_t5_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    445\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 447\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    448\u001b[0m         raise ValueError(\n\u001b[0;32m    449\u001b[0m             \u001b[1;34mf\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   1491\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1492\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mno_init_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_enable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_fast_init\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1493\u001b[1;33m                 \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1495\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfrom_pt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, config, add_pooling_layer)\u001b[0m\n\u001b[0;32m    716\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    717\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 718\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRobertaEmbeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    719\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRobertaEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    720\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad_token_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_position_embeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_vocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, device, dtype)\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_embeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfactory_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_weight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnum_embeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mreset_parameters\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         \u001b[0minit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fill_padding_idx_with_zero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py\u001b[0m in \u001b[0;36mnormal_\u001b[1;34m(tensor, mean, std)\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_no_grad_normal_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrunc_normal_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m2.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2.\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py\u001b[0m in \u001b[0;36m_no_grad_normal_\u001b[1;34m(tensor, mean, std)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_no_grad_normal_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    print(\"##### Sample \" + str(i+1) + \" #####\")\n",
    "    context_sentence = df_char['ctx'][i]\n",
    "    chatbot_response = df_char['prd_greedy'][i]\n",
    "    label_response   = df_char['lbl'][i]\n",
    "    compute_sample_metrics(context_sentence, label_response, chatbot_response)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_size = 50\n",
    "i = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"##### Set (Size \" + str(set_size) + \") #####\")\n",
    "context_sentences = list(df_char['ctx'][i:i+set_size])\n",
    "chatbot_responses = list(df_char['prd_greedy'][i:i+set_size])\n",
    "label_responses   = list(df_char['lbl'][i:i+set_size])\n",
    "scores = compute_set_metrics(model, tokenizer,\n",
    "                    context_sentences, label_responses, chatbot_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Full Test Set #####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (7597 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SEMANTIC SIMILARITY ===\n",
      "semantic similarity:         [0.09542313, 0.22581792, 0.09963352, 0.14029152443011603]\n",
      "===       PERPLEXITY     ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 119/119 [00:12<00:00,  9.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity:                  398.63258558262714\n",
      "===         BLEU         ===\n",
      "bleu:                        0.0\n",
      "===        ROUGE-L       ===\n",
      "rouge:                       0.07784712634918524\n",
      "===        DISTINCT      ===\n",
      "distinct:                    0.10609917782254247\n",
      "===        EMOTION       ===\n",
      "chatbot emotions:            \n",
      " [('sadness', 0.2896285032952671), ('joy', 0.271495469938105), ('love', 0.023309589239469394), ('anger', 0.34172616484604523), ('fear', 0.06777737714981699), ('surprise', 0.0060628916286927835)]\n",
      "label emotions:              \n",
      " [('sadness', 0.08338458265166866), ('joy', 0.38902365875618744), ('love', 0.024806545307496035), ('anger', 0.40497622549917656), ('fear', 0.08848321566827828), ('surprise', 0.00932577022948622)]\n",
      "label-chatbot emotion corr:  \n",
      " (0.794296557894927, 0.05911880107460219)\n",
      "===         SAS         ===\n",
      "context-label sas:           0.29280096\n",
      "context-chatbot sas:         0.36482108\n",
      "label-chatbot sas:           0.25272134\n",
      "> Merged Metrics\n",
      "sas-mean:                    0.30344779292742413\n",
      "=== SEMANTIC CLASSIFIER ===\n",
      "semantic classifier chatbot:                 {'score': 0.44610128, 'std': 0.46138492}\n",
      "semantic classifier label:                   {'score': 0.9950678, 'std': 0.054852862}\n",
      "time elapsed computing semantic classifier:  27.69 s\n"
     ]
    }
   ],
   "source": [
    "print(\"##### Full Test Set #####\")\n",
    "context_sentences = list(df_char['ctx'])\n",
    "chatbot_responses = list(df_char['prd_greedy'])\n",
    "label_responses   = list(df_char['lbl'])\n",
    "scores = compute_set_metrics(model, tokenizer,\n",
    "                    context_sentences, \n",
    "                    label_responses, \n",
    "                    chatbot_responses,\n",
    "                    classifier_n_sentences=75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Between Character 1 & Character 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_small(sample_questions, model, generation_method):\n",
    "    print(\"Creating predictions\")\n",
    "    predictions = list()\n",
    "    for x in tqdm(sample_questions):\n",
    "        tokenized_question = tokenizer.encode(x + tokenizer.eos_token, return_tensors='tf')\n",
    "        max_length = 128 + tokenized_question.shape[1]\n",
    "        if generation_method == \"Greedy\":\n",
    "            generated_answer = model.generate(tokenized_question,\n",
    "                                pad_token_id=tokenizer.eos_token_id, max_length=max_length)[0].numpy().tolist()\n",
    "        elif generation_method == \"Beam Search\":\n",
    "            generated_answer = model.generate(tokenized_question,\n",
    "                                         pad_token_id=tokenizer.eos_token_id, max_length=max_length,\n",
    "                                         n_beams=n_beams)[0].numpy().tolist()\n",
    "        elif generation_method == \"Sampling\":\n",
    "            generated_answer = model.generate(tokenized_question,\n",
    "                                         pad_token_id=tokenizer.eos_token_id, max_length=max_length,\n",
    "                                         do_sample=True, top_k=top_k, top_p=top_p)[0].numpy().tolist()\n",
    "        predictions.append(generated_answer[len(tokenized_question[0]):])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_common = load_dataset('csv',\n",
    "                         data_files=os.path.join(base_folder, 'Data', 'common_dataset.csv'), \n",
    "                         cache_dir=os.path.join(base_folder, \"cache\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_1_sampling = get_predictions_small(df_common['train']['context'], model, \"Sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_2_sampling = get_predictions_small(df_common['train']['context'], model_2, \"Sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_common_char_1 = get_dataframe_for_metrics(df_common['train'], None, None, predictions_1_sampling)\n",
    "df_common_char_2 = get_dataframe_for_metrics(df_common['train'], None, None, predictions_2_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"##### \" + character + \"  Vs. \" + character_2 + \" #####\")\n",
    "context_sentences   = list(df_common_char_1['ctx'])\n",
    "chatbot_responses   = list(df_common_char_1['prd_sampling'])\n",
    "chatbot_2_responses = list(df_common_char_2['prd_sampling'])\n",
    "compute_set_metrics(None, None,\n",
    "                    context_sentences, chatbot_responses, chatbot_2_responses,\n",
    "                    include_qualitative=True, label_chatbot_symmetry=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Between Different Sampling Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"##### Greedy vs. N-Beams #####\")\n",
    "context_sentences = list(df_char['ctx'])\n",
    "greedy_responses  = list(df_char['prd_greedy'])\n",
    "nbeams_responses  = list(df_char['prd_nbeams'])\n",
    "compute_set_metrics(None, None,\n",
    "                    context_sentences,\n",
    "                    greedy_responses,\n",
    "                    nbeams_responses,\n",
    "                    classifier_n_sentences=75,\n",
    "                    label_chatbot_symmetry=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"##### Greedy vs. Sampling #####\")\n",
    "context_sentences   = list(df_char['ctx'])\n",
    "greedy_responses    = list(df_char['prd_greedy'])\n",
    "sampling_responses  = list(df_char['prd_sampling'])\n",
    "compute_set_metrics(None, None,\n",
    "                    context_sentences,\n",
    "                    greedy_responses,\n",
    "                    sampling_responses,\n",
    "                    classifier_n_sentences=75,\n",
    "                    label_chatbot_symmetry=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"##### N-Beams vs. Sampling #####\")\n",
    "context_sentences   = list(df_char['ctx'])\n",
    "nbeams_responses    = list(df_char['prd_nbeams'])\n",
    "sampling_responses  = list(df_char['prd_sampling'])\n",
    "compute_set_metrics(None, None,\n",
    "                    context_sentences,\n",
    "                    nbeams_responses,\n",
    "                    sampling_responses,\n",
    "                    classifier_n_sentences=75,\n",
    "                    label_chatbot_symmetry=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Between Non-Finetuned And Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_def_sampling = get_predictions_cached(sample_questions, model_def,\n",
    "                                                  os.path.join(in_folder_def, 'from_' + character + '_df_' + '_sampling.json'),\n",
    "                                                  \"Sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char_def = get_dataframe_for_metrics(character_hg['test'], None, None, predictions_def_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    print(\"##### Sample \" + str(i+1) + \" #####\")\n",
    "    context_sentence   = df_char['ctx'][i]\n",
    "    character_response = df_char['prd_sampling'][i]\n",
    "    default_response   = df_char_def['prd_sampling'][i]\n",
    "    compute_sample_metrics(context_sentence, default_response, character_response, label_chatbot_symmetry=True)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_size = 50\n",
    "i = 30\n",
    "print(\"##### Set (Size \" + str(set_size) + \") #####\")\n",
    "context_sentences   = list(df_char['ctx'][i:i+set_size])\n",
    "character_responses = list(df_char['prd_sampling'][i:i+set_size])\n",
    "default_responses   = list(df_char_def['prd_sampling'][i:i+set_size])\n",
    "compute_set_metrics(None, None,\n",
    "                    context_sentences, default_responses, character_responses, label_chatbot_symmetry=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"##### Full Test Set (Size \" + str(set_size) + \") #####\")\n",
    "context_sentences   = list(df_char['ctx'])\n",
    "character_responses = list(df_char['prd_sampling'])\n",
    "default_responses   = list(df_char_def['prd_sampling'])\n",
    "compute_set_metrics(None, None,\n",
    "                    context_sentences, \n",
    "                    default_responses, \n",
    "                    character_responses,\n",
    "                    classifier_n_sentences=75,\n",
    "                    label_chatbot_symmetry=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

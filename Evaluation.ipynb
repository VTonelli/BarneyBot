{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data.data_dicts import character_dict, source_dict, random_state\n",
    "\n",
    "model_name = 'microsoft/DialoGPT-small'\n",
    "character = 'Vader' # 'Barney' | 'Sheldon' | 'Harry' | 'Fry' | 'Vader' | 'Joey' | 'Phoebe' | 'Bender' | Default'\n",
    "character_2 = 'Harry'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount google drive\n",
    "import os\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive',force_remount=True)\n",
    "    base_folder = '/content/drive/My Drive/unibo/NLP_project/BarneyBot'\n",
    "    os.system(\"pip install datasets\")\n",
    "    os.system(\"pip install transformers\")\n",
    "    os.system(\"pip install rouge_score\")\n",
    "    os.system(\"pip install -U sentence-transformers\")\n",
    "else:\n",
    "    base_folder = os.getcwd()\n",
    "    \n",
    "in_folder = os.path.join(base_folder, 'Data', 'Characters', character)\n",
    "if not os.path.exists(in_folder):\n",
    "    os.makedirs(in_folder)\n",
    "out_folder = os.path.join(base_folder, 'Data', 'Characters', character)\n",
    "if not os.path.exists(out_folder):\n",
    "    os.makedirs(out_folder)\n",
    "    \n",
    "in_folder_2 = os.path.join(base_folder, 'Data', 'Characters', character_2)\n",
    "if not os.path.exists(in_folder_2):\n",
    "    os.makedirs(in_folder_2)\n",
    "out_folder_2 = os.path.join(base_folder, 'Data', 'Characters', character_2)\n",
    "if not os.path.exists(out_folder_2):\n",
    "    os.makedirs(out_folder_2)\n",
    "    \n",
    "in_folder_def = os.path.join(base_folder, 'Data', 'Characters', 'Default')\n",
    "if not os.path.exists(in_folder_def):\n",
    "    os.makedirs(in_folder_def)\n",
    "out_folder_def = os.path.join(base_folder, 'Data', 'Characters', 'Default')\n",
    "if not os.path.exists(out_folder_def):\n",
    "    os.makedirs(out_folder_def)\n",
    "    \n",
    "metrics_folder = os.path.join(base_folder, 'Metrics')\n",
    "if not os.path.exists(metrics_folder):\n",
    "    os.makedirs(metrics_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_json(filepath, filename, data):\n",
    "    if not os.path.exists(filepath):\n",
    "        os.makedirs(filepath, exist_ok=True)\n",
    "    with open(os.path.join(filepath, filename + \".json\"), 'w') as f:\n",
    "        f.write(json.dumps(data, indent=4))\n",
    "\n",
    "def load_from_json(filepath, filename):\n",
    "    if not os.path.exists(os.path.join(filepath, filename + '.json')):\n",
    "        return dict()\n",
    "    with open(os.path.join(filepath, filename + '.json'), 'r') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "def load_df(character):\n",
    "    dataset_path = os.path.join(base_folder, \"Data\", \"Characters\", character, character+'.csv')\n",
    "    \n",
    "    character_hg = load_dataset('csv', \n",
    "                                data_files=dataset_path, \n",
    "                                cache_dir=os.path.join(base_folder, \"cache\"))\n",
    "    \n",
    "    # 85% train / 10% test / 5% validation\n",
    "    train_test_hg = character_hg['train'].train_test_split(test_size=0.15, seed=random_state)\n",
    "    test_val = train_test_hg['test'].train_test_split(test_size=0.33, seed=random_state)\n",
    "    \n",
    "    \n",
    "    character_hg = DatasetDict({\n",
    "        'train': train_test_hg['train'],\n",
    "        'test': test_val['train'],\n",
    "        'val': test_val['test']\n",
    "    })\n",
    "    \n",
    "    return character_hg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_conv(row, tokenizer):\n",
    "    MAX_LENGTH = 512\n",
    "    row = list(reversed(list(row.values())))\n",
    "    model_inputs = tokenizer(row)\n",
    "    tokenizer_pad_token_id = tokenizer.encode('#')[0]\n",
    "    for i in range(len(model_inputs['input_ids'])):\n",
    "        model_inputs['input_ids'][i].append(tokenizer.eos_token_id)\n",
    "        model_inputs['attention_mask'][i].append(1)\n",
    "    model_inputs['input_ids'] = [item for sublist in model_inputs['input_ids'] for item in sublist]\n",
    "    model_inputs['attention_mask'] = [item for sublist in model_inputs['attention_mask'] for item in sublist]\n",
    "    if MAX_LENGTH > len(model_inputs['input_ids']):\n",
    "        model_inputs['input_ids'] += [tokenizer_pad_token_id] * (MAX_LENGTH - len(model_inputs['input_ids']))\n",
    "        model_inputs['attention_mask'] += [0] * (MAX_LENGTH - len(model_inputs['attention_mask']))\n",
    "    elif MAX_LENGTH < len(model_inputs['input_ids']):\n",
    "        model_inputs['input_ids'] = model_inputs['input_ids'][:MAX_LENGTH-1]\n",
    "        model_inputs['input_ids'][-1] = tokenizer.eos_token_id\n",
    "        model_inputs['attention_mask'] = model_inputs['attention_mask'][:MAX_LENGTH-1]\n",
    "        model_inputs['attention_mask'][-1] = 1\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    tokenizer.pad_token = '#'\n",
    "    model_inputs = construct_conv(examples, tokenizer)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-51ade516a19edf18\n",
      "Reusing dataset csv (C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\cache\\csv\\default-51ade516a19edf18\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7994b62390c140cda7a2f25c5faee1b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\cache\\csv\\default-51ade516a19edf18\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-5c36a7a962370e03.arrow and C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\cache\\csv\\default-51ade516a19edf18\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-f8ff1848c7476ff8.arrow\n",
      "Loading cached split indices for dataset at C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\cache\\csv\\default-51ade516a19edf18\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-ca5a49546d137fcb.arrow and C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\cache\\csv\\default-51ade516a19edf18\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-56a7e670cd7f99a9.arrow\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"HF_DATASETS_CACHE\"] = os.path.join(base_folder, \"cache\")\n",
    "character_hg = load_df(character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_folder = os.path.join(out_folder, character_dict[character]['checkpoint_folder'])\n",
    "checkpoint_folder_2 = os.path.join(out_folder_2, character_dict[character_2]['checkpoint_folder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\Data\\Characters\\Vader\\vader_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\Data\\Characters\\Harry\\harry_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at microsoft/DialoGPT-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=os.path.join(base_folder, \"cache\"))\n",
    "tokenizer.pad_token = '#'\n",
    "\n",
    "model = TFAutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=checkpoint_folder)\n",
    "model_2 = TFAutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=checkpoint_folder_2)\n",
    "model_def = TFAutoModelForCausalLM.from_pretrained(model_name, cache_dir=os.path.join(base_folder, \"cache\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_questions = character_hg['test']['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_beams = 3\n",
    "top_k = 50\n",
    "top_p = 0.92\n",
    "\n",
    "def get_predictions_cached(sample_questions, model, filename, generation_method, override_predictions=False):\n",
    "    prediction_path = os.path.join(in_folder, filename)\n",
    "    if os.path.exists(prediction_path) and not override_predictions:\n",
    "        print(\"Loading predictions from stored file\")\n",
    "        with open(prediction_path, 'r') as file:\n",
    "            json_string = file.read()\n",
    "        predictions = json.loads(json_string)\n",
    "        print(\"Loaded predictions from stored file\")\n",
    "\n",
    "    else:\n",
    "        print(\"Creating predictions\")\n",
    "        predictions = list()\n",
    "        for x in tqdm(sample_questions):\n",
    "            tokenized_question = tokenizer.encode(x + tokenizer.eos_token, return_tensors='tf')\n",
    "            max_length = 128 + tokenized_question.shape[1]\n",
    "            if generation_method == \"Greedy\":\n",
    "                generated_answer = model.generate(tokenized_question,\n",
    "                                    pad_token_id=tokenizer.eos_token_id, max_length=max_length)[0].numpy().tolist()\n",
    "            elif generation_method == \"Beam Search\":\n",
    "                generated_answer = model.generate(tokenized_question,\n",
    "                                             pad_token_id=tokenizer.eos_token_id, max_length=max_length,\n",
    "                                             n_beams=n_beams)[0].numpy().tolist()\n",
    "            elif generation_method == \"Sampling\":\n",
    "                b = True\n",
    "                c = 0\n",
    "                while b:\n",
    "                    generated_answer = model.generate(tokenized_question,\n",
    "                                                 pad_token_id=tokenizer.eos_token_id, max_length=max_length,\n",
    "                                                 do_sample=True, top_k=top_k, top_p=top_p)[0].numpy().tolist()\n",
    "                    c += 1\n",
    "                    if len(generated_answer[len(tokenized_question[0]):])>1:\n",
    "                        b = False       \n",
    "                    if c>100: \n",
    "                        generated_answer[len(tokenized_question[0]):] = tokenizer.encode('hi') + [tokenizer.eos_token_id]\n",
    "                        break\n",
    "            \n",
    "            predictions.append(generated_answer[len(tokenized_question[0]):])\n",
    "\n",
    "        # Save predictions as a JSON file\n",
    "        output_string = json.dumps(predictions)\n",
    "        with open(prediction_path, 'w') as file:\n",
    "            file.write(output_string)\n",
    "        \n",
    "        assert all([len(p)>1 for p in predictions])\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading predictions from stored file\n",
      "Loaded predictions from stored file\n",
      "Loading predictions from stored file\n",
      "Loaded predictions from stored file\n",
      "Loading predictions from stored file\n",
      "Loaded predictions from stored file\n"
     ]
    }
   ],
   "source": [
    "predictions_greedy = get_predictions_cached(sample_questions, model,\n",
    "                                            character_dict[character]['prediction_filename'] + '_greedy.json',\n",
    "                                            \"Greedy\")\n",
    "predictions_nbeams = get_predictions_cached(sample_questions, model,\n",
    "                                            character_dict[character]['prediction_filename'] + '_nbeams.json',\n",
    "                                            \"Beam Search\")\n",
    "predictions_sampling = get_predictions_cached(sample_questions, model,\n",
    "                                              character_dict[character]['prediction_filename'] + '_sampling.json',\n",
    "                                              \"Sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe_for_metrics(data_test, predictions_greedy, predictions_nbeams, predictions_sampling):\n",
    "    i = 0\n",
    "    df = {'ctx':[], 'ctx_tk':[]}\n",
    "    has_labels = 'response' in data_test.features\n",
    "    if has_labels:\n",
    "        df['lbl'] = []\n",
    "        df['lbl_tk'] = []\n",
    "    if predictions_greedy:\n",
    "        df['prd_greedy'] = []\n",
    "        df['prd_greedy_tk'] = []\n",
    "    if predictions_nbeams:\n",
    "        df['prd_nbeams'] = []\n",
    "        df['prd_nbeams_tk'] = [] \n",
    "    if predictions_sampling:\n",
    "        df['prd_sampling'] = []\n",
    "        df['prd_sampling_tk'] = []\n",
    "    for sample in tqdm(data_test):\n",
    "        # encode the context and label sentences, add the eos_token and return a tensor\n",
    "        ctx_tk = tokenizer.encode(sample['context'] + tokenizer.eos_token, return_tensors='tf').numpy().tolist()\n",
    "        ctx = sample['context']\n",
    "        df['ctx_tk'].append(ctx_tk)\n",
    "        df['ctx'].append(ctx)\n",
    "        if has_labels:\n",
    "            lbl_tk = tokenizer.encode(sample['response'] + tokenizer.eos_token, return_tensors='tf').numpy().tolist()\n",
    "            lbl = sample['response']\n",
    "            df['lbl'].append(lbl)\n",
    "            df['lbl_tk'].append(lbl_tk)\n",
    "        if predictions_greedy:\n",
    "            prd_greedy_tk = predictions_greedy[i]\n",
    "            prd_greedy = tokenizer.decode(prd_greedy_tk, skip_special_tokens=True)\n",
    "            df['prd_greedy'].append(prd_greedy)\n",
    "            df['prd_greedy_tk'].append(prd_greedy_tk)\n",
    "        if predictions_nbeams:\n",
    "            prd_nbeams_tk = predictions_nbeams[i]\n",
    "            prd_nbeams = tokenizer.decode(prd_nbeams_tk, skip_special_tokens=True)\n",
    "            df['prd_nbeams'].append(prd_nbeams)\n",
    "            df['prd_nbeams_tk'].append(prd_nbeams_tk)\n",
    "        if predictions_sampling:\n",
    "            prd_sampling_tk = predictions_sampling[i]\n",
    "            prd_sampling = tokenizer.decode(prd_sampling_tk, skip_special_tokens=True)\n",
    "            df['prd_sampling'].append(prd_sampling)\n",
    "            df['prd_sampling_tk'].append(prd_sampling_tk)\n",
    "        i += 1\n",
    "    return pd.DataFrame(data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 1229.69it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ctx</th>\n",
       "      <th>ctx_tk</th>\n",
       "      <th>lbl</th>\n",
       "      <th>lbl_tk</th>\n",
       "      <th>prd_greedy</th>\n",
       "      <th>prd_greedy_tk</th>\n",
       "      <th>prd_nbeams</th>\n",
       "      <th>prd_nbeams_tk</th>\n",
       "      <th>prd_sampling</th>\n",
       "      <th>prd_sampling_tk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I will not fight you.</td>\n",
       "      <td>[[40, 481, 407, 1907, 345, 13, 50256]]</td>\n",
       "      <td>Give yourself to the dark side. It is the only...</td>\n",
       "      <td>[[23318, 3511, 284, 262, 3223, 1735, 13, 632, ...</td>\n",
       "      <td>I will not fight you, father.</td>\n",
       "      <td>[40, 481, 407, 1907, 345, 11, 2988, 13, 50256]</td>\n",
       "      <td>I will not fight you, father.</td>\n",
       "      <td>[40, 481, 407, 1907, 345, 11, 2988, 13, 50256]</td>\n",
       "      <td>You cannot make me destroy you.</td>\n",
       "      <td>[1639, 2314, 787, 502, 4117, 345, 13, 50256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unlock one-five-seven and nine.Release charges.</td>\n",
       "      <td>[[3118, 5354, 530, 12, 13261, 12, 26548, 290, ...</td>\n",
       "      <td>Did you find any droids?</td>\n",
       "      <td>[[11633, 345, 1064, 597, 3102, 2340, 30, 50256]]</td>\n",
       "      <td>Three Imperial TIE ships, one armed, and ready...</td>\n",
       "      <td>[12510, 11773, 309, 10008, 7937, 11, 530, 6936...</td>\n",
       "      <td>Three Imperial TIE ships, one armed, and ready...</td>\n",
       "      <td>[12510, 11773, 309, 10008, 7937, 11, 530, 6936...</td>\n",
       "      <td>Vader calmly adjusts his control sticks as the...</td>\n",
       "      <td>[53, 5067, 30180, 46094, 465, 1630, 16461, 355...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lord Vader, what about Leia and theWookiee?</td>\n",
       "      <td>[[22438, 27403, 11, 644, 546, 41212, 290, 262,...</td>\n",
       "      <td>They must never again leave thiscity.</td>\n",
       "      <td>[[2990, 1276, 1239, 757, 2666, 428, 19205, 13,...</td>\n",
       "      <td>Yes, my lord.</td>\n",
       "      <td>[5297, 11, 616, 15876, 13, 50256]</td>\n",
       "      <td>Yes, my lord.</td>\n",
       "      <td>[5297, 11, 616, 15876, 13, 50256]</td>\n",
       "      <td>The Senate will discuss this issue further.</td>\n",
       "      <td>[464, 3845, 481, 2112, 428, 2071, 2252, 13, 50...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No!</td>\n",
       "      <td>[[2949, 0, 50256]]</td>\n",
       "      <td>All to easy. Perhaps you are not as strong as ...</td>\n",
       "      <td>[[3237, 284, 2562, 13, 8673, 345, 389, 407, 35...</td>\n",
       "      <td>No!</td>\n",
       "      <td>[2949, 0, 50256]</td>\n",
       "      <td>No!</td>\n",
       "      <td>[2949, 0, 50256]</td>\n",
       "      <td>Yes, Admiral.</td>\n",
       "      <td>[5297, 11, 24646, 13, 50256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Give yourself to the dark side. It is the only...</td>\n",
       "      <td>[[23318, 3511, 284, 262, 3223, 1735, 13, 632, ...</td>\n",
       "      <td>Sister! So...you have a twinsister. Your feeli...</td>\n",
       "      <td>[[50, 1694, 0, 1406, 986, 5832, 423, 257, 2034...</td>\n",
       "      <td>I am your father.</td>\n",
       "      <td>[40, 716, 534, 2988, 13, 50256]</td>\n",
       "      <td>I am your father.</td>\n",
       "      <td>[40, 716, 534, 2988, 13, 50256]</td>\n",
       "      <td>I can't stop.</td>\n",
       "      <td>[40, 460, 470, 2245, 13, 50256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Open the blast doors! Open theblast doors!</td>\n",
       "      <td>[[11505, 262, 11975, 8215, 0, 4946, 262, 39806...</td>\n",
       "      <td>I've been waiting for you, Obi-Wan. We meet ag...</td>\n",
       "      <td>[[40, 1053, 587, 4953, 329, 345, 11, 46662, 12...</td>\n",
       "      <td>The Force is with you, Skywalker.</td>\n",
       "      <td>[464, 5221, 318, 351, 345, 11, 29715, 13, 50256]</td>\n",
       "      <td>The Force is with you, Skywalker.</td>\n",
       "      <td>[464, 5221, 318, 351, 345, 11, 29715, 13, 50256]</td>\n",
       "      <td>You'll be</td>\n",
       "      <td>[1639, 1183, 307, 50256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Strange, that I have not. I wonder if your fee...</td>\n",
       "      <td>[[38114, 11, 326, 314, 423, 407, 13, 314, 4240...</td>\n",
       "      <td>They are clear, my Master.</td>\n",
       "      <td>[[2990, 389, 1598, 11, 616, 5599, 13, 50256]]</td>\n",
       "      <td>I have felt it.</td>\n",
       "      <td>[40, 423, 2936, 340, 13, 50256]</td>\n",
       "      <td>I have felt it.</td>\n",
       "      <td>[40, 423, 2936, 340, 13, 50256]</td>\n",
       "      <td>As you wish.</td>\n",
       "      <td>[1722, 345, 4601, 13, 50256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Vader's targeting computer swings around into ...</td>\n",
       "      <td>[[53, 5067, 338, 10822, 3644, 26728, 1088, 656...</td>\n",
       "      <td>I have you now.</td>\n",
       "      <td>[[40, 423, 345, 783, 13, 50256]]</td>\n",
       "      <td>Vader adjusts his control stick as the stars w...</td>\n",
       "      <td>[53, 5067, 46094, 465, 1630, 4859, 355, 262, 5...</td>\n",
       "      <td>Vader adjusts his control stick as the stars w...</td>\n",
       "      <td>[53, 5067, 46094, 465, 1630, 4859, 355, 262, 5...</td>\n",
       "      <td>Vader adjusts his control stick and adjusts hi...</td>\n",
       "      <td>[53, 5067, 46094, 465, 1630, 4859, 290, 46094,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Emperor's coming here?</td>\n",
       "      <td>[[464, 10851, 338, 2406, 994, 30, 50256]]</td>\n",
       "      <td>That is correct, Commander. And heis most disp...</td>\n",
       "      <td>[[2504, 318, 3376, 11, 13353, 13, 843, 339, 27...</td>\n",
       "      <td>Yes, my lord.</td>\n",
       "      <td>[5297, 11, 616, 15876, 13, 50256]</td>\n",
       "      <td>Yes, my lord.</td>\n",
       "      <td>[5297, 11, 616, 15876, 13, 50256]</td>\n",
       "      <td>You know what they say about a man with two li...</td>\n",
       "      <td>[1639, 760, 644, 484, 910, 546, 257, 582, 351,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The princess! Put all sections onalert!</td>\n",
       "      <td>[[464, 21752, 0, 5930, 477, 9004, 319, 44598, ...</td>\n",
       "      <td>Obi-Wan is here. The Force is withhim.</td>\n",
       "      <td>[[5944, 72, 12, 45681, 318, 994, 13, 383, 5221...</td>\n",
       "      <td>The Millennium Falcon lifts gracefully into th...</td>\n",
       "      <td>[464, 26139, 17621, 27103, 11542, 2759, 656, 2...</td>\n",
       "      <td>The Millennium Falcon lifts gracefully into th...</td>\n",
       "      <td>[464, 26139, 17621, 27103, 11542, 2759, 656, 2...</td>\n",
       "      <td>Skywalker, what is it?</td>\n",
       "      <td>[22308, 20783, 11, 644, 318, 340, 30, 50256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Good.Use your aggressivefeelings, boy!Let the ...</td>\n",
       "      <td>[[10248, 13, 11041, 534, 8361, 36410, 654, 11,...</td>\n",
       "      <td>Obi-Wan has taught you well.</td>\n",
       "      <td>[[5944, 72, 12, 45681, 468, 7817, 345, 880, 13...</td>\n",
       "      <td>I sense a disturbance in theForce.</td>\n",
       "      <td>[40, 2565, 257, 30497, 287, 262, 10292, 13, 50...</td>\n",
       "      <td>I sense a disturbance in theForce.</td>\n",
       "      <td>[40, 2565, 257, 30497, 287, 262, 10292, 13, 50...</td>\n",
       "      <td>Heheheh, I wish you well. I will not fight you...</td>\n",
       "      <td>[1544, 258, 258, 71, 11, 314, 4601, 345, 880, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Surely he must be dead by now.</td>\n",
       "      <td>[[19457, 306, 339, 1276, 307, 2636, 416, 783, ...</td>\n",
       "      <td>Don't underestimate the power ofthe Force.</td>\n",
       "      <td>[[3987, 470, 34994, 262, 1176, 286, 1169, 5221...</td>\n",
       "      <td>He's a real trooper.</td>\n",
       "      <td>[1544, 338, 257, 1103, 41967, 13, 50256]</td>\n",
       "      <td>He's a real trooper.</td>\n",
       "      <td>[1544, 338, 257, 1103, 41967, 13, 50256]</td>\n",
       "      <td>It must be true, my lord.</td>\n",
       "      <td>[1026, 1276, 307, 2081, 11, 616, 15876, 13, 50...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Shall I hold them?</td>\n",
       "      <td>[[2484, 439, 314, 1745, 606, 30, 50256]]</td>\n",
       "      <td>No. Leave them to me. I will deal</td>\n",
       "      <td>[[2949, 13, 17446, 606, 284, 502, 13, 314, 481...</td>\n",
       "      <td>Yes, my lord.</td>\n",
       "      <td>[5297, 11, 616, 15876, 13, 50256]</td>\n",
       "      <td>Yes, my lord.</td>\n",
       "      <td>[5297, 11, 616, 15876, 13, 50256]</td>\n",
       "      <td>Oh, Lord Vader.</td>\n",
       "      <td>[5812, 11, 4453, 27403, 13, 50256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What if he doesn't survive? He's worth a lot t...</td>\n",
       "      <td>[[2061, 611, 339, 1595, 470, 7866, 30, 679, 33...</td>\n",
       "      <td>The Empire will compensate you if he dies. Put...</td>\n",
       "      <td>[[464, 8065, 481, 21392, 345, 611, 339, 10564,...</td>\n",
       "      <td>If he survives the next round, he may be of so...</td>\n",
       "      <td>[1532, 339, 36417, 262, 1306, 2835, 11, 339, 7...</td>\n",
       "      <td>If he survives the next round, he may be of so...</td>\n",
       "      <td>[1532, 339, 36417, 262, 1306, 2835, 11, 339, 7...</td>\n",
       "      <td>What if he does survive?</td>\n",
       "      <td>[2061, 611, 339, 857, 7866, 30, 50256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Then you must go to the Sanctuary</td>\n",
       "      <td>[[6423, 345, 1276, 467, 284, 262, 27036, 50256]]</td>\n",
       "      <td>He will come to me?</td>\n",
       "      <td>[[1544, 481, 1282, 284, 502, 30, 50256]]</td>\n",
       "      <td>I have foreseen this.</td>\n",
       "      <td>[40, 423, 1674, 15898, 428, 13, 50256]</td>\n",
       "      <td>I have foreseen this.</td>\n",
       "      <td>[40, 423, 1674, 15898, 428, 13, 50256]</td>\n",
       "      <td>Yes, my lord.</td>\n",
       "      <td>[5297, 11, 616, 15876, 13, 50256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Are you sure?</td>\n",
       "      <td>[[8491, 345, 1654, 30, 50256]]</td>\n",
       "      <td>I have felt him, my Master.</td>\n",
       "      <td>[[40, 423, 2936, 683, 11, 616, 5599, 13, 50256]]</td>\n",
       "      <td>Yes, my lord.</td>\n",
       "      <td>[5297, 11, 616, 15876, 13, 50256]</td>\n",
       "      <td>Yes, my lord.</td>\n",
       "      <td>[5297, 11, 616, 15876, 13, 50256]</td>\n",
       "      <td>What is thy bidding, my Master?</td>\n",
       "      <td>[2061, 318, 11906, 23829, 11, 616, 5599, 30, 5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  ctx  \\\n",
       "0                               I will not fight you.   \n",
       "1     Unlock one-five-seven and nine.Release charges.   \n",
       "2         Lord Vader, what about Leia and theWookiee?   \n",
       "3                                                 No!   \n",
       "4   Give yourself to the dark side. It is the only...   \n",
       "5          Open the blast doors! Open theblast doors!   \n",
       "6   Strange, that I have not. I wonder if your fee...   \n",
       "7   Vader's targeting computer swings around into ...   \n",
       "8                          The Emperor's coming here?   \n",
       "9             The princess! Put all sections onalert!   \n",
       "10  Good.Use your aggressivefeelings, boy!Let the ...   \n",
       "11                     Surely he must be dead by now.   \n",
       "12                                 Shall I hold them?   \n",
       "13  What if he doesn't survive? He's worth a lot t...   \n",
       "14                  Then you must go to the Sanctuary   \n",
       "15                                      Are you sure?   \n",
       "\n",
       "                                               ctx_tk  \\\n",
       "0              [[40, 481, 407, 1907, 345, 13, 50256]]   \n",
       "1   [[3118, 5354, 530, 12, 13261, 12, 26548, 290, ...   \n",
       "2   [[22438, 27403, 11, 644, 546, 41212, 290, 262,...   \n",
       "3                                  [[2949, 0, 50256]]   \n",
       "4   [[23318, 3511, 284, 262, 3223, 1735, 13, 632, ...   \n",
       "5   [[11505, 262, 11975, 8215, 0, 4946, 262, 39806...   \n",
       "6   [[38114, 11, 326, 314, 423, 407, 13, 314, 4240...   \n",
       "7   [[53, 5067, 338, 10822, 3644, 26728, 1088, 656...   \n",
       "8           [[464, 10851, 338, 2406, 994, 30, 50256]]   \n",
       "9   [[464, 21752, 0, 5930, 477, 9004, 319, 44598, ...   \n",
       "10  [[10248, 13, 11041, 534, 8361, 36410, 654, 11,...   \n",
       "11  [[19457, 306, 339, 1276, 307, 2636, 416, 783, ...   \n",
       "12           [[2484, 439, 314, 1745, 606, 30, 50256]]   \n",
       "13  [[2061, 611, 339, 1595, 470, 7866, 30, 679, 33...   \n",
       "14   [[6423, 345, 1276, 467, 284, 262, 27036, 50256]]   \n",
       "15                     [[8491, 345, 1654, 30, 50256]]   \n",
       "\n",
       "                                                  lbl  \\\n",
       "0   Give yourself to the dark side. It is the only...   \n",
       "1                            Did you find any droids?   \n",
       "2               They must never again leave thiscity.   \n",
       "3   All to easy. Perhaps you are not as strong as ...   \n",
       "4   Sister! So...you have a twinsister. Your feeli...   \n",
       "5   I've been waiting for you, Obi-Wan. We meet ag...   \n",
       "6                          They are clear, my Master.   \n",
       "7                                     I have you now.   \n",
       "8   That is correct, Commander. And heis most disp...   \n",
       "9              Obi-Wan is here. The Force is withhim.   \n",
       "10                       Obi-Wan has taught you well.   \n",
       "11         Don't underestimate the power ofthe Force.   \n",
       "12                  No. Leave them to me. I will deal   \n",
       "13  The Empire will compensate you if he dies. Put...   \n",
       "14                                He will come to me?   \n",
       "15                        I have felt him, my Master.   \n",
       "\n",
       "                                               lbl_tk  \\\n",
       "0   [[23318, 3511, 284, 262, 3223, 1735, 13, 632, ...   \n",
       "1    [[11633, 345, 1064, 597, 3102, 2340, 30, 50256]]   \n",
       "2   [[2990, 1276, 1239, 757, 2666, 428, 19205, 13,...   \n",
       "3   [[3237, 284, 2562, 13, 8673, 345, 389, 407, 35...   \n",
       "4   [[50, 1694, 0, 1406, 986, 5832, 423, 257, 2034...   \n",
       "5   [[40, 1053, 587, 4953, 329, 345, 11, 46662, 12...   \n",
       "6       [[2990, 389, 1598, 11, 616, 5599, 13, 50256]]   \n",
       "7                    [[40, 423, 345, 783, 13, 50256]]   \n",
       "8   [[2504, 318, 3376, 11, 13353, 13, 843, 339, 27...   \n",
       "9   [[5944, 72, 12, 45681, 318, 994, 13, 383, 5221...   \n",
       "10  [[5944, 72, 12, 45681, 468, 7817, 345, 880, 13...   \n",
       "11  [[3987, 470, 34994, 262, 1176, 286, 1169, 5221...   \n",
       "12  [[2949, 13, 17446, 606, 284, 502, 13, 314, 481...   \n",
       "13  [[464, 8065, 481, 21392, 345, 611, 339, 10564,...   \n",
       "14           [[1544, 481, 1282, 284, 502, 30, 50256]]   \n",
       "15   [[40, 423, 2936, 683, 11, 616, 5599, 13, 50256]]   \n",
       "\n",
       "                                           prd_greedy  \\\n",
       "0                       I will not fight you, father.   \n",
       "1   Three Imperial TIE ships, one armed, and ready...   \n",
       "2                                       Yes, my lord.   \n",
       "3                                                 No!   \n",
       "4                                   I am your father.   \n",
       "5                   The Force is with you, Skywalker.   \n",
       "6                                     I have felt it.   \n",
       "7   Vader adjusts his control stick as the stars w...   \n",
       "8                                       Yes, my lord.   \n",
       "9   The Millennium Falcon lifts gracefully into th...   \n",
       "10                 I sense a disturbance in theForce.   \n",
       "11                               He's a real trooper.   \n",
       "12                                      Yes, my lord.   \n",
       "13  If he survives the next round, he may be of so...   \n",
       "14                              I have foreseen this.   \n",
       "15                                      Yes, my lord.   \n",
       "\n",
       "                                        prd_greedy_tk  \\\n",
       "0      [40, 481, 407, 1907, 345, 11, 2988, 13, 50256]   \n",
       "1   [12510, 11773, 309, 10008, 7937, 11, 530, 6936...   \n",
       "2                   [5297, 11, 616, 15876, 13, 50256]   \n",
       "3                                    [2949, 0, 50256]   \n",
       "4                     [40, 716, 534, 2988, 13, 50256]   \n",
       "5    [464, 5221, 318, 351, 345, 11, 29715, 13, 50256]   \n",
       "6                     [40, 423, 2936, 340, 13, 50256]   \n",
       "7   [53, 5067, 46094, 465, 1630, 4859, 355, 262, 5...   \n",
       "8                   [5297, 11, 616, 15876, 13, 50256]   \n",
       "9   [464, 26139, 17621, 27103, 11542, 2759, 656, 2...   \n",
       "10  [40, 2565, 257, 30497, 287, 262, 10292, 13, 50...   \n",
       "11           [1544, 338, 257, 1103, 41967, 13, 50256]   \n",
       "12                  [5297, 11, 616, 15876, 13, 50256]   \n",
       "13  [1532, 339, 36417, 262, 1306, 2835, 11, 339, 7...   \n",
       "14             [40, 423, 1674, 15898, 428, 13, 50256]   \n",
       "15                  [5297, 11, 616, 15876, 13, 50256]   \n",
       "\n",
       "                                           prd_nbeams  \\\n",
       "0                       I will not fight you, father.   \n",
       "1   Three Imperial TIE ships, one armed, and ready...   \n",
       "2                                       Yes, my lord.   \n",
       "3                                                 No!   \n",
       "4                                   I am your father.   \n",
       "5                   The Force is with you, Skywalker.   \n",
       "6                                     I have felt it.   \n",
       "7   Vader adjusts his control stick as the stars w...   \n",
       "8                                       Yes, my lord.   \n",
       "9   The Millennium Falcon lifts gracefully into th...   \n",
       "10                 I sense a disturbance in theForce.   \n",
       "11                               He's a real trooper.   \n",
       "12                                      Yes, my lord.   \n",
       "13  If he survives the next round, he may be of so...   \n",
       "14                              I have foreseen this.   \n",
       "15                                      Yes, my lord.   \n",
       "\n",
       "                                        prd_nbeams_tk  \\\n",
       "0      [40, 481, 407, 1907, 345, 11, 2988, 13, 50256]   \n",
       "1   [12510, 11773, 309, 10008, 7937, 11, 530, 6936...   \n",
       "2                   [5297, 11, 616, 15876, 13, 50256]   \n",
       "3                                    [2949, 0, 50256]   \n",
       "4                     [40, 716, 534, 2988, 13, 50256]   \n",
       "5    [464, 5221, 318, 351, 345, 11, 29715, 13, 50256]   \n",
       "6                     [40, 423, 2936, 340, 13, 50256]   \n",
       "7   [53, 5067, 46094, 465, 1630, 4859, 355, 262, 5...   \n",
       "8                   [5297, 11, 616, 15876, 13, 50256]   \n",
       "9   [464, 26139, 17621, 27103, 11542, 2759, 656, 2...   \n",
       "10  [40, 2565, 257, 30497, 287, 262, 10292, 13, 50...   \n",
       "11           [1544, 338, 257, 1103, 41967, 13, 50256]   \n",
       "12                  [5297, 11, 616, 15876, 13, 50256]   \n",
       "13  [1532, 339, 36417, 262, 1306, 2835, 11, 339, 7...   \n",
       "14             [40, 423, 1674, 15898, 428, 13, 50256]   \n",
       "15                  [5297, 11, 616, 15876, 13, 50256]   \n",
       "\n",
       "                                         prd_sampling  \\\n",
       "0                     You cannot make me destroy you.   \n",
       "1   Vader calmly adjusts his control sticks as the...   \n",
       "2         The Senate will discuss this issue further.   \n",
       "3                                       Yes, Admiral.   \n",
       "4                                       I can't stop.   \n",
       "5                                           You'll be   \n",
       "6                                        As you wish.   \n",
       "7   Vader adjusts his control stick and adjusts hi...   \n",
       "8   You know what they say about a man with two li...   \n",
       "9                              Skywalker, what is it?   \n",
       "10  Heheheh, I wish you well. I will not fight you...   \n",
       "11                          It must be true, my lord.   \n",
       "12                                    Oh, Lord Vader.   \n",
       "13                           What if he does survive?   \n",
       "14                                      Yes, my lord.   \n",
       "15                    What is thy bidding, my Master?   \n",
       "\n",
       "                                      prd_sampling_tk  \n",
       "0        [1639, 2314, 787, 502, 4117, 345, 13, 50256]  \n",
       "1   [53, 5067, 30180, 46094, 465, 1630, 16461, 355...  \n",
       "2   [464, 3845, 481, 2112, 428, 2071, 2252, 13, 50...  \n",
       "3                        [5297, 11, 24646, 13, 50256]  \n",
       "4                     [40, 460, 470, 2245, 13, 50256]  \n",
       "5                            [1639, 1183, 307, 50256]  \n",
       "6                        [1722, 345, 4601, 13, 50256]  \n",
       "7   [53, 5067, 46094, 465, 1630, 4859, 290, 46094,...  \n",
       "8   [1639, 760, 644, 484, 910, 546, 257, 582, 351,...  \n",
       "9        [22308, 20783, 11, 644, 318, 340, 30, 50256]  \n",
       "10  [1544, 258, 258, 71, 11, 314, 4601, 345, 880, ...  \n",
       "11  [1026, 1276, 307, 2081, 11, 616, 15876, 13, 50...  \n",
       "12                 [5812, 11, 4453, 27403, 13, 50256]  \n",
       "13             [2061, 611, 339, 857, 7866, 30, 50256]  \n",
       "14                  [5297, 11, 616, 15876, 13, 50256]  \n",
       "15  [2061, 318, 11906, 23829, 11, 616, 5599, 30, 5...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_char = get_dataframe_for_metrics(character_hg['test'], predictions_greedy, predictions_nbeams, predictions_sampling)\n",
    "df_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics For Character 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ccl_sim(ctx_lbl, ctx_cht, lbl_cht):\n",
    "    return ((1 - abs(ctx_lbl - ctx_cht))**2 + lbl_cht**2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Lib.BBMetrics import BBMetric\n",
    "\n",
    "def compute_sample_metrics(context_sentence, label_response, chatbot_response, verbose=True,\n",
    "                           label_chatbot_symmetry=False):\n",
    "    scores = {}\n",
    "    lbl_text = 'label' if not label_chatbot_symmetry else 'chatbota'\n",
    "    cht_text = 'chatbot' if not label_chatbot_symmetry else 'chatbotb'\n",
    "    \n",
    "    scores['metadata'] = {}\n",
    "    scores['metadata']['ordering'] = ['context-'+lbl_text,\n",
    "                                      'context-'+cht_text,\n",
    "                                      cht_text+'-'+lbl_text,\n",
    "                                      'ccl']\n",
    "    \n",
    "    if verbose:\n",
    "        # prints the sample\n",
    "        print('* context:', context_sentence) \n",
    "        print('* ' + lbl_text  + ':  ', label_response)\n",
    "        print('* ' + cht_text  + ':', chatbot_response) \n",
    "    # 1) computes metrics for semantic similarity\n",
    "    metric = BBMetric.load_metric(\"semantic similarity\")\n",
    "    scores['semantic similarity'] = [metric.compute(sentences_a=context_sentence,\n",
    "                                                      sentences_b=label_response)['score']]\n",
    "    scores['semantic similarity'].append(metric.compute(sentences_a=context_sentence,\n",
    "                                                      sentences_b=chatbot_response)['score'])\n",
    "    scores['semantic similarity'].append(metric.compute(sentences_a=label_response,\n",
    "                                                      sentences_b=chatbot_response)['score'])\n",
    "    scores['semantic similarity'].append(ccl_sim(scores['semantic similarity'][0],\n",
    "                                                 scores['semantic similarity'][1],\n",
    "                                                 scores['semantic similarity'][2]))\n",
    "    if verbose:\n",
    "        print('=== SEMANTIC SIMILARITY ===')\n",
    "        print('context-'+lbl_text+' similarity:   ', scores['semantic similarity'][0])\n",
    "        print('context-'+cht_text+' similarity: ', scores['semantic similarity'][1])\n",
    "        print(cht_text+'-'+lbl_text+' similarity:   ', scores['semantic similarity'][2])\n",
    "        print('ccl-sim similarity:            ', scores['semantic similarity'][3])\n",
    "    # 2) computes metrics for bleu\n",
    "    metric = BBMetric.load_metric(\"bleu\")\n",
    "    scores['bleu'] = [metric.compute(predictions=label_response, references=context_sentence)['score']]\n",
    "    scores['bleu'].append(metric.compute(predictions=chatbot_response, references=context_sentence)['score'])\n",
    "    scores['bleu'].append(metric.compute(predictions=chatbot_response, references=label_response)['score'])\n",
    "    scores['bleu'].append(ccl_sim(scores['bleu'][0],\n",
    "                                  scores['bleu'][1],\n",
    "                                  scores['bleu'][2]))\n",
    "    if verbose:\n",
    "        print('===         BLEU         ===')\n",
    "        print('context-to-'+lbl_text+' bleu:      ', scores['bleu'][0])\n",
    "        print('context-to-'+cht_text+' bleu:    ', scores['bleu'][1])\n",
    "        print(lbl_text+'-to-'+cht_text+' bleu:      ', scores['bleu'][2])\n",
    "        print('ccl-sim bleu:            ', scores['bleu'][3])\n",
    "    # 3) computes metrics for rouge-L\n",
    "    metric = BBMetric.load_metric(\"rouge l\")\n",
    "    scores['rouge l'] = [metric.compute(predictions=label_response, references=context_sentence)['score']]\n",
    "    scores['rouge l'].append(metric.compute(predictions=chatbot_response, references=context_sentence)['score'])\n",
    "    scores['rouge l'].append(metric.compute(predictions=chatbot_response, references=label_response)['score'])\n",
    "    scores['rouge l'].append(ccl_sim(scores['rouge l'][0],\n",
    "                                     scores['rouge l'][1],\n",
    "                                     scores['rouge l'][2]))\n",
    "    if verbose:\n",
    "        print('===        ROUGE-L       ===')\n",
    "        print('context-to-'+lbl_text+' rouge:     ', scores['rouge l'][0])\n",
    "        print('context-to-'+cht_text+' rouge:   ', scores['rouge l'][1])\n",
    "        print(lbl_text+'-to-'+cht_text+' rouge:     ', scores['rouge l'][2])\n",
    "        print('ccl-sim rouge:            ', scores['rouge l'][3])\n",
    "    # 6) computes metrics for distinct\n",
    "    metric = BBMetric.load_metric(\"distinct\")\n",
    "    scores['distinct'] = [metric.compute(sentences=context_sentence)['score']]\n",
    "    scores['distinct'].append(metric.compute(sentences=label_response)['score'])\n",
    "    scores['distinct'].append(metric.compute(sentences=chatbot_response)['score'])\n",
    "    if verbose:\n",
    "        print('===       DISTINCT      ===')\n",
    "        print('context distinct:          ', scores['distinct'][0])\n",
    "        print(lbl_text+' distinct:          ', scores['distinct'][1])\n",
    "        print(cht_text+' distinct:          ', scores['distinct'][2])\n",
    "    # 4) computes sas metric\n",
    "    metric = BBMetric.load_metric(\"semantic answer similarity\")\n",
    "    scores['semantic answer similarity'] = [metric.compute(predictions=context_sentence,\n",
    "                                                    references=label_response)['score']]\n",
    "    scores['semantic answer similarity'].append(metric.compute(predictions=context_sentence,\n",
    "                                                        references=chatbot_response)['score'])\n",
    "    scores['semantic answer similarity'].append(metric.compute(predictions=label_response,\n",
    "                                                        references=chatbot_response)['score'])\n",
    "    scores['semantic answer similarity'].append(ccl_sim(scores['semantic answer similarity'][0],\n",
    "                                                        scores['semantic answer similarity'][1],\n",
    "                                                        scores['semantic answer similarity'][2]))\n",
    "    if verbose:\n",
    "        print('===         SAS         ===')\n",
    "        print('context-'+lbl_text+' sas:          ', scores['semantic answer similarity'][0])\n",
    "        print('context-'+cht_text+' sas:        ', scores['semantic answer similarity'][1])\n",
    "        print(lbl_text+'-'+cht_text+' sas:          ', scores['semantic answer similarity'][2])\n",
    "        print('ccl-sim sas:               ', scores['semantic answer similarity'][3])\n",
    "    # 5) computes emotion metric\n",
    "    metric = BBMetric.load_metric(\"emotion\")\n",
    "    scores['emotion'] = [metric.compute(sentences=context_sentence)]\n",
    "    scores['emotion'].append(metric.compute(sentences=label_response))\n",
    "    scores['emotion'].append(metric.compute(sentences=chatbot_response))\n",
    "    scores['emotion'].append(sp.stats.stats.pearsonr(scores['emotion'][1]['score'],\n",
    "                                                     scores['emotion'][2]['score'])[0])\n",
    "    if verbose:\n",
    "        print('===       EMOTION       ===')\n",
    "        print('context emotions:            \\n', list(zip(scores['emotion'][0]['label'], scores['emotion'][0]['score'])))\n",
    "        print(lbl_text+' emotions:              \\n', list(zip(scores['emotion'][1]['label'], scores['emotion'][1]['score'])))\n",
    "        print(cht_text+' emotions:            \\n', list(zip(scores['emotion'][2]['label'], scores['emotion'][2]['score'])))\n",
    "        print(lbl_text+'-'+cht_text+'emotion corr:  \\n', scores['emotion'][3])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_set_metrics(model, tokenizer, context_sentences, label_responses, chatbot_responses, character, verbose=True,\n",
    "                        classifier_n_sentences=50, include_sentences=False, label_chatbot_symmetry=False):\n",
    "    scores = {}\n",
    "    \n",
    "    lbl_text = 'label' if not label_chatbot_symmetry else 'chatbota'\n",
    "    cht_text = 'chatbot' if not label_chatbot_symmetry else 'chatbotb'\n",
    "    \n",
    "    scores['metadata'] = {}\n",
    "    scores['metadata']['ordering'] = ['context-'+lbl_text,\n",
    "                                      'context-'+cht_text,\n",
    "                                      cht_text+'-'+lbl_text,\n",
    "                                      'ccl']\n",
    "    \n",
    "    # 0) computes metrics for perplexity\n",
    "    metric = BBMetric.load_metric(\"semantic similarity\")\n",
    "    scores['semantic similarity'] = [metric.compute(sentences_a=context_sentences,\n",
    "                                            sentences_b=label_responses)]\n",
    "    scores['semantic similarity'].append(metric.compute(sentences_a=context_sentences,\n",
    "                                            sentences_b=chatbot_responses)),\n",
    "    scores['semantic similarity'].append(metric.compute(sentences_a=label_responses,\n",
    "                                              sentences_b=chatbot_responses))\n",
    "    scores['semantic similarity'].append(ccl_sim(scores['semantic similarity'][0]['score'],\n",
    "                                                 scores['semantic similarity'][1]['score'],\n",
    "                                                 scores['semantic similarity'][2]['score']))\n",
    "    if verbose:\n",
    "        print('=== SEMANTIC SIMILARITY ===')\n",
    "        print('context-'+lbl_text+' similarity:   ', scores['semantic similarity'][0])\n",
    "        print('context-'+cht_text+' similarity: ', scores['semantic similarity'][1])\n",
    "        print(cht_text+'-'+lbl_text+' similarity:   ', scores['semantic similarity'][2])\n",
    "        print('ccl-sim similarity:            ', scores['semantic similarity'][3])\n",
    "    # 1) computes metrics for perplexity\n",
    "    if not label_chatbot_symmetry:\n",
    "        metric = BBMetric.load_metric(\"perplexity\")\n",
    "        scores['perplexity'] = metric.compute(model=model, tokenizer=tokenizer, sentences=chatbot_responses)['score_concat']\n",
    "        if verbose:\n",
    "            print('===       PERPLEXITY     ===')\n",
    "            print('chatbot perplexity:         ', scores['perplexity'])\n",
    "    elif verbose:\n",
    "        print(\"Symmetric mode, skipping Perplexity.\")\n",
    "    # 2) computes metrics for bleu\n",
    "    metric = BBMetric.load_metric(\"bleu\")\n",
    "    scores['bleu'] = [metric.compute(predictions=label_responses, references=context_sentences)]\n",
    "    scores['bleu'].append(metric.compute(predictions=chatbot_responses, references=context_sentences))\n",
    "    scores['bleu'].append(metric.compute(predictions=chatbot_responses, references=label_responses))\n",
    "    scores['bleu'].append(ccl_sim(scores['bleu'][0]['score'],\n",
    "                                  scores['bleu'][1]['score'],\n",
    "                                  scores['bleu'][2]['score']))\n",
    "    if verbose:\n",
    "        print('===         BLEU         ===')\n",
    "        print('context-to-'+lbl_text+' bleu:      ', scores['bleu'][0])\n",
    "        print('context-to-'+cht_text+' bleu:    ', scores['bleu'][1])\n",
    "        print(lbl_text+'-to-'+cht_text+' bleu:      ', scores['bleu'][2])\n",
    "        print('ccl-sim bleu:            ', scores['bleu'][3])\n",
    "    # 3) computes metrics for rouge-L\n",
    "    metric = BBMetric.load_metric(\"rouge l\")\n",
    "    scores['rouge l'] = [metric.compute(predictions=label_responses, references=context_sentences)]\n",
    "    scores['rouge l'].append(metric.compute(predictions=chatbot_responses, references=context_sentences))\n",
    "    scores['rouge l'].append(metric.compute(predictions=chatbot_responses, references=label_responses))\n",
    "    scores['rouge l'].append(ccl_sim(scores['rouge l'][0]['score'],\n",
    "                                     scores['rouge l'][1]['score'],\n",
    "                                     scores['rouge l'][2]['score']))\n",
    "    if verbose:\n",
    "        print('===        ROUGE-L       ===')\n",
    "        print('context-to-'+lbl_text+' rouge:     ', scores['rouge l'][0])\n",
    "        print('context-to-'+cht_text+' rouge:   ', scores['rouge l'][1])\n",
    "        print(lbl_text+'-to-'+cht_text+' rouge:     ', scores['rouge l'][2])\n",
    "        print('ccl-sim rouge:            ', scores['rouge l'][3])\n",
    "    # 4) computes metrics for distinct\n",
    "    metric = BBMetric.load_metric(\"distinct\")\n",
    "    scores['distinct'] = [metric.compute(sentences=context_sentences)]\n",
    "    scores['distinct'].append(metric.compute(sentences=label_responses))\n",
    "    scores['distinct'].append(metric.compute(sentences=chatbot_responses))\n",
    "    if verbose:\n",
    "        print('===       DISTINCT      ===')\n",
    "        print('context distinct:          ', scores['distinct'][0])\n",
    "        print(lbl_text+' distinct:          ', scores['distinct'][1])\n",
    "        print(cht_text+' distinct:          ', scores['distinct'][2])\n",
    "    # 6) computes emotion metric\n",
    "    metric = BBMetric.load_metric(\"emotion\")\n",
    "    scores['emotion'] = [metric.compute(sentences=context_sentences)]\n",
    "    scores['emotion'].append(metric.compute(sentences=label_responses))\n",
    "    scores['emotion'].append(metric.compute(sentences=chatbot_responses))\n",
    "    scores['emotion'].append(sp.stats.stats.pearsonr(scores['emotion'][1]['score'],\n",
    "                                                     scores['emotion'][2]['score'])[0])\n",
    "    if verbose:\n",
    "        print('===       EMOTION       ===')\n",
    "        print('context emotions:            \\n', list(zip(scores['emotion'][0]['label'], scores['emotion'][0]['score'])))\n",
    "        print(lbl_text+' emotions:              \\n', list(zip(scores['emotion'][1]['label'], scores['emotion'][1]['score'])))\n",
    "        print(cht_text+' emotions:            \\n', list(zip(scores['emotion'][2]['label'], scores['emotion'][2]['score'])))\n",
    "        print(lbl_text+'-'+cht_text+'emotion corr:  \\n', scores['emotion'][3])\n",
    "    # 8) computes sas metric\n",
    "    metric = BBMetric.load_metric(\"semantic answer similarity\")\n",
    "    scores['semantic answer similarity'] = [metric.compute(predictions=context_sentences,\n",
    "                                                    references=label_responses)]\n",
    "    scores['semantic answer similarity'].append(metric.compute(predictions=context_sentences,\n",
    "                                                        references=chatbot_responses))\n",
    "    scores['semantic answer similarity'].append(metric.compute(predictions=label_responses,\n",
    "                                                        references=chatbot_responses))\n",
    "    scores['semantic answer similarity'].append(ccl_sim(scores['semantic answer similarity'][0]['score'],\n",
    "                                                        scores['semantic answer similarity'][1]['score'],\n",
    "                                                        scores['semantic answer similarity'][2]['score']))\n",
    "    if verbose:\n",
    "        print('===         SAS         ===')\n",
    "        print('context-'+lbl_text+' sas:          ', scores['semantic answer similarity'][0])\n",
    "        print('context-'+cht_text+' sas:        ', scores['semantic answer similarity'][1])\n",
    "        print(lbl_text+'-'+cht_text+' sas:          ', scores['semantic answer similarity'][2])\n",
    "        print('ccl-sim sas:               ', scores['semantic answer similarity'][3])\n",
    "    # 9) computes metrics for semantic classifier\n",
    "    metric = BBMetric.load_metric(\"semantic classifier\")\n",
    "    start_time = time.time()\n",
    "    scores['semantic classifier'] = [metric.compute(character=character, character_dict=character_dict, \n",
    "                                                   base_folder=base_folder, sentences=label_responses,\n",
    "                                                   n_sentences=classifier_n_sentences)]\n",
    "    scores['semantic classifier'].append(metric.compute(character=character, character_dict=character_dict, \n",
    "                                                   base_folder=base_folder, sentences=chatbot_responses,\n",
    "                                                   n_sentences=classifier_n_sentences))\n",
    "    end_time = time.time()\n",
    "    if verbose:\n",
    "        print('=== SEMANTIC CLASSIFIER ===')\n",
    "        print('sem-classifier '+lbl_text+':                ', scores['semantic classifier'][0])\n",
    "        print('sem-classifier '+cht_text+':                  ', scores['semantic classifier'][1])\n",
    "        print('time elapsed computing semantic classifier:  {:.2f} s'.format(end_time - start_time))\n",
    "    if not label_chatbot_symmetry and os.path.exists(os.path.join(os.getcwd(), \"Data\", \"Characters\", character, \"humancoherence.csv\")):\n",
    "        scores['human'] = {}\n",
    "        metric = BBMetric.load_metric(\"human - coherence\")\n",
    "        scores['human']['coherence'] = metric.compute(filepath=os.path.join(os.getcwd(), \"Data\", \"Characters\",\n",
    "                                                                            character, \"humancoherence.csv\"))\n",
    "        metric = BBMetric.load_metric(\"human - style\")\n",
    "        scores['human']['style'] = metric.compute(filepath=os.path.join(os.getcwd(), \"Data\", \"Characters\",\n",
    "                                                                        character, \"humanstyle.csv\"))\n",
    "        metric = BBMetric.load_metric(\"human - consistency\")\n",
    "        scores['human']['consistency'] = metric.compute(filepath=os.path.join(os.getcwd(), \"Data\", \"Characters\",\n",
    "                                                                              character, \"humanconsistency.csv\"))\n",
    "        if verbose:\n",
    "            print('===    HUMAN METRICS    ===')\n",
    "            print('coherence:                 ', scores['human']['coherence'])\n",
    "            print('consistency:               ', scores['human']['consistency'])\n",
    "            print('style:                     ', scores['human']['style'])\n",
    "    elif verbose:\n",
    "        print(\"Symmetric mode, skipping Human metrics.\")\n",
    "    if include_sentences:\n",
    "        sentences_df = {}\n",
    "        sentences_df['context'] = context_sentences\n",
    "        sentences_df[lbl_text] = label_responses\n",
    "        sentences_df[cht_text] = chatbot_responses\n",
    "        scores['sentences'] = sentences_df\n",
    "        if verbose:\n",
    "            print('===      SENTENCES      ===')\n",
    "            for i in range(len(context_sentences)):\n",
    "                print(\"* context: \", context_sentences[i])\n",
    "                print(\"* \" + lbl_text + \":\", label_responses[i])\n",
    "                print(\"* \" + cht_text + \":\", chatbot_responses[i])\n",
    "                print()\n",
    "    elif verbose:\n",
    "        print(\"Skipping sentence outputting.\")\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for i in range(1):\n",
    "    print(\"##### Sample \" + str(i+1) + \" #####\")\n",
    "    context_sentence = df_char['ctx'][i]\n",
    "    chatbot_response = df_char['prd_greedy'][i]\n",
    "    label_response   = df_char['lbl'][i]\n",
    "    compute_sample_metrics(context_sentence, label_response, chatbot_response)\n",
    "    print()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "set_size = 10\n",
    "i = 30\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "print(\"##### Set (Size \" + str(set_size) + \") #####\")\n",
    "context_sentences = list(df_char['ctx'][i:i+set_size])\n",
    "chatbot_responses = list(df_char['prd_greedy'][i:i+set_size])\n",
    "label_responses   = list(df_char['lbl'][i:i+set_size])\n",
    "compute_set_metrics(model, tokenizer,\n",
    "                    context_sentences, label_responses, chatbot_responses, character)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"##### Full Test Set #####\")\n",
    "context_sentences = list(df_char['ctx'])\n",
    "chatbot_responses = list(df_char['prd_greedy'])\n",
    "label_responses   = list(df_char['lbl'])\n",
    "scores = compute_set_metrics(model, tokenizer,\n",
    "                    context_sentences, \n",
    "                    label_responses, \n",
    "                    chatbot_responses,\n",
    "                    character,\n",
    "                    classifier_n_sentences=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_json(metrics_folder, character+'_base_metrics', scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Between Different Sampling Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"##### Greedy vs. N-Beams #####\")\n",
    "context_sentences = list(df_char['ctx'])\n",
    "greedy_responses  = list(df_char['prd_greedy'])\n",
    "nbeams_responses  = list(df_char['prd_nbeams'])\n",
    "scores['greedy_vs_nbeams'] = compute_set_metrics(None, None,\n",
    "                                                 context_sentences,\n",
    "                                                 greedy_responses,\n",
    "                                                 nbeams_responses,\n",
    "                                                 character,\n",
    "                                                 classifier_n_sentences=75,\n",
    "                                                 label_chatbot_symmetry=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split == True:\n",
    "    save_as_json(metrics_folder, character+'_greedy_vs_nbeams_metrics', scores['greedy_vs_nbeams'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"##### Greedy vs. Sampling #####\")\n",
    "context_sentences   = list(df_char['ctx'])\n",
    "greedy_responses    = list(df_char['prd_greedy'])\n",
    "sampling_responses  = list(df_char['prd_sampling'])\n",
    "scores['greedy_vs_sampling'] = compute_set_metrics(None, None,\n",
    "                                                   context_sentences,\n",
    "                                                   greedy_responses,\n",
    "                                                   sampling_responses,\n",
    "                                                   character,\n",
    "                                                   classifier_n_sentences=75,\n",
    "                                                   label_chatbot_symmetry=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split == True:\n",
    "    save_as_json(metrics_folder, character+'_greedy_vs_sampling_metrics', scores['greedy_vs_sampling'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"##### N-Beams vs. Sampling #####\")\n",
    "context_sentences   = list(df_char['ctx'])\n",
    "nbeams_responses    = list(df_char['prd_nbeams'])\n",
    "sampling_responses  = list(df_char['prd_sampling'])\n",
    "scores['nbeams_vs_sampling'] = compute_set_metrics(None, None,\n",
    "                                                   context_sentences,\n",
    "                                                   nbeams_responses,\n",
    "                                                   sampling_responses,\n",
    "                                                   character,\n",
    "                                                   classifier_n_sentences=75,\n",
    "                                                   label_chatbot_symmetry=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split == True:\n",
    "    save_as_json(metrics_folder, character+'_nbeams_vs_sampling_metrics', scores['nbeams_vs_sampling'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split == True:    \n",
    "    scores = {}\n",
    "    scores['greedy_vs_nbeams'] = load_from_json(\n",
    "        filepath=metrics_folder,\n",
    "        filename=character+'_greedy_vs_nbeams_metrics'\n",
    "    )\n",
    "    scores['greedy_vs_sampling'] = load_from_json(\n",
    "        filepath=metrics_folder,\n",
    "        filename=character+'_greedy_vs_sampling_metrics'\n",
    "    )\n",
    "    scores['nbeams_vs_sampling'] = load_from_json(\n",
    "        filepath=metrics_folder,\n",
    "        filename=character+'_nbeams_vs_sampling_metrics'\n",
    "    )\n",
    "    \n",
    "    os.remove(os.path.join(\n",
    "        metrics_folder,\n",
    "        character+'_greedy_vs_nbeams_metrics.json'\n",
    "    ))\n",
    "    os.remove(os.path.join(\n",
    "        metrics_folder,\n",
    "        character+'_greedy_vs_sampling_metrics.json'\n",
    "    ))\n",
    "    os.remove(os.path.join(\n",
    "        metrics_folder,\n",
    "        character+'_nbeams_vs_sampling_metrics.json'\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_json(metrics_folder, character+'_sampling_comparison_metrics', scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Between Non-Finetuned And Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_def_sampling = get_predictions_cached(sample_questions, model_def,\n",
    "                                                  os.path.join(in_folder_def, 'from_' + character + '_df_' + '_sampling.json'),\n",
    "                                                  \"Sampling\", override_predictions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_char_def = get_dataframe_for_metrics(character_hg['test'], None, None, predictions_def_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for i in range(1):\n",
    "    print(\"##### Sample \" + str(i+1) + \" #####\")\n",
    "    context_sentence   = df_char['ctx'][i]\n",
    "    character_response = df_char['prd_sampling'][i]\n",
    "    default_response   = df_char_def['prd_sampling'][i]\n",
    "    compute_sample_metrics(context_sentence, default_response, character_response, label_chatbot_symmetry=True)\n",
    "    print()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "set_size = 50\n",
    "i = 30\n",
    "print(\"##### Set (Size \" + str(set_size) + \") #####\")\n",
    "context_sentences   = list(df_char['ctx'][i:i+set_size])\n",
    "character_responses = list(df_char['prd_sampling'][i:i+set_size])\n",
    "default_responses   = list(df_char_def['prd_sampling'][i:i+set_size])\n",
    "compute_set_metrics(None, None,\n",
    "                    context_sentences, default_responses, character_responses, character, label_chatbot_symmetry=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"##### Full Test Set #####\")\n",
    "context_sentences   = list(df_char['ctx'])\n",
    "character_responses = list(df_char['prd_sampling'])\n",
    "default_responses   = list(df_char_def['prd_sampling'])\n",
    "scores = compute_set_metrics(None, None,\n",
    "                             context_sentences, \n",
    "                             default_responses, \n",
    "                             character_responses,\n",
    "                             character,\n",
    "                             classifier_n_sentences=75,\n",
    "                             label_chatbot_symmetry=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_json(metrics_folder, character+'_vs_nonfinetuned_metrics', scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Between Character 1 & Character 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_small(sample_questions, model, generation_method):\n",
    "    print(\"Creating predictions\")\n",
    "    predictions = list()\n",
    "    for x in tqdm(sample_questions):\n",
    "        tokenized_question = tokenizer.encode(x + tokenizer.eos_token, return_tensors='tf')\n",
    "        max_length = 128 + tokenized_question.shape[1]\n",
    "        if generation_method == \"Greedy\":\n",
    "            generated_answer = model.generate(tokenized_question,\n",
    "                                pad_token_id=tokenizer.eos_token_id, max_length=max_length)[0].numpy().tolist()\n",
    "        elif generation_method == \"Beam Search\":\n",
    "            generated_answer = model.generate(tokenized_question,\n",
    "                                         pad_token_id=tokenizer.eos_token_id, max_length=max_length,\n",
    "                                         n_beams=n_beams)[0].numpy().tolist()\n",
    "        elif generation_method == \"Sampling\":\n",
    "                b = True\n",
    "                c = 0\n",
    "                while b:\n",
    "                    generated_answer = model.generate(tokenized_question,\n",
    "                                                 pad_token_id=tokenizer.eos_token_id, max_length=max_length,\n",
    "                                                 do_sample=True, top_k=top_k, top_p=top_p)[0].numpy().tolist()\n",
    "                    \n",
    "                    c+= 1\n",
    "                    if len(generated_answer[len(tokenized_question[0]):])>1:\n",
    "                        b = False         \n",
    "                    if c>100: \n",
    "                        generated_answer[len(tokenized_question[0]):] = tokenizer.encode('hi') + [tokenizer.eos_token_id]\n",
    "                        break \n",
    "                        \n",
    "        predictions.append(generated_answer[len(tokenized_question[0]):])\n",
    "        \n",
    "        assert all([len(p)>1 for p in predictions])\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-1e3df9a8d064519d\n",
      "Reusing dataset csv (C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\cache\\csv\\default-1e3df9a8d064519d\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d2ddf5e349445c84909e2777d5d648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_common = load_dataset('csv',\n",
    "                         data_files=os.path.join(base_folder, 'Data', 'common_dataset.csv'), \n",
    "                         cache_dir=os.path.join(base_folder, \"cache\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['context', 'label', 'source'],\n",
       "        num_rows: 35\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 35/35 [00:43<00:00,  1.24s/it]\n"
     ]
    }
   ],
   "source": [
    "predictions_1_sampling = get_predictions_small(df_common['train']['context'], model, \"Sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 35/35 [00:30<00:00,  1.15it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions_2_sampling = get_predictions_small(df_common['train']['context'], model_2, \"Sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 35/35 [00:00<00:00, 3178.88it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 35/35 [00:00<00:00, 3179.02it/s]\n"
     ]
    }
   ],
   "source": [
    "df_common_char_1 = get_dataframe_for_metrics(df_common['train'], None, None, predictions_1_sampling)\n",
    "df_common_char_2 = get_dataframe_for_metrics(df_common['train'], None, None, predictions_2_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Vader  Vs. Harry #####\n",
      "=== SEMANTIC SIMILARITY ===\n",
      "context-chatbota similarity:    {'score': 0.2226836234331131, 'std': 0.11853612959384918}\n",
      "context-chatbotb similarity:  {'score': 0.29005467891693115, 'std': 0.223917156457901}\n",
      "chatbotb-chatbota similarity:    {'score': 0.2229309231042862, 'std': 0.1433691382408142}\n",
      "ccl-sim similarity:             0.4597474723127484\n",
      "Symmetric mode, skipping Perplexity.\n",
      "===         BLEU         ===\n",
      "context-to-chatbota bleu:       {'score': 0.0, 'std': 0.0}\n",
      "context-to-chatbotb bleu:     {'score': 0.0, 'std': 0.0}\n",
      "chatbota-to-chatbotb bleu:       {'score': 0.0, 'std': 0.0}\n",
      "ccl-sim bleu:             0.5\n",
      "===        ROUGE-L       ===\n",
      "context-to-chatbota rouge:      {'score': 0.033956470083679594, 'std': 0.06358705660302356}\n",
      "context-to-chatbotb rouge:    {'score': 0.07784162927020069, 'std': 0.2178383019726325}\n",
      "chatbota-to-chatbotb rouge:      {'score': 0.03275117578929959, 'std': 0.06086268768316233}\n",
      "ccl-sim rouge:             0.4576141141696828\n",
      "===       DISTINCT      ===\n",
      "context distinct:           {'score': 0.10984328048288133, 'std': 0.06188826948933667}\n",
      "chatbota distinct:           {'score': 0.12587495659002015, 'std': 0.04845288016185349}\n",
      "chatbotb distinct:           {'score': 0.09660274303910543, 'std': 0.05414288069778125}\n",
      "ccl-sim distinct:           0.488762876193603\n",
      "===       EMOTION       ===\n",
      "context emotions:            \n",
      " [('sadness', 0.014541776718605043), ('joy', 0.34016804504208265), ('love', 0.11487390943678162), ('anger', 0.37266245127040226), ('fear', 0.15484204553899222), ('surprise', 0.002911792164585287)]\n",
      "chatbota emotions:              \n",
      " [('sadness', 0.028288115906096728), ('joy', 0.47811903446646675), ('love', 0.009788645557793123), ('anger', 0.3695682842417487), ('fear', 0.11203806384084081), ('surprise', 0.0021978484408464284)]\n",
      "chatbotb emotions:            \n",
      " [('sadness', 0.12901116356952116), ('joy', 0.35583138256666386), ('love', 0.03307676180806344), ('anger', 0.370416255773411), ('fear', 0.10844796957140455), ('surprise', 0.00321646689595322)]\n",
      "chatbota-chatbotbemotion corr:  \n",
      " 0.9550296524630981\n",
      "===         SAS         ===\n",
      "context-chatbota sas:           {'score': 0.046411119401454926, 'std': 0.0846337229013443}\n",
      "context-chatbotb sas:         {'score': 0.1339196413755417, 'std': 0.2192155122756958}\n",
      "chatbota-chatbotb sas:           {'score': 0.057538606226444244, 'std': 0.12020241469144821}\n",
      "ccl-sim sas:                0.4179756943381988\n",
      "=== SEMANTIC CLASSIFIER ===\n",
      "sem-classifier chatbota:                 {'score': 0.716515839099884, 'std': 0.4132871627807617}\n",
      "sem-classifier chatbotb:                   {'score': 0.06622001528739929, 'std': 0.20783083140850067}\n",
      "time elapsed computing semantic classifier:  11.81 s\n",
      "Symmetric mode, skipping Human metrics.\n",
      "===      SENTENCES      ===\n",
      "* context:  Barney, this is about the building.\n",
      "* chatbota: Aaah, Admiral Piett\n",
      "* chatbotb: It's not a building.\n",
      "\n",
      "* context:  All right. I'll be right there. Stay where you are.\n",
      "* chatbota: Yes, my master.\n",
      "* chatbotb: Sirius.\n",
      "\n",
      "* context:  I think there's a pretty girl smiling at me there.\n",
      "* chatbota: It's too late, Obi-Wan.\n",
      "* chatbotb: Malfoy, come on.\n",
      "\n",
      "* context:  I love you, man.\n",
      "* chatbota: I am altering the deal.Commence firestorm.\n",
      "* chatbotb: I love you.\n",
      "\n",
      "* context:  Not even if she's hot?\n",
      "* chatbota: If I had a nickel for everytime I just told you that you'd find that you're unwise to lower your\n",
      "* chatbotb: Don't worry. You'll find her.\n",
      "\n",
      "* context:  Soft kitty, warm kitty Little ball of fur\n",
      "* chatbota: Admiral Piett walks by the window as the stars\n",
      "* chatbotb: Now I want to know what's going on with this little fella, do you think he's asleep?\n",
      "\n",
      "* context:  Penny.\n",
      "* chatbota: Come on.\n",
      "* chatbotb: This is going to take some of the fun out of tomorrow.\n",
      "\n",
      "* context:  Oh. Sheldon, thank you. Thats so romantic. But what about Rajesh? He was okay with you choosing the name?\n",
      "* chatbota: He told you not to go with me, Commander. I don't believe you!\n",
      "* chatbotb: Yes, I'd never have called him that.\n",
      "\n",
      "* context:  I didnt break it. I, I guess Stuart sold it to me like this.\n",
      "* chatbota: Good. See to it that you accepted our help. I will not fight you again, nor join you!\n",
      "* chatbotb: It's the fastest broom in the world.\n",
      "\n",
      "* context:  Be careful.\n",
      "* chatbota: It is your destiny. Let's do this.\n",
      "* chatbotb: I wonder if I could have that in my bedroom?\n",
      "\n",
      "* context:  But why would anyone go near that dog?\n",
      "* chatbota: I told you that you would never know the power of thedark side. The Force is strong with this one.\n",
      "* chatbotb: To find you.\n",
      "\n",
      "* context:  Expecto Patronum!\n",
      "* chatbota: Aaah, the Death Star plans are hidden in the most obscure part of the image projected by the Falcon.\n",
      "* chatbotb: All right there, Tom.\n",
      "\n",
      "* context:  Ron Weasley.\n",
      "* chatbota: He certainly would not want to face his son again, nor his father.\n",
      "* chatbotb: Ron Weasley.\n",
      "\n",
      "* context:  I spoke a different language?\n",
      "* chatbota: Perhaps you are in danger.\n",
      "* chatbotb: Didja?\n",
      "\n",
      "* context:  Harry?\n",
      "* chatbota: Luke. There will be more of him!\n",
      "* chatbotb: Harry, wait! We've got to tell you something!\n",
      "\n",
      "* context:  OK. First Bender, then Flexo, then Fry.\n",
      "* chatbota: Yes, my lord.\n",
      "* chatbotb: No. This is Professor Quirrell.\n",
      "\n",
      "* context:  Just relax, Bender. Tomorrow we'll pry you down, have a nice breakfast and then go hunt down and slaughter that ancient evil.\n",
      "* chatbota: Vader calmly adjusts his control stick as the MillenniumFalcon's\n",
      "* chatbotb: What did I do?\n",
      "\n",
      "* context:  I'm too scared.\n",
      "* chatbota: Ohh you're so right. Leave me.\n",
      "* chatbotb: Get out.\n",
      "\n",
      "* context:  Dr. Zoidberg? Are you OK?\n",
      "* chatbota: What is thy bidding, my Master?\n",
      "* chatbotb: Sorry, Harry, but your parents died in a car crash.\n",
      "\n",
      "* context:  Fry, thank God we found you.\n",
      "* chatbota: Oh, no, my lord.\n",
      "* chatbotb: I was lost, I...\n",
      "\n",
      "* context:  I will not fight you.\n",
      "* chatbota: We must not turn\n",
      "* chatbotb: Sirius Black, your Grace.\n",
      "\n",
      "* context:  Lord Vader, what about Leia and theWookiee?\n",
      "* chatbota: He would be honored to sit on the throne of Anakin.\n",
      "* chatbotb: Not so fast.\n",
      "\n",
      "* context:  The Emperor's coming here?\n",
      "* chatbota: My lord, what is it?\n",
      "* chatbotb: Now I can relax and think about what I've missed.\n",
      "\n",
      "* context:  Shall I hold them?\n",
      "* chatbota: As you wish.\n",
      "* chatbotb: Yeah, no problem. Go in.\n",
      "\n",
      "* context:  Lord Vader, what about Leia and theWookiee?\n",
      "* chatbota: You know nothing, son.\n",
      "* chatbotb: We can't be sure.\n",
      "\n",
      "* context:  Oh! Joey uh, were you in our room last night?\n",
      "* chatbota: The Force is strong with this one.\n",
      "* chatbotb: It was you.\n",
      "\n",
      "* context:  Hey.\n",
      "* chatbota: You're a part of this operation.\n",
      "* chatbotb: Come on, now!\n",
      "\n",
      "* context:  Joey... are you sure? I mean, I know how much you love him!\n",
      "* chatbota: You don't have to make me destroy you, my son. I'm here\n",
      "* chatbotb: He's my everything.\n",
      "\n",
      "* context:  Ok, ten.\n",
      "* chatbota: All right.\n",
      "* chatbotb: One, and a half!\n",
      "\n",
      "* context:  Joey, Ross is gonna be here any second, would you mind watching Ben for me while I use the ladies' room?\n",
      "* chatbota: Good. Just hold them off for a minute.\n",
      "* chatbotb: Do you mind?\n",
      "\n",
      "* context:  What are you doing for a living?\n",
      "* chatbota: Good... now tell me about your journey, General.\n",
      "* chatbotb: Hagrid, this is my final form.\n",
      "\n",
      "* context:  How are you doing?\n",
      "* chatbota: He sits in the control room, watching as the stars whip past\n",
      "* chatbotb: Goodbye.\n",
      "\n",
      "* context:  Where are you going to?\n",
      "* chatbota: Good. Let's go\n",
      "* chatbotb: No. It's a secret.\n",
      "\n",
      "* context:  What are you wearing?\n",
      "* chatbota: He turned to his men. His eyes glazed over the horizon, and he turned thirty seconds later.\n",
      "* chatbotb: We have to know.\n",
      "\n",
      "* context:  What do you want to do tonight?\n",
      "* chatbota: Vader swings his ship around toward the Millennium Falcon.\n",
      "* chatbotb: Not particularly.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"##### \" + character + \"  Vs. \" + character_2 + \" #####\")\n",
    "context_sentences   = list(df_common_char_1['ctx'])\n",
    "chatbot_responses   = list(df_common_char_1['prd_sampling'])\n",
    "chatbot_2_responses = list(df_common_char_2['prd_sampling'])\n",
    "scores = compute_set_metrics(None, None,\n",
    "                            context_sentences, chatbot_responses, chatbot_2_responses, character,\n",
    "                            include_sentences=True, label_chatbot_symmetry=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_json(metrics_folder, character+'_vs_'+character_2+'_metrics', scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

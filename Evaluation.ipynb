{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data.data_dicts import character_dict, source_dict, random_state\n",
    "\n",
    "model_name = 'microsoft/DialoGPT-small'\n",
    "character = 'Barney' # 'Barney' | 'Sheldon' | 'Harry' | 'Fry' | 'Vader' | 'Joey' | 'Phoebe' | 'Bender' | Default'\n",
    "character_2 = 'Sheldon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount google drive\n",
    "import os\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive',force_remount=True)\n",
    "    base_folder = '/content/drive/My Drive/unibo/NLP_project/BarneyBot'\n",
    "    os.system(\"pip install datasets\")\n",
    "    os.system(\"pip install transformers\")\n",
    "    os.system(\"pip install rouge_score\")\n",
    "    os.system(\"pip install -U sentence-transformers\")\n",
    "else:\n",
    "    base_folder = os.getcwd()\n",
    "    \n",
    "in_folder = os.path.join(base_folder, 'Data', 'Characters', character)\n",
    "if not os.path.exists(in_folder):\n",
    "    os.makedirs(in_folder)\n",
    "out_folder = os.path.join(base_folder, 'Data', 'Characters', character)\n",
    "if not os.path.exists(out_folder):\n",
    "    os.makedirs(out_folder)\n",
    "    \n",
    "in_folder_2 = os.path.join(base_folder, 'Data', 'Characters', character_2)\n",
    "if not os.path.exists(in_folder_2):\n",
    "    os.makedirs(in_folder_2)\n",
    "out_folder_2 = os.path.join(base_folder, 'Data', 'Characters', character_2)\n",
    "if not os.path.exists(out_folder_2):\n",
    "    os.makedirs(out_folder_2)\n",
    "    \n",
    "in_folder_def = os.path.join(base_folder, 'Data', 'Characters', 'Default')\n",
    "if not os.path.exists(in_folder_def):\n",
    "    os.makedirs(in_folder_def)\n",
    "out_folder_def = os.path.join(base_folder, 'Data', 'Characters', 'Default')\n",
    "if not os.path.exists(out_folder_def):\n",
    "    os.makedirs(out_folder_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "def load_df(character):\n",
    "    dataset_path = os.path.join(base_folder, \"Data\", \"Characters\", character, character+'.csv')\n",
    "    \n",
    "    character_hg = load_dataset('csv', \n",
    "                                data_files=dataset_path, \n",
    "                                cache_dir=os.path.join(base_folder, \"cache\"))\n",
    "    \n",
    "    # 85% train / 10% test / 5% validation\n",
    "    train_test_hg = character_hg['train'].train_test_split(test_size=0.15, seed=random_state)\n",
    "    test_val = train_test_hg['test'].train_test_split(test_size=0.33, seed=random_state)\n",
    "    \n",
    "    \n",
    "    character_hg = DatasetDict({\n",
    "        'train': train_test_hg['train'],\n",
    "        'test': test_val['train'],\n",
    "        'val': test_val['test']\n",
    "    })\n",
    "    \n",
    "    return character_hg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_conv(row, tokenizer):\n",
    "    MAX_LENGTH = 512\n",
    "    row = list(reversed(list(row.values())))\n",
    "    model_inputs = tokenizer(row)\n",
    "    tokenizer_pad_token_id = tokenizer.encode('#')[0]\n",
    "    for i in range(len(model_inputs['input_ids'])):\n",
    "        model_inputs['input_ids'][i].append(tokenizer.eos_token_id)\n",
    "        model_inputs['attention_mask'][i].append(1)\n",
    "    model_inputs['input_ids'] = [item for sublist in model_inputs['input_ids'] for item in sublist]\n",
    "    model_inputs['attention_mask'] = [item for sublist in model_inputs['attention_mask'] for item in sublist]\n",
    "    if MAX_LENGTH > len(model_inputs['input_ids']):\n",
    "        model_inputs['input_ids'] += [tokenizer_pad_token_id] * (MAX_LENGTH - len(model_inputs['input_ids']))\n",
    "        model_inputs['attention_mask'] += [0] * (MAX_LENGTH - len(model_inputs['attention_mask']))\n",
    "    elif MAX_LENGTH < len(model_inputs['input_ids']):\n",
    "        model_inputs['input_ids'] = model_inputs['input_ids'][:MAX_LENGTH-1]\n",
    "        model_inputs['input_ids'][-1] = tokenizer.eos_token_id\n",
    "        model_inputs['attention_mask'] = model_inputs['attention_mask'][:MAX_LENGTH-1]\n",
    "        model_inputs['attention_mask'][-1] = 1\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    tokenizer.pad_token = '#'\n",
    "    model_inputs = construct_conv(examples, tokenizer)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-c2a24858b2300081\n",
      "Reusing dataset csv (C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\cache\\csv\\default-c2a24858b2300081\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b27bdb4fc4e94e62978defd601d43b3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\cache\\csv\\default-c2a24858b2300081\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-d2728d4eab5179a3.arrow and C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\cache\\csv\\default-c2a24858b2300081\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-87ea209a870637bc.arrow\n",
      "Loading cached split indices for dataset at C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\cache\\csv\\default-c2a24858b2300081\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-bcd94ff086697f0e.arrow and C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\cache\\csv\\default-c2a24858b2300081\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-bd8f111a48e25ec3.arrow\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"HF_DATASETS_CACHE\"] = os.path.join(base_folder, \"cache\")\n",
    "character_hg = load_df(character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_folder = os.path.join(out_folder, character_dict[character]['checkpoint_folder'])\n",
    "checkpoint_folder_2 = os.path.join(out_folder_2, character_dict[character_2]['checkpoint_folder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\Data\\Characters\\Barney\\barney_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\Data\\Characters\\Sheldon\\sheldon_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at microsoft/DialoGPT-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=os.path.join(base_folder, \"cache\"))\n",
    "tokenizer.pad_token = '#'\n",
    "\n",
    "model = TFAutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=checkpoint_folder)\n",
    "model_2 = TFAutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=checkpoint_folder_2)\n",
    "model_def = TFAutoModelForCausalLM.from_pretrained(model_name, cache_dir=os.path.join(base_folder, \"cache\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_questions = character_hg['test']['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_beams = 3\n",
    "top_k = 50\n",
    "top_p = 0.92\n",
    "\n",
    "def get_predictions_cached(sample_questions, model, filename, generation_method, override_predictions=False):\n",
    "    prediction_path = os.path.join(in_folder, filename)\n",
    "    if os.path.exists(prediction_path) and not override_predictions:\n",
    "        print(\"Loading predictions from stored file\")\n",
    "        with open(prediction_path, 'r') as file:\n",
    "            json_string = file.read()\n",
    "        predictions = json.loads(json_string)\n",
    "        print(\"Loaded predictions from stored file\")\n",
    "\n",
    "    else:\n",
    "        print(\"Creating predictions\")\n",
    "        predictions = list()\n",
    "        for x in tqdm(sample_questions):\n",
    "            tokenized_question = tokenizer.encode(x + tokenizer.eos_token, return_tensors='tf')\n",
    "            max_length = 128 + tokenized_question.shape[1]\n",
    "            if generation_method == \"Greedy\":\n",
    "                generated_answer = model.generate(tokenized_question,\n",
    "                                    pad_token_id=tokenizer.eos_token_id, max_length=max_length)[0].numpy().tolist()\n",
    "            elif generation_method == \"Beam Search\":\n",
    "                generated_answer = model.generate(tokenized_question,\n",
    "                                             pad_token_id=tokenizer.eos_token_id, max_length=max_length,\n",
    "                                             n_beams=n_beams)[0].numpy().tolist()\n",
    "            elif generation_method == \"Sampling\":\n",
    "                generated_answer = model.generate(tokenized_question,\n",
    "                                             pad_token_id=tokenizer.eos_token_id, max_length=max_length,\n",
    "                                             do_sample=True, top_k=top_k, top_p=top_p)[0].numpy().tolist()\n",
    "            predictions.append(generated_answer[len(tokenized_question[0]):])\n",
    "\n",
    "        # Save predictions as a JSON file\n",
    "        output_string = json.dumps(predictions)\n",
    "        with open(prediction_path, 'w') as file:\n",
    "            file.write(output_string)\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading predictions from stored file\n",
      "Loaded predictions from stored file\n",
      "Loading predictions from stored file\n",
      "Loaded predictions from stored file\n",
      "Loading predictions from stored file\n",
      "Loaded predictions from stored file\n"
     ]
    }
   ],
   "source": [
    "predictions_greedy = get_predictions_cached(sample_questions, model,\n",
    "                                            character_dict[character]['prediction_filename'] + '_greedy.json',\n",
    "                                            \"Greedy\")\n",
    "predictions_nbeams = get_predictions_cached(sample_questions, model,\n",
    "                                            character_dict[character]['prediction_filename'] + '_nbeams.json',\n",
    "                                            \"Beam Search\")\n",
    "predictions_sampling = get_predictions_cached(sample_questions, model,\n",
    "                                              character_dict[character]['prediction_filename'] + '_sampling.json',\n",
    "                                              \"Sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe_for_metrics(data_test, predictions_greedy, predictions_nbeams, predictions_sampling):\n",
    "    i = 0\n",
    "    df = {'ctx':[], 'ctx_tk':[]}\n",
    "    has_labels = 'response' in data_test.features\n",
    "    if has_labels:\n",
    "        df['lbl'] = []\n",
    "        df['lbl_tk'] = []\n",
    "    if predictions_greedy:\n",
    "        df['prd_greedy'] = []\n",
    "        df['prd_greedy_tk'] = []\n",
    "    if predictions_nbeams:\n",
    "        df['prd_nbeams'] = []\n",
    "        df['prd_nbeams_tk'] = [] \n",
    "    if predictions_sampling:\n",
    "        df['prd_sampling'] = []\n",
    "        df['prd_sampling_tk'] = []\n",
    "    for sample in tqdm(data_test):\n",
    "        # encode the context and label sentences, add the eos_token and return a tensor\n",
    "        ctx_tk = tokenizer.encode(sample['context'] + tokenizer.eos_token, return_tensors='tf').numpy().tolist()\n",
    "        ctx = sample['context']\n",
    "        df['ctx_tk'].append(ctx_tk)\n",
    "        df['ctx'].append(ctx)\n",
    "        if has_labels:\n",
    "            lbl_tk = tokenizer.encode(sample['response'] + tokenizer.eos_token, return_tensors='tf').numpy().tolist()\n",
    "            lbl = sample['response']\n",
    "            df['lbl'].append(lbl)\n",
    "            df['lbl_tk'].append(lbl_tk)\n",
    "        if predictions_greedy:\n",
    "            prd_greedy_tk = predictions_greedy[i]\n",
    "            prd_greedy = tokenizer.decode(prd_greedy_tk, skip_special_tokens=True)\n",
    "            df['prd_greedy'].append(prd_greedy)\n",
    "            df['prd_greedy_tk'].append(prd_greedy_tk)\n",
    "        if predictions_nbeams:\n",
    "            prd_nbeams_tk = predictions_nbeams[i]\n",
    "            prd_nbeams = tokenizer.decode(prd_nbeams_tk, skip_special_tokens=True)\n",
    "            df['prd_nbeams'].append(prd_nbeams)\n",
    "            df['prd_nbeams_tk'].append(prd_nbeams_tk)\n",
    "        if predictions_sampling:\n",
    "            prd_sampling_tk = predictions_sampling[i]\n",
    "            prd_sampling = tokenizer.decode(prd_sampling_tk, skip_special_tokens=True)\n",
    "            df['prd_sampling'].append(prd_sampling)\n",
    "            df['prd_sampling_tk'].append(prd_sampling_tk)\n",
    "        i += 1\n",
    "    return pd.DataFrame(data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 522/522 [00:00<00:00, 1660.91it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ctx</th>\n",
       "      <th>ctx_tk</th>\n",
       "      <th>lbl</th>\n",
       "      <th>lbl_tk</th>\n",
       "      <th>prd_greedy</th>\n",
       "      <th>prd_greedy_tk</th>\n",
       "      <th>prd_nbeams</th>\n",
       "      <th>prd_nbeams_tk</th>\n",
       "      <th>prd_sampling</th>\n",
       "      <th>prd_sampling_tk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I know, it's two years of my life I'm never ge...</td>\n",
       "      <td>[[40, 760, 11, 340, 338, 734, 812, 286, 616, 1...</td>\n",
       "      <td>Daddy's home.</td>\n",
       "      <td>[[48280, 338, 1363, 13, 50256]]</td>\n",
       "      <td>Oh, God!</td>\n",
       "      <td>[5812, 11, 1793, 0, 50256]</td>\n",
       "      <td>Oh, God!</td>\n",
       "      <td>[5812, 11, 1793, 0, 50256]</td>\n",
       "      <td>Oh, of course.</td>\n",
       "      <td>[5812, 11, 286, 1781, 13, 50256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wh-Where'd you get a meatball...</td>\n",
       "      <td>[[1199, 12, 8496, 1549, 345, 651, 257, 6174, 1...</td>\n",
       "      <td>I don't have much time!</td>\n",
       "      <td>[[40, 836, 470, 423, 881, 640, 0, 50256]]</td>\n",
       "      <td>I don't know. I just saw a meatball sub.</td>\n",
       "      <td>[40, 836, 470, 760, 13, 314, 655, 2497, 257, 6...</td>\n",
       "      <td>I don't know. I just saw a meatball sub.</td>\n",
       "      <td>[40, 836, 470, 760, 13, 314, 655, 2497, 257, 6...</td>\n",
       "      <td>You don't remember?</td>\n",
       "      <td>[1639, 836, 470, 3505, 30, 50256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Okay, what is so urgent that you called me and...</td>\n",
       "      <td>[[16454, 11, 644, 318, 523, 18039, 326, 345, 1...</td>\n",
       "      <td>I could tell you knew something was up with me...</td>\n",
       "      <td>[[40, 714, 1560, 345, 2993, 1223, 373, 510, 35...</td>\n",
       "      <td>I'm sorry, I don't follow you.</td>\n",
       "      <td>[40, 1101, 7926, 11, 314, 836, 470, 1061, 345,...</td>\n",
       "      <td>I'm sorry, I don't follow you.</td>\n",
       "      <td>[40, 1101, 7926, 11, 314, 836, 470, 1061, 345,...</td>\n",
       "      <td>All right. It's time to start?</td>\n",
       "      <td>[3237, 826, 13, 632, 338, 640, 284, 923, 30, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How much?</td>\n",
       "      <td>[[2437, 881, 30, 50256]]</td>\n",
       "      <td>A little.</td>\n",
       "      <td>[[32, 1310, 13, 50256]]</td>\n",
       "      <td>I have not decided. I want to get married in a...</td>\n",
       "      <td>[40, 423, 407, 3066, 13, 314, 765, 284, 651, 6...</td>\n",
       "      <td>I have not decided. I want to get married in a...</td>\n",
       "      <td>[40, 423, 407, 3066, 13, 314, 765, 284, 651, 6...</td>\n",
       "      <td>He said he had like 3,000 in suits.</td>\n",
       "      <td>[1544, 531, 339, 550, 588, 513, 11, 830, 287, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You're being super nice. It's... freaking me o...</td>\n",
       "      <td>[[1639, 821, 852, 2208, 3621, 13, 632, 338, 98...</td>\n",
       "      <td>I'm being Barney, and I think tonight's going ...</td>\n",
       "      <td>[[40, 1101, 852, 41921, 11, 290, 314, 892, 997...</td>\n",
       "      <td>I'm not gross. I'm just... gross.</td>\n",
       "      <td>[40, 1101, 407, 10319, 13, 314, 1101, 655, 986...</td>\n",
       "      <td>I'm not gross. I'm just... gross.</td>\n",
       "      <td>[40, 1101, 407, 10319, 13, 314, 1101, 655, 986...</td>\n",
       "      <td>Buckminster Fuller?</td>\n",
       "      <td>[33, 1347, 18462, 31863, 30, 50256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>Okay, I want to lay down some ground rules for...</td>\n",
       "      <td>[[16454, 11, 314, 765, 284, 3830, 866, 617, 23...</td>\n",
       "      <td>Well, well, well. How rich. You make me promis...</td>\n",
       "      <td>[[5779, 11, 880, 11, 880, 13, 1374, 5527, 13, ...</td>\n",
       "      <td>Oh, I know. I just want to be as awesome as sh...</td>\n",
       "      <td>[5812, 11, 314, 760, 13, 314, 655, 765, 284, 3...</td>\n",
       "      <td>Oh, I know. I just want to be as awesome as sh...</td>\n",
       "      <td>[5812, 11, 314, 760, 13, 314, 655, 765, 284, 3...</td>\n",
       "      <td>No, I'll be right over.</td>\n",
       "      <td>[2949, 11, 314, 1183, 307, 826, 625, 13, 50256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>It looks to be a... sacred... spa.</td>\n",
       "      <td>[[1026, 3073, 284, 307, 257, 986, 13626, 986, ...</td>\n",
       "      <td>Owl. How do we go? We will do what? Jump?</td>\n",
       "      <td>[[46, 40989, 13, 1374, 466, 356, 467, 30, 775,...</td>\n",
       "      <td>I'm sorry, I don't follow you.</td>\n",
       "      <td>[40, 1101, 7926, 11, 314, 836, 470, 1061, 345,...</td>\n",
       "      <td>I'm sorry, I don't follow you.</td>\n",
       "      <td>[40, 1101, 7926, 11, 314, 836, 470, 1061, 345,...</td>\n",
       "      <td>It's called a spa.</td>\n",
       "      <td>[1026, 338, 1444, 257, 41900, 13, 50256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>That's putting it a bit strongly.</td>\n",
       "      <td>[[2504, 338, 5137, 340, 257, 1643, 7634, 13, 5...</td>\n",
       "      <td>A bit strongly. She's not my girlfriend.</td>\n",
       "      <td>[[32, 1643, 7634, 13, 1375, 338, 407, 616, 110...</td>\n",
       "      <td>I'm not going to put it in a little strong.</td>\n",
       "      <td>[40, 1101, 407, 1016, 284, 1234, 340, 287, 257...</td>\n",
       "      <td>I'm not going to put it in a little strong.</td>\n",
       "      <td>[40, 1101, 407, 1016, 284, 1234, 340, 287, 257...</td>\n",
       "      <td>And here comes the fun part.</td>\n",
       "      <td>[1870, 994, 2058, 262, 1257, 636, 13, 50256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>I do.</td>\n",
       "      <td>[[40, 466, 13, 50256]]</td>\n",
       "      <td>I'm gonna head out to a reggae concert. I'm a ...</td>\n",
       "      <td>[[40, 1101, 8066, 1182, 503, 284, 257, 842, 25...</td>\n",
       "      <td>You're a good man.</td>\n",
       "      <td>[1639, 821, 257, 922, 582, 13, 50256]</td>\n",
       "      <td>You're a good man.</td>\n",
       "      <td>[1639, 821, 257, 922, 582, 13, 50256]</td>\n",
       "      <td>Hey.</td>\n",
       "      <td>[10814, 13, 50256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>It's great. Drinking at work.</td>\n",
       "      <td>[[1026, 338, 1049, 13, 43963, 379, 670, 13, 50...</td>\n",
       "      <td>Basically, it is of Mad Men.</td>\n",
       "      <td>[[31524, 11, 340, 318, 286, 4627, 6065, 13, 50...</td>\n",
       "      <td>I'm not drinking at work.</td>\n",
       "      <td>[40, 1101, 407, 7722, 379, 670, 13, 50256]</td>\n",
       "      <td>I'm not drinking at work.</td>\n",
       "      <td>[40, 1101, 407, 7722, 379, 670, 13, 50256]</td>\n",
       "      <td>Barney, I didn't mean any personal attacks!</td>\n",
       "      <td>[10374, 1681, 11, 314, 1422, 470, 1612, 597, 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>522 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   ctx  \\\n",
       "0    I know, it's two years of my life I'm never ge...   \n",
       "1                     Wh-Where'd you get a meatball...   \n",
       "2    Okay, what is so urgent that you called me and...   \n",
       "3                                            How much?   \n",
       "4    You're being super nice. It's... freaking me o...   \n",
       "..                                                 ...   \n",
       "517  Okay, I want to lay down some ground rules for...   \n",
       "518                 It looks to be a... sacred... spa.   \n",
       "519                  That's putting it a bit strongly.   \n",
       "520                                              I do.   \n",
       "521                      It's great. Drinking at work.   \n",
       "\n",
       "                                                ctx_tk  \\\n",
       "0    [[40, 760, 11, 340, 338, 734, 812, 286, 616, 1...   \n",
       "1    [[1199, 12, 8496, 1549, 345, 651, 257, 6174, 1...   \n",
       "2    [[16454, 11, 644, 318, 523, 18039, 326, 345, 1...   \n",
       "3                             [[2437, 881, 30, 50256]]   \n",
       "4    [[1639, 821, 852, 2208, 3621, 13, 632, 338, 98...   \n",
       "..                                                 ...   \n",
       "517  [[16454, 11, 314, 765, 284, 3830, 866, 617, 23...   \n",
       "518  [[1026, 3073, 284, 307, 257, 986, 13626, 986, ...   \n",
       "519  [[2504, 338, 5137, 340, 257, 1643, 7634, 13, 5...   \n",
       "520                             [[40, 466, 13, 50256]]   \n",
       "521  [[1026, 338, 1049, 13, 43963, 379, 670, 13, 50...   \n",
       "\n",
       "                                                   lbl  \\\n",
       "0                                        Daddy's home.   \n",
       "1                              I don't have much time!   \n",
       "2    I could tell you knew something was up with me...   \n",
       "3                                            A little.   \n",
       "4    I'm being Barney, and I think tonight's going ...   \n",
       "..                                                 ...   \n",
       "517  Well, well, well. How rich. You make me promis...   \n",
       "518          Owl. How do we go? We will do what? Jump?   \n",
       "519           A bit strongly. She's not my girlfriend.   \n",
       "520  I'm gonna head out to a reggae concert. I'm a ...   \n",
       "521                       Basically, it is of Mad Men.   \n",
       "\n",
       "                                                lbl_tk  \\\n",
       "0                      [[48280, 338, 1363, 13, 50256]]   \n",
       "1            [[40, 836, 470, 423, 881, 640, 0, 50256]]   \n",
       "2    [[40, 714, 1560, 345, 2993, 1223, 373, 510, 35...   \n",
       "3                              [[32, 1310, 13, 50256]]   \n",
       "4    [[40, 1101, 852, 41921, 11, 290, 314, 892, 997...   \n",
       "..                                                 ...   \n",
       "517  [[5779, 11, 880, 11, 880, 13, 1374, 5527, 13, ...   \n",
       "518  [[46, 40989, 13, 1374, 466, 356, 467, 30, 775,...   \n",
       "519  [[32, 1643, 7634, 13, 1375, 338, 407, 616, 110...   \n",
       "520  [[40, 1101, 8066, 1182, 503, 284, 257, 842, 25...   \n",
       "521  [[31524, 11, 340, 318, 286, 4627, 6065, 13, 50...   \n",
       "\n",
       "                                            prd_greedy  \\\n",
       "0                                             Oh, God!   \n",
       "1             I don't know. I just saw a meatball sub.   \n",
       "2                       I'm sorry, I don't follow you.   \n",
       "3    I have not decided. I want to get married in a...   \n",
       "4                    I'm not gross. I'm just... gross.   \n",
       "..                                                 ...   \n",
       "517  Oh, I know. I just want to be as awesome as sh...   \n",
       "518                     I'm sorry, I don't follow you.   \n",
       "519        I'm not going to put it in a little strong.   \n",
       "520                                 You're a good man.   \n",
       "521                          I'm not drinking at work.   \n",
       "\n",
       "                                         prd_greedy_tk  \\\n",
       "0                           [5812, 11, 1793, 0, 50256]   \n",
       "1    [40, 836, 470, 760, 13, 314, 655, 2497, 257, 6...   \n",
       "2    [40, 1101, 7926, 11, 314, 836, 470, 1061, 345,...   \n",
       "3    [40, 423, 407, 3066, 13, 314, 765, 284, 651, 6...   \n",
       "4    [40, 1101, 407, 10319, 13, 314, 1101, 655, 986...   \n",
       "..                                                 ...   \n",
       "517  [5812, 11, 314, 760, 13, 314, 655, 765, 284, 3...   \n",
       "518  [40, 1101, 7926, 11, 314, 836, 470, 1061, 345,...   \n",
       "519  [40, 1101, 407, 1016, 284, 1234, 340, 287, 257...   \n",
       "520              [1639, 821, 257, 922, 582, 13, 50256]   \n",
       "521         [40, 1101, 407, 7722, 379, 670, 13, 50256]   \n",
       "\n",
       "                                            prd_nbeams  \\\n",
       "0                                             Oh, God!   \n",
       "1             I don't know. I just saw a meatball sub.   \n",
       "2                       I'm sorry, I don't follow you.   \n",
       "3    I have not decided. I want to get married in a...   \n",
       "4                    I'm not gross. I'm just... gross.   \n",
       "..                                                 ...   \n",
       "517  Oh, I know. I just want to be as awesome as sh...   \n",
       "518                     I'm sorry, I don't follow you.   \n",
       "519        I'm not going to put it in a little strong.   \n",
       "520                                 You're a good man.   \n",
       "521                          I'm not drinking at work.   \n",
       "\n",
       "                                         prd_nbeams_tk  \\\n",
       "0                           [5812, 11, 1793, 0, 50256]   \n",
       "1    [40, 836, 470, 760, 13, 314, 655, 2497, 257, 6...   \n",
       "2    [40, 1101, 7926, 11, 314, 836, 470, 1061, 345,...   \n",
       "3    [40, 423, 407, 3066, 13, 314, 765, 284, 651, 6...   \n",
       "4    [40, 1101, 407, 10319, 13, 314, 1101, 655, 986...   \n",
       "..                                                 ...   \n",
       "517  [5812, 11, 314, 760, 13, 314, 655, 765, 284, 3...   \n",
       "518  [40, 1101, 7926, 11, 314, 836, 470, 1061, 345,...   \n",
       "519  [40, 1101, 407, 1016, 284, 1234, 340, 287, 257...   \n",
       "520              [1639, 821, 257, 922, 582, 13, 50256]   \n",
       "521         [40, 1101, 407, 7722, 379, 670, 13, 50256]   \n",
       "\n",
       "                                    prd_sampling  \\\n",
       "0                                 Oh, of course.   \n",
       "1                            You don't remember?   \n",
       "2                 All right. It's time to start?   \n",
       "3            He said he had like 3,000 in suits.   \n",
       "4                            Buckminster Fuller?   \n",
       "..                                           ...   \n",
       "517                      No, I'll be right over.   \n",
       "518                           It's called a spa.   \n",
       "519                 And here comes the fun part.   \n",
       "520                                         Hey.   \n",
       "521  Barney, I didn't mean any personal attacks!   \n",
       "\n",
       "                                       prd_sampling_tk  \n",
       "0                     [5812, 11, 286, 1781, 13, 50256]  \n",
       "1                    [1639, 836, 470, 3505, 30, 50256]  \n",
       "2    [3237, 826, 13, 632, 338, 640, 284, 923, 30, 5...  \n",
       "3    [1544, 531, 339, 550, 588, 513, 11, 830, 287, ...  \n",
       "4                  [33, 1347, 18462, 31863, 30, 50256]  \n",
       "..                                                 ...  \n",
       "517    [2949, 11, 314, 1183, 307, 826, 625, 13, 50256]  \n",
       "518           [1026, 338, 1444, 257, 41900, 13, 50256]  \n",
       "519       [1870, 994, 2058, 262, 1257, 636, 13, 50256]  \n",
       "520                                 [10814, 13, 50256]  \n",
       "521  [10374, 1681, 11, 314, 1422, 470, 1612, 597, 2...  \n",
       "\n",
       "[522 rows x 10 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_char = get_dataframe_for_metrics(character_hg['test'], predictions_greedy, predictions_nbeams, predictions_sampling)\n",
    "df_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics For Character 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Lib.BBMetrics import BBMetric\n",
    "\n",
    "def compute_sample_metrics(context_sentence, label_response, chatbot_response, verbose=True, w=(1,1,1)):\n",
    "    scores = {}\n",
    "    if verbose:\n",
    "        # prints the sentences\n",
    "        print('* context:', context_sentence) \n",
    "        print('* label:  ', label_response)\n",
    "        print('* chatbot:', chatbot_response) \n",
    "    # 1) computes metrics for semantic similarity\n",
    "    metric = BBMetric.load_metric(\"semantic similarity\")\n",
    "    scores['semantic similarity'] = [metric.compute(sentences_a=context_sentence,\n",
    "                                                      sentences_b=label_response)['score']]\n",
    "    scores['semantic similarity'].append(metric.compute(sentences_a=context_sentence,\n",
    "                                                      sentences_b=chatbot_response)['score'])\n",
    "    scores['semantic similarity'].append(metric.compute(sentences_a=label_response,\n",
    "                                                      sentences_b=chatbot_response)['score'])\n",
    "    scores['semantic similarity'].append(sum(np.array(scores['semantic similarity']) * np.array(w)) / sum(w))\n",
    "    # ss_scores = scores['semantic similarity']\n",
    "    if verbose:\n",
    "        print('=== SEMANTIC SIMILARITY ===')\n",
    "        print('context-label similarity:   ', scores['semantic similarity'][0])\n",
    "        print('context-chatbot similarity: ', scores['semantic similarity'][1])\n",
    "        print('label-chatbot similarity:   ', scores['semantic similarity'][2])\n",
    "        print('> Merged Metrics')\n",
    "        print('  semantic similarity mean:     ',  scores['semantic similarity'][3])\n",
    "    # 2) computes metrics for bleu\n",
    "    metric = BBMetric.load_metric(\"bleu\")\n",
    "    scores['bleu'] = metric.compute(predictions=chatbot_response, references=label_response)['score']\n",
    "    if verbose:\n",
    "        print('===        BLEU         ===')\n",
    "        print('bleu:                       ', scores['bleu'])\n",
    "    # 3) computes metrics for rouge-L\n",
    "    metric = BBMetric.load_metric(\"rouge l\")\n",
    "    scores['rouge l'] = [metric.compute(predictions=context_sentence, references=label_response)['score']]\n",
    "    scores['rouge l'].append(metric.compute(predictions=context_sentence, references=chatbot_response)['score'])\n",
    "    scores['rouge l'].append(metric.compute(predictions=chatbot_response, references=label_response)['score'])\n",
    "    scores['rouge l'].append(sum(np.array(scores['rouge l']) * np.array(w)) / sum(w))\n",
    "    if verbose:\n",
    "        print('===       ROUGE-L       ===')\n",
    "        print('context-label rouge:        ', scores['rouge l'][0])\n",
    "        print('context-chatbot rouge:      ', scores['rouge l'][1])\n",
    "        print('label-chatbot rouge:        ', scores['rouge l'][2])\n",
    "        print('> Merged Metrics')\n",
    "        print('  rouge mean:                 ', scores['rouge l'][3] )\n",
    "    # 4) computes sas metric\n",
    "    metric = BBMetric.load_metric(\"semantic answer similarity\")\n",
    "    scores['semantic answer similarity'] = [metric.compute(predictions=context_sentence,\n",
    "                                                    references=label_response)['score']]\n",
    "    scores['semantic answer similarity'].append(metric.compute(predictions=context_sentence,\n",
    "                                                        references=chatbot_response)['score'])\n",
    "    scores['semantic answer similarity'].append(metric.compute(predictions=label_response,\n",
    "                                                        references=chatbot_response)['score'])\n",
    "    scores['semantic answer similarity'].append(sum(np.array(scores['semantic answer similarity']) * np.array(w)) / sum(w))\n",
    "    if verbose:\n",
    "        print('===         SAS         ===')\n",
    "        print('context-label sas:          ', scores['semantic answer similarity'][0])\n",
    "        print('context-chatbot sas:        ', scores['semantic answer similarity'][1])\n",
    "        print('label-chatbot sas:          ', scores['semantic answer similarity'][2])\n",
    "        print('> Merged Metrics')\n",
    "        print('  sas mean:                   ',  scores['semantic answer similarity'][3])\n",
    "    # 5) computes emotion metric\n",
    "    metric = BBMetric.load_metric(\"emotion\")\n",
    "    scores['emotion'] = [metric.compute(sentences=context_sentence)]\n",
    "    scores['emotion'].append(metric.compute(sentences=label_response))\n",
    "    scores['emotion'].append(metric.compute(sentences=chatbot_response))\n",
    "    if verbose:\n",
    "        print('===       EMOTION       ===')\n",
    "        print('context emotions:            \\n', list(zip(scores['emotion'][0]['label'], scores['emotion'][0]['score'])))\n",
    "        print('label emotions:              \\n', list(zip(scores['emotion'][1]['label'], scores['emotion'][1]['score'])))\n",
    "        print('chatbot emotions:            \\n', list(zip(scores['emotion'][2]['label'], scores['emotion'][2]['score'])))\n",
    "        print('label-chatbot emotion corr:  \\n', sp.stats.stats.pearsonr(scores['emotion'][1]['score'],\n",
    "                                                                         scores['emotion'][2]['score']))\n",
    "\n",
    "    # 6) computes metrics for distinct\n",
    "    metric = BBMetric.load_metric(\"distinct\")\n",
    "    scores['distinct'] = metric.compute(sentences=chatbot_response)['score']\n",
    "    if verbose:\n",
    "        print('===       DISTINCT      ===')\n",
    "        print('distinct:                   ', scores['distinct'])\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_set_metrics(model, tokenizer, context_sentences, label_responses, chatbot_responses, verbose=True, w=(1,1,1),\n",
    "                        classifier_n_sentences=50, include_qualitative=False):\n",
    "    scores = {}\n",
    "    \n",
    "    # 0) computes metrics for perplexity\n",
    "    metric = BBMetric.load_metric(\"semantic similarity\")\n",
    "    scores['semantic similarity'] = [metric.compute(sentences_a=context_sentences,\n",
    "                                                      sentences_b=label_responses)['score']]\n",
    "    scores['semantic similarity'].append(metric.compute(sentences_a=context_sentences,\n",
    "                                                      sentences_b=chatbot_responses)['score'])\n",
    "    scores['semantic similarity'].append(metric.compute(sentences_a=label_responses,\n",
    "                                                      sentences_b=chatbot_responses)['score'])\n",
    "    scores['semantic similarity'].append(sum(np.array(scores['semantic similarity']) * np.array(w)) / sum(w))\n",
    "    if verbose:\n",
    "        print('=== SEMANTIC SIMILARITY ===')\n",
    "        print('semantic similarity:        ', scores['semantic similarity'])\n",
    "    # 1) computes metrics for perplexity\n",
    "    metric = BBMetric.load_metric(\"perplexity\")\n",
    "    if verbose:\n",
    "        print('===       PERPLEXITY     ===')\n",
    "    scores['perplexity'] = metric.compute(model=model, tokenizer=tokenizer, sentences=chatbot_responses)['score_concat']\n",
    "    if verbose:\n",
    "        print('perplexity:                 ', scores['perplexity'])\n",
    "    # 2) computes metrics for bleu\n",
    "    metric = BBMetric.load_metric(\"bleu\")\n",
    "    scores['bleu'] = metric.compute(predictions=chatbot_responses, references=label_responses)['score']\n",
    "    if verbose:\n",
    "        print('===         BLEU         ===')\n",
    "        print('bleu:                       ', scores['bleu'])\n",
    "    # 3) computes metrics for rouge-L\n",
    "    metric = BBMetric.load_metric(\"rouge l\")\n",
    "    scores['rouge l'] = metric.compute(predictions=chatbot_responses, references=label_responses)['score']\n",
    "    if verbose:\n",
    "        print('===        ROUGE-L       ===')\n",
    "        print('rouge:                      ', scores['rouge l'])\n",
    "    # 4) computes metrics for distinct\n",
    "    metric = BBMetric.load_metric(\"distinct\")\n",
    "    scores['distinct'] = metric.compute(sentences=chatbot_responses)['score']\n",
    "    if verbose:\n",
    "        print('===        DISTINCT      ===')\n",
    "        print('distinct:                   ', scores['distinct'])\n",
    "    # 6) computes emotion metric\n",
    "    metric = BBMetric.load_metric(\"emotion\")\n",
    "    scores['emotion'] = [metric.compute(sentences=chatbot_responses)]\n",
    "    scores['emotion'].append(metric.compute(sentences=label_responses))\n",
    "    if verbose:\n",
    "        print('===        EMOTION       ===')\n",
    "        print('chatbot emotions:            \\n', list(zip(scores['emotion'][0]['label'], scores['emotion'][0]['score'])))\n",
    "        print('label emotions:              \\n', list(zip(scores['emotion'][1]['label'], scores['emotion'][1]['score'])))\n",
    "        print('label-chatbot emotion corr:  \\n', sp.stats.stats.pearsonr(scores['emotion'][0]['score'],\n",
    "                                                                         scores['emotion'][1]['score']))\n",
    "    # 8) computes sas metric\n",
    "    metric = BBMetric.load_metric(\"semantic answer similarity\")\n",
    "    scores['semantic answer similarity'] = [metric.compute(predictions=context_sentences,\n",
    "                                                    references=label_responses)['score']]\n",
    "    scores['semantic answer similarity'].append(metric.compute(predictions=context_sentences,\n",
    "                                                        references=chatbot_responses)['score'])\n",
    "    scores['semantic answer similarity'].append(metric.compute(predictions=label_responses,\n",
    "                                                        references=chatbot_responses)['score'])\n",
    "    scores['semantic answer similarity'].append(sum(np.array(scores['semantic answer similarity']) * np.array(w)) / sum(w))\n",
    "    if verbose:\n",
    "        print('===         SAS         ===')\n",
    "        print('context-label sas:          ', scores['semantic answer similarity'][0])\n",
    "        print('context-chatbot sas:        ', scores['semantic answer similarity'][1])\n",
    "        print('label-chatbot sas:          ', scores['semantic answer similarity'][2])\n",
    "        print('> Merged Metrics')\n",
    "        print('sas-mean:                   ',  scores['semantic answer similarity'][3])\n",
    "    # 9) computes metrics for semantic classifyer\n",
    "    metric = BBMetric.load_metric(\"semantic classifier\")\n",
    "    start_time = time.time()\n",
    "    scores['semantic classifier'] = [metric.compute(character=character, character_dict=character_dict, \n",
    "                                                   base_folder=base_folder, sentences=chatbot_responses,\n",
    "                                                   n_sentences=classifier_n_sentences)]\n",
    "    scores['semantic classifier'].append(metric.compute(character=character, character_dict=character_dict, \n",
    "                                                   base_folder=base_folder, sentences=label_responses,\n",
    "                                                   n_sentences=classifier_n_sentences))\n",
    "    end_time = time.time()\n",
    "    if verbose:\n",
    "        print('=== SEMANTIC CLASSIFIER ===')\n",
    "        print('semantic classifier chatbot:                ', scores['semantic classifier'][0])\n",
    "        print('semantic classifier label:                  ', scores['semantic classifier'][1])\n",
    "        print('time elapsed computing semantic classifier:  {:.2f} s'.format(end_time - start_time))\n",
    "        \n",
    "    if include_qualitative:\n",
    "        # Do stuff with human metrics and print sentences\n",
    "        pass\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    print(\"##### Sample \" + str(i+1) + \" #####\")\n",
    "    context_sentence = df_char['ctx'][i]\n",
    "    chatbot_response = df_char['prd_greedy'][i]\n",
    "    label_response   = df_char['lbl'][i]\n",
    "    compute_sample_metrics(context_sentence, label_response, chatbot_response)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_size = 50\n",
    "i = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"##### Set (Size \" + str(set_size) + \") #####\")\n",
    "context_sentences = list(df_char['ctx'][i:i+set_size])\n",
    "chatbot_responses = list(df_char['prd_greedy'][i:i+set_size])\n",
    "label_responses   = list(df_char['lbl'][i:i+set_size])\n",
    "scores = compute_set_metrics(model, tokenizer,\n",
    "                    context_sentences, label_responses, chatbot_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Full Test Set #####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (7597 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SEMANTIC SIMILARITY ===\n",
      "semantic similarity:         [0.09542313, 0.22581792, 0.09963352, 0.14029152443011603]\n",
      "===       PERPLEXITY     ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 119/119 [00:12<00:00,  9.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity:                  398.63258558262714\n",
      "===         BLEU         ===\n",
      "bleu:                        0.0\n",
      "===        ROUGE-L       ===\n",
      "rouge:                       0.07784712634918524\n",
      "===        DISTINCT      ===\n",
      "distinct:                    0.10609917782254247\n",
      "===        EMOTION       ===\n",
      "chatbot emotions:            \n",
      " [('sadness', 0.2896285032952671), ('joy', 0.271495469938105), ('love', 0.023309589239469394), ('anger', 0.34172616484604523), ('fear', 0.06777737714981699), ('surprise', 0.0060628916286927835)]\n",
      "label emotions:              \n",
      " [('sadness', 0.08338458265166866), ('joy', 0.38902365875618744), ('love', 0.024806545307496035), ('anger', 0.40497622549917656), ('fear', 0.08848321566827828), ('surprise', 0.00932577022948622)]\n",
      "label-chatbot emotion corr:  \n",
      " (0.794296557894927, 0.05911880107460219)\n",
      "===         SAS         ===\n",
      "context-label sas:           0.29280096\n",
      "context-chatbot sas:         0.36482108\n",
      "label-chatbot sas:           0.25272134\n",
      "> Merged Metrics\n",
      "sas-mean:                    0.30344779292742413\n",
      "=== SEMANTIC CLASSIFIER ===\n",
      "semantic classifier chatbot:                 {'score': 0.44610128, 'std': 0.46138492}\n",
      "semantic classifier label:                   {'score': 0.9950678, 'std': 0.054852862}\n",
      "time elapsed computing semantic classifier:  27.69 s\n"
     ]
    }
   ],
   "source": [
    "print(\"##### Full Test Set #####\")\n",
    "context_sentences = list(df_char['ctx'])\n",
    "chatbot_responses = list(df_char['prd_greedy'])\n",
    "label_responses   = list(df_char['lbl'])\n",
    "scores = compute_set_metrics(model, tokenizer,\n",
    "                    context_sentences, \n",
    "                    label_responses, \n",
    "                    chatbot_responses,\n",
    "                    classifier_n_sentences=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Between Character 1 & Character 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_small(sample_questions, model, generation_method):\n",
    "    print(\"Creating predictions\")\n",
    "    predictions = list()\n",
    "    for x in tqdm(sample_questions):\n",
    "        tokenized_question = tokenizer.encode(x + tokenizer.eos_token, return_tensors='tf')\n",
    "        max_length = 128 + tokenized_question.shape[1]\n",
    "        if generation_method == \"Greedy\":\n",
    "            generated_answer = model.generate(tokenized_question,\n",
    "                                pad_token_id=tokenizer.eos_token_id, max_length=max_length)[0].numpy().tolist()\n",
    "        elif generation_method == \"Beam Search\":\n",
    "            generated_answer = model.generate(tokenized_question,\n",
    "                                         pad_token_id=tokenizer.eos_token_id, max_length=max_length,\n",
    "                                         n_beams=n_beams)[0].numpy().tolist()\n",
    "        elif generation_method == \"Sampling\":\n",
    "            generated_answer = model.generate(tokenized_question,\n",
    "                                         pad_token_id=tokenizer.eos_token_id, max_length=max_length,\n",
    "                                         do_sample=True, top_k=top_k, top_p=top_p)[0].numpy().tolist()\n",
    "        predictions.append(generated_answer[len(tokenized_question[0]):])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_common = load_dataset('csv',\n",
    "                         data_files=os.path.join(base_folder, 'Data', 'common_dataset.csv'), \n",
    "                         cache_dir=os.path.join(base_folder, \"cache\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_1_sampling = get_predictions_small(df_common['train']['context'], model, \"Sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_2_sampling = get_predictions_small(df_common['train']['context'], model_2, \"Sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_common_char_1 = get_dataframe_for_metrics(df_common['train'], None, None, predictions_1_sampling)\n",
    "df_common_char_2 = get_dataframe_for_metrics(df_common['train'], None, None, predictions_2_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"##### \" + character + \"  Vs. \" + character_2 + \" #####\")\n",
    "context_sentences   = list(df_common_char_1['ctx'])\n",
    "chatbot_responses   = list(df_common_char_1['prd_sampling'])\n",
    "chatbot_2_responses = list(df_common_char_2['prd_sampling'])\n",
    "compute_set_metrics(model, tokenizer,\n",
    "                    context_sentences, chatbot_2_responses, chatbot_responses, include_qualitative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Between Different Sampling Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"##### Greedy vs. N-Beams #####\")\n",
    "context_sentences = list(df_char['ctx'])\n",
    "greedy_responses  = list(df_char['prd_greedy'])\n",
    "nbeams_responses  = list(df_char['prd_nbeams'])\n",
    "compute_set_metrics(model, tokenizer,\n",
    "                    context_sentences,\n",
    "                    greedy_responses,\n",
    "                    nbeams_responses,\n",
    "                    classifier_n_sentences=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"##### Greedy vs. Sampling #####\")\n",
    "context_sentences   = list(df_char['ctx'])\n",
    "greedy_responses    = list(df_char['prd_greedy'])\n",
    "sampling_responses  = list(df_char['prd_sampling'])\n",
    "compute_set_metrics(model, tokenizer,\n",
    "                    context_sentences,\n",
    "                    greedy_responses,\n",
    "                    sampling_responses,\n",
    "                    classifier_n_sentences=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"##### N-Beams vs. Sampling #####\")\n",
    "context_sentences   = list(df_char['ctx'])\n",
    "nbeams_responses    = list(df_char['prd_nbeams'])\n",
    "sampling_responses  = list(df_char['prd_sampling'])\n",
    "compute_set_metrics(model, tokenizer,\n",
    "                    context_sentences,\n",
    "                    nbeams_responses,\n",
    "                    sampling_responses,\n",
    "                    classifier_n_sentences=75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Between Non-Finetuned And Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_def_sampling = get_predictions_cached(sample_questions, model_def,\n",
    "                                                  os.path.join(in_folder_def, 'from_' + character + '_df_' + '_sampling.json'),\n",
    "                                                  \"Sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char_def = get_dataframe_for_metrics(character_hg['test'], None, None, predictions_def_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    print(\"##### Sample \" + str(i+1) + \" #####\")\n",
    "    context_sentence   = df_char['ctx'][i]\n",
    "    character_response = df_char['prd_sampling'][i]\n",
    "    default_response   = df_char_def['prd_sampling'][i]\n",
    "    compute_sample_metrics(context_sentence, default_response, character_response)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_size = 50\n",
    "i = 30\n",
    "print(\"##### Set (Size \" + str(set_size) + \") #####\")\n",
    "context_sentences   = list(df_char['ctx'][i:i+set_size])\n",
    "character_responses = list(df_char['prd_sampling'][i:i+set_size])\n",
    "default_responses   = list(df_char_def['prd_sampling'][i:i+set_size])\n",
    "compute_set_metrics(model, tokenizer,\n",
    "                    context_sentences, default_responses, character_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"##### Full Test Set (Size \" + str(set_size) + \") #####\")\n",
    "context_sentences   = list(df_char['ctx'])\n",
    "character_responses = list(df_char['prd_sampling'])\n",
    "default_responses   = list(df_char_def['prd_sampling'])\n",
    "compute_set_metrics(model, tokenizer,\n",
    "                    context_sentences, \n",
    "                    default_responses, \n",
    "                    character_responses,\n",
    "                    classifier_n_sentences=75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

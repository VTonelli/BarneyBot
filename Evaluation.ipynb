{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data.data_dicts import character_dict, source_dict, random_state\n",
    "\n",
    "model_name = 'microsoft/DialoGPT-small'\n",
    "character = 'Barney' # 'Barney' | 'Sheldon' | 'Harry' | 'Fry' | 'Vader' | 'Joey' | 'Phoebe' | 'Bender' | Default'\n",
    "character_2 = 'Sheldon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount google drive\n",
    "import os\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive',force_remount=True)\n",
    "    base_folder = '/content/drive/My Drive/unibo/NLP_project/BarneyBot'\n",
    "    os.system(\"pip install datasets\")\n",
    "    os.system(\"pip install transformers\")\n",
    "    os.system(\"pip install rouge_score\")\n",
    "    os.system(\"pip install -U sentence-transformers\")\n",
    "else:\n",
    "    base_folder = os.getcwd()\n",
    "    \n",
    "in_folder = os.path.join(base_folder, 'Data', 'Characters', character)\n",
    "if not os.path.exists(in_folder):\n",
    "    os.makedirs(in_folder)\n",
    "out_folder = os.path.join(base_folder, 'Data', 'Characters', character)\n",
    "if not os.path.exists(out_folder):\n",
    "    os.makedirs(out_folder)\n",
    "    \n",
    "in_folder_2 = os.path.join(base_folder, 'Data', 'Characters', character_2)\n",
    "if not os.path.exists(in_folder_2):\n",
    "    os.makedirs(in_folder_2)\n",
    "out_folder_2 = os.path.join(base_folder, 'Data', 'Characters', character_2)\n",
    "if not os.path.exists(out_folder_2):\n",
    "    os.makedirs(out_folder_2)\n",
    "    \n",
    "in_folder_def = os.path.join(base_folder, 'Data', 'Characters', 'Default')\n",
    "if not os.path.exists(in_folder_def):\n",
    "    os.makedirs(in_folder_def)\n",
    "out_folder_def = os.path.join(base_folder, 'Data', 'Characters', 'Default')\n",
    "if not os.path.exists(out_folder_def):\n",
    "    os.makedirs(out_folder_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "def load_df(character):\n",
    "    dataset_path = os.path.join(base_folder, \"Data\", \"Characters\", character, character+'.csv')\n",
    "    \n",
    "    character_hg = load_dataset('csv', \n",
    "                                data_files=dataset_path, \n",
    "                                cache_dir=os.path.join(base_folder, \"cache\"))\n",
    "    \n",
    "    # 85% train / 10% test / 5% validation\n",
    "    train_test_hg = character_hg['train'].train_test_split(test_size=0.15, seed=random_state)\n",
    "    test_val = train_test_hg['test'].train_test_split(test_size=0.33, seed=random_state)\n",
    "    \n",
    "    \n",
    "    character_hg = DatasetDict({\n",
    "        'train': train_test_hg['train'],\n",
    "        'test': test_val['train'],\n",
    "        'val': test_val['test']\n",
    "    })\n",
    "    \n",
    "    return character_hg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_conv(row, tokenizer):\n",
    "    MAX_LENGTH = 512\n",
    "    row = list(reversed(list(row.values())))\n",
    "    model_inputs = tokenizer(row)\n",
    "    tokenizer_pad_token_id = tokenizer.encode('#')[0]\n",
    "    for i in range(len(model_inputs['input_ids'])):\n",
    "        model_inputs['input_ids'][i].append(tokenizer.eos_token_id)\n",
    "        model_inputs['attention_mask'][i].append(1)\n",
    "    model_inputs['input_ids'] = [item for sublist in model_inputs['input_ids'] for item in sublist]\n",
    "    model_inputs['attention_mask'] = [item for sublist in model_inputs['attention_mask'] for item in sublist]\n",
    "    if MAX_LENGTH > len(model_inputs['input_ids']):\n",
    "        model_inputs['input_ids'] += [tokenizer_pad_token_id] * (MAX_LENGTH - len(model_inputs['input_ids']))\n",
    "        model_inputs['attention_mask'] += [0] * (MAX_LENGTH - len(model_inputs['attention_mask']))\n",
    "    elif MAX_LENGTH < len(model_inputs['input_ids']):\n",
    "        model_inputs['input_ids'] = model_inputs['input_ids'][:MAX_LENGTH-1]\n",
    "        model_inputs['input_ids'][-1] = tokenizer.eos_token_id\n",
    "        model_inputs['attention_mask'] = model_inputs['attention_mask'][:MAX_LENGTH-1]\n",
    "        model_inputs['attention_mask'][-1] = 1\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    tokenizer.pad_token = '#'\n",
    "    model_inputs = construct_conv(examples, tokenizer)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-0c3b759885fd8d5f\n",
      "Reusing dataset csv (D:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBot\\BarneyBot\\cache\\csv\\default-0c3b759885fd8d5f\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb0d6674103146fb8cd9439930557b01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at D:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBot\\BarneyBot\\cache\\csv\\default-0c3b759885fd8d5f\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-c79122a57f55334a.arrow and D:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBot\\BarneyBot\\cache\\csv\\default-0c3b759885fd8d5f\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-4faecb1c641ab4b9.arrow\n",
      "Loading cached split indices for dataset at D:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBot\\BarneyBot\\cache\\csv\\default-0c3b759885fd8d5f\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-7b672756e3cf7bb2.arrow and D:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBot\\BarneyBot\\cache\\csv\\default-0c3b759885fd8d5f\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-bb39fb19125a9103.arrow\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"HF_DATASETS_CACHE\"] = os.path.join(base_folder, \"cache\")\n",
    "character_hg = load_df(character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_folder = os.path.join(out_folder, character_dict[character]['checkpoint_folder'])\n",
    "checkpoint_folder_2 = os.path.join(out_folder_2, character_dict[character_2]['checkpoint_folder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at D:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBot\\BarneyBot\\Data\\Characters\\Barney\\barney_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=os.path.join(base_folder, \"cache\"))\n",
    "tokenizer.pad_token = '#'\n",
    "\n",
    "model = TFAutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=checkpoint_folder)\n",
    "model_2 = TFAutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=checkpoint_folder_2)\n",
    "model_def = TFAutoModelForCausalLM.from_pretrained(model_name, cache_dir=os.path.join(base_folder, \"cache\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_questions = character_hg['test']['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_beams = 3\n",
    "top_k = 50\n",
    "top_p = 0.92\n",
    "\n",
    "def get_predictions_cached(sample_questions, model, filepath, generation_method):\n",
    "    prediction_path = os.path.join(in_folder, filename)\n",
    "    if os.path.exists(prediction_path) and not override_predictions:\n",
    "        print(\"Loading predictions from stored file\")\n",
    "        with open(filepath, 'r') as file:\n",
    "            json_string = file.read()\n",
    "        predictions = json.loads(json_string)\n",
    "        print(\"Loaded predictions from stored file\")\n",
    "\n",
    "    else:\n",
    "        print(\"Creating predictions\")\n",
    "        predictions = list()\n",
    "        for x in tqdm(sample_questions):\n",
    "            tokenized_question = tokenizer.encode(x + tokenizer.eos_token, return_tensors='tf')\n",
    "            max_length = 128 + tokenized_question.shape[1]\n",
    "            if generation_method == \"Greedy\":\n",
    "                generated_answer = model.generate(tokenized_question,\n",
    "                                    pad_token_id=tokenizer.eos_token_id, max_length=max_length)[0].numpy().tolist()\n",
    "            elif generation_method == \"Beam Search\":\n",
    "                generated_answer = model.generate(tokenized_question,\n",
    "                                             pad_token_id=tokenizer.eos_token_id, max_length=max_length,\n",
    "                                             n_beams=n_beams)[0].numpy().tolist()\n",
    "            elif generation_method == \"Sampling\":\n",
    "                generated_answer = model.generate(tokenized_question,\n",
    "                                             pad_token_id=tokenizer.eos_token_id, max_length=max_length,\n",
    "                                             do_sample=True, top_k=top_k, top_p=top_p)[0].numpy().tolist()\n",
    "            predictions.append(generated_answer[len(tokenized_question[0]):])\n",
    "\n",
    "        # Save predictions as a JSON file\n",
    "        output_string = json.dumps(predictions)\n",
    "        with open(filepath, 'w') as file:\n",
    "            file.write(output_string)\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading predictions from stored file\n",
      "Loaded predictions from stored file\n",
      "Loading predictions from stored file\n",
      "Loaded predictions from stored file\n",
      "Loading predictions from stored file\n",
      "Loaded predictions from stored file\n"
     ]
    }
   ],
   "source": [
    "predictions_greedy = get_predictions_cached(sample_questions, model,\n",
    "                                            os.path.join(in_folder, \n",
    "                                                         character_dict[character]['prediction_filename'] + '_greedy.json'),\n",
    "                                            \"Greedy\")\n",
    "predictions_nbeams = get_predictions_cached(sample_questions, model,\n",
    "                                            os.path.join(in_folder, \n",
    "                                                         character_dict[character]['prediction_filename'] + '_nbeams.json'),\n",
    "                                            \"Beam Search\")\n",
    "predictions_sampling = get_predictions_cached(sample_questions, model,\n",
    "                                              os.path.join(in_folder,\n",
    "                                                           character_dict[character]['prediction_filename'] + '_sampling.json'),\n",
    "                                              \"Sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe_for_metrics(data_test, predictions_greedy, predictions_nbeams, predictions_sampling):\n",
    "    i = 0\n",
    "    df = {'ctx':[], 'ctx_tk':[]}\n",
    "    has_labels = 'response' in df.columns\n",
    "    if has_labels:\n",
    "        df['lbl'] = []\n",
    "        df['lbl_tk'] = []\n",
    "    if predictions_greedy:\n",
    "        df['prd_greedy'] = []\n",
    "        df['prd_greedy_tk'] = []\n",
    "    if predictions_nbeams:\n",
    "        df['prd_nbeams'] = []\n",
    "        df['prd_nbeams_tk'] = [] \n",
    "    if predictions_sampling:\n",
    "        df['prd_sampling'] = []\n",
    "        df['prd_sampling_tk'] = []\n",
    "    for sample in tqdm(data_test):\n",
    "        # encode the context and label sentences, add the eos_token and return a tensor\n",
    "        ctx_tk = tokenizer.encode(sample['context'] + tokenizer.eos_token, return_tensors='tf').numpy().tolist()\n",
    "        ctx = sample['context']\n",
    "        df['ctx_tk'].append(ctx_tk)\n",
    "        df['ctx'].append(ctx)\n",
    "        if has_labels:\n",
    "            lbl_tk = tokenizer.encode(sample['response'] + tokenizer.eos_token, return_tensors='tf').numpy().tolist()\n",
    "            lbl = sample['response']\n",
    "            df['lbl'].append(lbl)\n",
    "            df['lbl_tk'].append(lbl_tk)\n",
    "        if predictions_greedy:\n",
    "            prd_greedy_tk = predictions_greedy[i]\n",
    "            prd_greedy = tokenizer.decode(prd_greedy_tk, skip_special_tokens=True)\n",
    "            df['prd_greedy'].append(prd_greedy)\n",
    "            df['prd_greedy_tk'].append(prd_greedy_tk)\n",
    "        if predictions_nbeams:\n",
    "            prd_nbeams_tk = predictions_nbeams[i]\n",
    "            prd_nbeams = tokenizer.decode(prd_nbeams_tk, skip_special_tokens=True)\n",
    "            df['prd_nbeams'].append(prd_nbeams)\n",
    "            df['prd_nbeams_tk'].append(prd_nbeams_tk)\n",
    "        if predictions_sampling:\n",
    "            prd_sampling_tk = predictions_sampling[i]\n",
    "            prd_sampling = tokenizer.decode(prd_sampling_tk, skip_special_tokens=True)\n",
    "            df['prd_sampling'].append(prd_sampling)\n",
    "            df['prd_sampling_tk'].append(prd_sampling_tk)\n",
    "        i += 1\n",
    "    return pd.DataFrame(data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 522/522 [00:00<00:00, 626.65it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ctx</th>\n",
       "      <th>prd_greedy</th>\n",
       "      <th>prd_nbeams</th>\n",
       "      <th>prd_sampling</th>\n",
       "      <th>lbl</th>\n",
       "      <th>ctx_tk</th>\n",
       "      <th>prd_greedy_tk</th>\n",
       "      <th>prd_nbeams_tk</th>\n",
       "      <th>prd_sampling_tk</th>\n",
       "      <th>lbl_tk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I know, it's two years of my life I'm never ge...</td>\n",
       "      <td>Oh, God!</td>\n",
       "      <td>Oh, God!</td>\n",
       "      <td>Oh, of course.</td>\n",
       "      <td>Daddy's home.</td>\n",
       "      <td>[[40, 760, 11, 340, 338, 734, 812, 286, 616, 1...</td>\n",
       "      <td>[5812, 11, 1793, 0, 50256]</td>\n",
       "      <td>[5812, 11, 1793, 0, 50256]</td>\n",
       "      <td>[5812, 11, 286, 1781, 13, 50256]</td>\n",
       "      <td>[[48280, 338, 1363, 13, 50256]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wh-Where'd you get a meatball...</td>\n",
       "      <td>I don't know. I just saw a meatball sub.</td>\n",
       "      <td>I don't know. I just saw a meatball sub.</td>\n",
       "      <td>You don't remember?</td>\n",
       "      <td>I don't have much time!</td>\n",
       "      <td>[[1199, 12, 8496, 1549, 345, 651, 257, 6174, 1...</td>\n",
       "      <td>[40, 836, 470, 760, 13, 314, 655, 2497, 257, 6...</td>\n",
       "      <td>[40, 836, 470, 760, 13, 314, 655, 2497, 257, 6...</td>\n",
       "      <td>[1639, 836, 470, 3505, 30, 50256]</td>\n",
       "      <td>[[40, 836, 470, 423, 881, 640, 0, 50256]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Okay, what is so urgent that you called me and...</td>\n",
       "      <td>I'm sorry, I don't follow you.</td>\n",
       "      <td>I'm sorry, I don't follow you.</td>\n",
       "      <td>All right. It's time to start?</td>\n",
       "      <td>I could tell you knew something was up with me...</td>\n",
       "      <td>[[16454, 11, 644, 318, 523, 18039, 326, 345, 1...</td>\n",
       "      <td>[40, 1101, 7926, 11, 314, 836, 470, 1061, 345,...</td>\n",
       "      <td>[40, 1101, 7926, 11, 314, 836, 470, 1061, 345,...</td>\n",
       "      <td>[3237, 826, 13, 632, 338, 640, 284, 923, 30, 5...</td>\n",
       "      <td>[[40, 714, 1560, 345, 2993, 1223, 373, 510, 35...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How much?</td>\n",
       "      <td>I have not decided. I want to get married in a...</td>\n",
       "      <td>I have not decided. I want to get married in a...</td>\n",
       "      <td>He said he had like 3,000 in suits.</td>\n",
       "      <td>A little.</td>\n",
       "      <td>[[2437, 881, 30, 50256]]</td>\n",
       "      <td>[40, 423, 407, 3066, 13, 314, 765, 284, 651, 6...</td>\n",
       "      <td>[40, 423, 407, 3066, 13, 314, 765, 284, 651, 6...</td>\n",
       "      <td>[1544, 531, 339, 550, 588, 513, 11, 830, 287, ...</td>\n",
       "      <td>[[32, 1310, 13, 50256]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You're being super nice. It's... freaking me o...</td>\n",
       "      <td>I'm not gross. I'm just... gross.</td>\n",
       "      <td>I'm not gross. I'm just... gross.</td>\n",
       "      <td>Buckminster Fuller?</td>\n",
       "      <td>I'm being Barney, and I think tonight's going ...</td>\n",
       "      <td>[[1639, 821, 852, 2208, 3621, 13, 632, 338, 98...</td>\n",
       "      <td>[40, 1101, 407, 10319, 13, 314, 1101, 655, 986...</td>\n",
       "      <td>[40, 1101, 407, 10319, 13, 314, 1101, 655, 986...</td>\n",
       "      <td>[33, 1347, 18462, 31863, 30, 50256]</td>\n",
       "      <td>[[40, 1101, 852, 41921, 11, 290, 314, 892, 997...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>Okay, I want to lay down some ground rules for...</td>\n",
       "      <td>Oh, I know. I just want to be as awesome as sh...</td>\n",
       "      <td>Oh, I know. I just want to be as awesome as sh...</td>\n",
       "      <td>No, I'll be right over.</td>\n",
       "      <td>Well, well, well. How rich. You make me promis...</td>\n",
       "      <td>[[16454, 11, 314, 765, 284, 3830, 866, 617, 23...</td>\n",
       "      <td>[5812, 11, 314, 760, 13, 314, 655, 765, 284, 3...</td>\n",
       "      <td>[5812, 11, 314, 760, 13, 314, 655, 765, 284, 3...</td>\n",
       "      <td>[2949, 11, 314, 1183, 307, 826, 625, 13, 50256]</td>\n",
       "      <td>[[5779, 11, 880, 11, 880, 13, 1374, 5527, 13, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>It looks to be a... sacred... spa.</td>\n",
       "      <td>I'm sorry, I don't follow you.</td>\n",
       "      <td>I'm sorry, I don't follow you.</td>\n",
       "      <td>It's called a spa.</td>\n",
       "      <td>Owl. How do we go? We will do what? Jump?</td>\n",
       "      <td>[[1026, 3073, 284, 307, 257, 986, 13626, 986, ...</td>\n",
       "      <td>[40, 1101, 7926, 11, 314, 836, 470, 1061, 345,...</td>\n",
       "      <td>[40, 1101, 7926, 11, 314, 836, 470, 1061, 345,...</td>\n",
       "      <td>[1026, 338, 1444, 257, 41900, 13, 50256]</td>\n",
       "      <td>[[46, 40989, 13, 1374, 466, 356, 467, 30, 775,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>That's putting it a bit strongly.</td>\n",
       "      <td>I'm not going to put it in a little strong.</td>\n",
       "      <td>I'm not going to put it in a little strong.</td>\n",
       "      <td>And here comes the fun part.</td>\n",
       "      <td>A bit strongly. She's not my girlfriend.</td>\n",
       "      <td>[[2504, 338, 5137, 340, 257, 1643, 7634, 13, 5...</td>\n",
       "      <td>[40, 1101, 407, 1016, 284, 1234, 340, 287, 257...</td>\n",
       "      <td>[40, 1101, 407, 1016, 284, 1234, 340, 287, 257...</td>\n",
       "      <td>[1870, 994, 2058, 262, 1257, 636, 13, 50256]</td>\n",
       "      <td>[[32, 1643, 7634, 13, 1375, 338, 407, 616, 110...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>I do.</td>\n",
       "      <td>You're a good man.</td>\n",
       "      <td>You're a good man.</td>\n",
       "      <td>Hey.</td>\n",
       "      <td>I'm gonna head out to a reggae concert. I'm a ...</td>\n",
       "      <td>[[40, 466, 13, 50256]]</td>\n",
       "      <td>[1639, 821, 257, 922, 582, 13, 50256]</td>\n",
       "      <td>[1639, 821, 257, 922, 582, 13, 50256]</td>\n",
       "      <td>[10814, 13, 50256]</td>\n",
       "      <td>[[40, 1101, 8066, 1182, 503, 284, 257, 842, 25...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>It's great. Drinking at work.</td>\n",
       "      <td>I'm not drinking at work.</td>\n",
       "      <td>I'm not drinking at work.</td>\n",
       "      <td>Barney, I didn't mean any personal attacks!</td>\n",
       "      <td>Basically, it is of Mad Men.</td>\n",
       "      <td>[[1026, 338, 1049, 13, 43963, 379, 670, 13, 50...</td>\n",
       "      <td>[40, 1101, 407, 7722, 379, 670, 13, 50256]</td>\n",
       "      <td>[40, 1101, 407, 7722, 379, 670, 13, 50256]</td>\n",
       "      <td>[10374, 1681, 11, 314, 1422, 470, 1612, 597, 2...</td>\n",
       "      <td>[[31524, 11, 340, 318, 286, 4627, 6065, 13, 50...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>522 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   ctx  \\\n",
       "0    I know, it's two years of my life I'm never ge...   \n",
       "1                     Wh-Where'd you get a meatball...   \n",
       "2    Okay, what is so urgent that you called me and...   \n",
       "3                                            How much?   \n",
       "4    You're being super nice. It's... freaking me o...   \n",
       "..                                                 ...   \n",
       "517  Okay, I want to lay down some ground rules for...   \n",
       "518                 It looks to be a... sacred... spa.   \n",
       "519                  That's putting it a bit strongly.   \n",
       "520                                              I do.   \n",
       "521                      It's great. Drinking at work.   \n",
       "\n",
       "                                            prd_greedy  \\\n",
       "0                                             Oh, God!   \n",
       "1             I don't know. I just saw a meatball sub.   \n",
       "2                       I'm sorry, I don't follow you.   \n",
       "3    I have not decided. I want to get married in a...   \n",
       "4                    I'm not gross. I'm just... gross.   \n",
       "..                                                 ...   \n",
       "517  Oh, I know. I just want to be as awesome as sh...   \n",
       "518                     I'm sorry, I don't follow you.   \n",
       "519        I'm not going to put it in a little strong.   \n",
       "520                                 You're a good man.   \n",
       "521                          I'm not drinking at work.   \n",
       "\n",
       "                                            prd_nbeams  \\\n",
       "0                                             Oh, God!   \n",
       "1             I don't know. I just saw a meatball sub.   \n",
       "2                       I'm sorry, I don't follow you.   \n",
       "3    I have not decided. I want to get married in a...   \n",
       "4                    I'm not gross. I'm just... gross.   \n",
       "..                                                 ...   \n",
       "517  Oh, I know. I just want to be as awesome as sh...   \n",
       "518                     I'm sorry, I don't follow you.   \n",
       "519        I'm not going to put it in a little strong.   \n",
       "520                                 You're a good man.   \n",
       "521                          I'm not drinking at work.   \n",
       "\n",
       "                                    prd_sampling  \\\n",
       "0                                 Oh, of course.   \n",
       "1                            You don't remember?   \n",
       "2                 All right. It's time to start?   \n",
       "3            He said he had like 3,000 in suits.   \n",
       "4                            Buckminster Fuller?   \n",
       "..                                           ...   \n",
       "517                      No, I'll be right over.   \n",
       "518                           It's called a spa.   \n",
       "519                 And here comes the fun part.   \n",
       "520                                         Hey.   \n",
       "521  Barney, I didn't mean any personal attacks!   \n",
       "\n",
       "                                                   lbl  \\\n",
       "0                                        Daddy's home.   \n",
       "1                              I don't have much time!   \n",
       "2    I could tell you knew something was up with me...   \n",
       "3                                            A little.   \n",
       "4    I'm being Barney, and I think tonight's going ...   \n",
       "..                                                 ...   \n",
       "517  Well, well, well. How rich. You make me promis...   \n",
       "518          Owl. How do we go? We will do what? Jump?   \n",
       "519           A bit strongly. She's not my girlfriend.   \n",
       "520  I'm gonna head out to a reggae concert. I'm a ...   \n",
       "521                       Basically, it is of Mad Men.   \n",
       "\n",
       "                                                ctx_tk  \\\n",
       "0    [[40, 760, 11, 340, 338, 734, 812, 286, 616, 1...   \n",
       "1    [[1199, 12, 8496, 1549, 345, 651, 257, 6174, 1...   \n",
       "2    [[16454, 11, 644, 318, 523, 18039, 326, 345, 1...   \n",
       "3                             [[2437, 881, 30, 50256]]   \n",
       "4    [[1639, 821, 852, 2208, 3621, 13, 632, 338, 98...   \n",
       "..                                                 ...   \n",
       "517  [[16454, 11, 314, 765, 284, 3830, 866, 617, 23...   \n",
       "518  [[1026, 3073, 284, 307, 257, 986, 13626, 986, ...   \n",
       "519  [[2504, 338, 5137, 340, 257, 1643, 7634, 13, 5...   \n",
       "520                             [[40, 466, 13, 50256]]   \n",
       "521  [[1026, 338, 1049, 13, 43963, 379, 670, 13, 50...   \n",
       "\n",
       "                                         prd_greedy_tk  \\\n",
       "0                           [5812, 11, 1793, 0, 50256]   \n",
       "1    [40, 836, 470, 760, 13, 314, 655, 2497, 257, 6...   \n",
       "2    [40, 1101, 7926, 11, 314, 836, 470, 1061, 345,...   \n",
       "3    [40, 423, 407, 3066, 13, 314, 765, 284, 651, 6...   \n",
       "4    [40, 1101, 407, 10319, 13, 314, 1101, 655, 986...   \n",
       "..                                                 ...   \n",
       "517  [5812, 11, 314, 760, 13, 314, 655, 765, 284, 3...   \n",
       "518  [40, 1101, 7926, 11, 314, 836, 470, 1061, 345,...   \n",
       "519  [40, 1101, 407, 1016, 284, 1234, 340, 287, 257...   \n",
       "520              [1639, 821, 257, 922, 582, 13, 50256]   \n",
       "521         [40, 1101, 407, 7722, 379, 670, 13, 50256]   \n",
       "\n",
       "                                         prd_nbeams_tk  \\\n",
       "0                           [5812, 11, 1793, 0, 50256]   \n",
       "1    [40, 836, 470, 760, 13, 314, 655, 2497, 257, 6...   \n",
       "2    [40, 1101, 7926, 11, 314, 836, 470, 1061, 345,...   \n",
       "3    [40, 423, 407, 3066, 13, 314, 765, 284, 651, 6...   \n",
       "4    [40, 1101, 407, 10319, 13, 314, 1101, 655, 986...   \n",
       "..                                                 ...   \n",
       "517  [5812, 11, 314, 760, 13, 314, 655, 765, 284, 3...   \n",
       "518  [40, 1101, 7926, 11, 314, 836, 470, 1061, 345,...   \n",
       "519  [40, 1101, 407, 1016, 284, 1234, 340, 287, 257...   \n",
       "520              [1639, 821, 257, 922, 582, 13, 50256]   \n",
       "521         [40, 1101, 407, 7722, 379, 670, 13, 50256]   \n",
       "\n",
       "                                       prd_sampling_tk  \\\n",
       "0                     [5812, 11, 286, 1781, 13, 50256]   \n",
       "1                    [1639, 836, 470, 3505, 30, 50256]   \n",
       "2    [3237, 826, 13, 632, 338, 640, 284, 923, 30, 5...   \n",
       "3    [1544, 531, 339, 550, 588, 513, 11, 830, 287, ...   \n",
       "4                  [33, 1347, 18462, 31863, 30, 50256]   \n",
       "..                                                 ...   \n",
       "517    [2949, 11, 314, 1183, 307, 826, 625, 13, 50256]   \n",
       "518           [1026, 338, 1444, 257, 41900, 13, 50256]   \n",
       "519       [1870, 994, 2058, 262, 1257, 636, 13, 50256]   \n",
       "520                                 [10814, 13, 50256]   \n",
       "521  [10374, 1681, 11, 314, 1422, 470, 1612, 597, 2...   \n",
       "\n",
       "                                                lbl_tk  \n",
       "0                      [[48280, 338, 1363, 13, 50256]]  \n",
       "1            [[40, 836, 470, 423, 881, 640, 0, 50256]]  \n",
       "2    [[40, 714, 1560, 345, 2993, 1223, 373, 510, 35...  \n",
       "3                              [[32, 1310, 13, 50256]]  \n",
       "4    [[40, 1101, 852, 41921, 11, 290, 314, 892, 997...  \n",
       "..                                                 ...  \n",
       "517  [[5779, 11, 880, 11, 880, 13, 1374, 5527, 13, ...  \n",
       "518  [[46, 40989, 13, 1374, 466, 356, 467, 30, 775,...  \n",
       "519  [[32, 1643, 7634, 13, 1375, 338, 407, 616, 110...  \n",
       "520  [[40, 1101, 8066, 1182, 503, 284, 257, 842, 25...  \n",
       "521  [[31524, 11, 340, 318, 286, 4627, 6065, 13, 50...  \n",
       "\n",
       "[522 rows x 10 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_char = get_dataframe_for_metrics(character_hg['test'], predictions_greedy, predictions_nbeams, predictions_sampling)\n",
    "df_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics For Character 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Lib.BBMetrics import BBMetric\n",
    "\n",
    "def compute_sample_metrics(context_sentence, label_response, chatbot_response, verbose=True, w=(1,1,1)):\n",
    "    scores = {}\n",
    "    if verbose:\n",
    "        # prints the sentences\n",
    "        print('* context:', context_sentence) \n",
    "        print('* label:  ', label_response)\n",
    "        print('* chatbot:', chatbot_response) \n",
    "    # 1) computes metrics for semantic similarity\n",
    "    metric = BBMetric.load_metric(\"semantic similarity\")\n",
    "    scores['semantic similarity'] = [metric.compute(sentences_a=context_sentence,\n",
    "                                                      sentences_b=label_response)['score']]\n",
    "    scores['semantic similarity'].append(metric.compute(sentences_a=context_sentence,\n",
    "                                                      sentences_b=chatbot_response)['score'])\n",
    "    scores['semantic similarity'].append(metric.compute(sentences_a=label_response,\n",
    "                                                      sentences_b=chatbot_response)['score'])\n",
    "    scores['semantic similarity'].append(sum(np.array(scores['semantic similarity']) * np.array(w)) / sum(w))\n",
    "    # ss_scores = scores['semantic similarity']\n",
    "    if verbose:\n",
    "        print('=== SEMANTIC SIMILARITY ===')\n",
    "        print('context-label similarity:   ', scores['semantic similarity'][0])\n",
    "        print('context-chatbot similarity: ', scores['semantic similarity'][1])\n",
    "        print('label-chatbot similarity:   ', scores['semantic similarity'][2])\n",
    "        print('> Merged Metrics')\n",
    "        print('  semantic similarity mean:     ',  scores['semantic similarity'][3])\n",
    "    # 2) computes metrics for bleu\n",
    "    metric = BBMetric.load_metric(\"bleu\")\n",
    "    scores['bleu'] = metric.compute(predictions=chatbot_response, references=label_response)['score']\n",
    "    if verbose:\n",
    "        print('===        BLEU         ===')\n",
    "        print('bleu:                       ', scores['bleu'])\n",
    "    # 3) computes metrics for rouge-L\n",
    "    metric = BBMetric.load_metric(\"rouge l\")\n",
    "    scores['rouge l'] = [metric.compute(predictions=context_sentence, references=label_response)['score']]\n",
    "    scores['rouge l'].append(metric.compute(predictions=context_sentence, references=chatbot_response)['score'])\n",
    "    scores['rouge l'].append(metric.compute(predictions=chatbot_response, references=label_response)['score'])\n",
    "    scores['rouge l'].append(sum(np.array(scores['rouge l']) * np.array(w)) / sum(w))\n",
    "    if verbose:\n",
    "        print('===       ROUGE-L       ===')\n",
    "        print('context-label rouge:        ', scores['rouge l'][0])\n",
    "        print('context-chatbot rouge:      ', scores['rouge l'][1])\n",
    "        print('label-chatbot rouge:        ', scores['rouge l'][2])\n",
    "        print('> Merged Metrics')\n",
    "        print('  rouge mean:                 ', scores['rouge l'][3] )\n",
    "    # 4) computes sas metric\n",
    "    metric = BBMetric.load_metric(\"semantic answer similarity\")\n",
    "    scores['semantic answer similarity'] = [metric.compute(predictions=context_sentence,\n",
    "                                                    references=label_response)['score']]\n",
    "    scores['semantic answer similarity'].append(metric.compute(predictions=context_sentence,\n",
    "                                                        references=chatbot_response)['score'])\n",
    "    scores['semantic answer similarity'].append(metric.compute(predictions=label_response,\n",
    "                                                        references=chatbot_response)['score'])\n",
    "    scores['semantic answer similarity'].append(sum(np.array(scores['semantic answer similarity']) * np.array(w)) / sum(w))\n",
    "    if verbose:\n",
    "        print('===         SAS         ===')\n",
    "        print('context-label sas:          ', scores['semantic answer similarity'][0])\n",
    "        print('context-chatbot sas:        ', scores['semantic answer similarity'][1])\n",
    "        print('label-chatbot sas:          ', scores['semantic answer similarity'][2])\n",
    "        print('> Merged Metrics')\n",
    "        print('  sas mean:                   ',  scores['semantic answer similarity'][3])\n",
    "    # 5) computes emotion metric\n",
    "    metric = BBMetric.load_metric(\"emotion\")\n",
    "    scores['emotion'] = [metric.compute(sentences=context_sentence)]\n",
    "    scores['emotion'].append(metric.compute(sentences=label_response))\n",
    "    scores['emotion'].append(metric.compute(sentences=chatbot_response))\n",
    "    if verbose:\n",
    "        print('===       EMOTION       ===')\n",
    "        print('context emotions:            \\n', list(zip(scores['emotion'][0]['label'], scores['emotion'][0]['score'])))\n",
    "        print('label emotions:              \\n', list(zip(scores['emotion'][1]['label'], scores['emotion'][1]['score'])))\n",
    "        print('chatbot emotions:            \\n', list(zip(scores['emotion'][2]['label'], scores['emotion'][2]['score'])))\n",
    "        print('label-chatbot emotion corr:  \\n', sp.stats.stats.pearsonr(scores['emotion'][1]['score'],\n",
    "                                                                         scores['emotion'][2]['score']))\n",
    "\n",
    "    # 6) computes metrics for distinct\n",
    "    metric = BBMetric.load_metric(\"distinct\")\n",
    "    scores['distinct'] = metric.compute(sentences=chatbot_response)['score']\n",
    "    if verbose:\n",
    "        print('===       DISTINCT      ===')\n",
    "        print('distinct:                   ', scores['distinct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_set_metrics(model, tokenizer, context_sentences, label_responses, chatbot_responses, verbose=True, w=(1,1,1),\n",
    "                        classifier_n_sentences=50, include_qualitative=False):\n",
    "    scores = {}\n",
    "    \n",
    "    # 0) computes metrics for perplexity\n",
    "    metric = BBMetric.load_metric(\"semantic similarity\")\n",
    "    scores['semantic similarity'] = [metric.compute(sentences_a=context_sentences,\n",
    "                                                      sentences_b=label_responses)['score']]\n",
    "    scores['semantic similarity'].append(metric.compute(sentences_a=context_sentences,\n",
    "                                                      sentences_b=chatbot_responses)['score'])\n",
    "    scores['semantic similarity'].append(metric.compute(sentences_a=label_responses,\n",
    "                                                      sentences_b=chatbot_responses)['score'])\n",
    "    scores['semantic similarity'].append(sum(np.array(scores['semantic similarity']) * np.array(w)) / sum(w))\n",
    "    if verbose:\n",
    "        print('=== SEMANTIC SIMILARITY ===')\n",
    "        print('semantic similarity:        ', scores['semantic similarity'])\n",
    "    # 1) computes metrics for perplexity\n",
    "    metric = BBMetric.load_metric(\"perplexity\")\n",
    "    if verbose:\n",
    "        print('===       PERPLEXITY     ===')\n",
    "    scores['perplexity'] = metric.compute(model=model, tokenizer=tokenizer, sentences=chatbot_responses)['score_concat']\n",
    "    if verbose:\n",
    "        print('perplexity:                 ', scores['perplexity'])\n",
    "    # 2) computes metrics for bleu\n",
    "    metric = BBMetric.load_metric(\"bleu\")\n",
    "    scores['bleu'] = metric.compute(predictions=chatbot_responses, references=label_responses)['score']\n",
    "    if verbose:\n",
    "        print('===         BLEU         ===')\n",
    "        print('bleu:                       ', scores['bleu'])\n",
    "    # 3) computes metrics for rouge-L\n",
    "    metric = BBMetric.load_metric(\"rouge l\")\n",
    "    scores['rouge l'] = metric.compute(predictions=chatbot_responses, references=label_responses)['score']\n",
    "    if verbose:\n",
    "        print('===        ROUGE-L       ===')\n",
    "        print('rouge:                      ', scores['rouge l'])\n",
    "    # 4) computes metrics for distinct\n",
    "    metric = BBMetric.load_metric(\"distinct\")\n",
    "    scores['distinct'] = metric.compute(sentences=chatbot_responses)['score']\n",
    "    if verbose:\n",
    "        print('===        DISTINCT      ===')\n",
    "        print('distinct:                   ', scores['distinct'])\n",
    "    # 6) computes emotion metric\n",
    "    metric = BBMetric.load_metric(\"emotion\")\n",
    "    scores['emotion'] = [metric.compute(sentences=chatbot_responses)]\n",
    "    scores['emotion'].append(metric.compute(sentences=label_responses))\n",
    "    if verbose:\n",
    "        print('===        EMOTION       ===')\n",
    "        print('chatbot emotions:            \\n', list(zip(scores['emotion'][0]['label'], scores['emotion'][0]['score'])))\n",
    "        print('label emotions:              \\n', list(zip(scores['emotion'][1]['label'], scores['emotion'][1]['score'])))\n",
    "        print('label-chatbot emotion corr:  \\n', sp.stats.stats.pearsonr(scores['emotion'][0]['score'],\n",
    "                                                                         scores['emotion'][1]['score']))\n",
    "    # 8) computes sas metric\n",
    "    metric = BBMetric.load_metric(\"semantic answer similarity\")\n",
    "    scores['semantic answer similarity'] = [metric.compute(predictions=context_sentences,\n",
    "                                                    references=label_responses)['score']]\n",
    "    scores['semantic answer similarity'].append(metric.compute(predictions=context_sentences,\n",
    "                                                        references=chatbot_responses)['score'])\n",
    "    scores['semantic answer similarity'].append(metric.compute(predictions=label_responses,\n",
    "                                                        references=chatbot_responses)['score'])\n",
    "    scores['semantic answer similarity'].append(sum(np.array(scores['semantic answer similarity']) * np.array(w)) / sum(w))\n",
    "    if verbose:\n",
    "        print('===         SAS         ===')\n",
    "        print('context-label sas:          ', scores['semantic answer similarity'][0])\n",
    "        print('context-chatbot sas:        ', scores['semantic answer similarity'][1])\n",
    "        print('label-chatbot sas:          ', scores['semantic answer similarity'][2])\n",
    "        print('> Merged Metrics')\n",
    "        print('sas-mean:                   ',  scores['semantic answer similarity'][3])\n",
    "    # 9) computes metrics for semantic classifyer\n",
    "    metric = BBMetric.load_metric(\"semantic classifier\")\n",
    "    start_time = time.time()\n",
    "    scores['semantic classifier'] = [metric.compute(character=character, character_dict=character_dict, \n",
    "                                                   base_folder=base_folder, sentences=chatbot_responses,\n",
    "                                                   n_sentences=classifier_n_sentences)]\n",
    "    scores['semantic classifier'].append(metric.compute(character=character, character_dict=character_dict, \n",
    "                                                   base_folder=base_folder, sentences=label_responses,\n",
    "                                                   n_sentences=classifier_n_sentences))\n",
    "    end_time = time.time()\n",
    "    if verbose:\n",
    "        print('=== SEMANTIC CLASSIFIER ===')\n",
    "        print('semantic classifier chatbot:                ', scores['semantic classifier'][0])\n",
    "        print('semantic classifier label:                  ', scores['semantic classifier'][1])\n",
    "        print('time elapsed computing semantic classifier:  {:.2f} s'.format(end_time - start_time))\n",
    "        \n",
    "    if include_qualitative:\n",
    "        # Do stuff with human metrics and print sentences\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Sample 1 #####\n",
      "* context: I know, it's two years of my life I'm never getting back. A little part of me just wants to jump the bones of the next guy I see.\n",
      "* label:   Daddy's home.\n",
      "* chatbot: Oh, God!\n",
      "=== SEMANTIC SIMILARITY ===\n",
      "context-label similarity:    0.009417437\n",
      "context-chatbot similarity:  0.017228587\n",
      "label-chatbot similarity:    0.008852758\n",
      "> Merged Metrics\n",
      "script-similarity-mean:      0.011832927353680134\n",
      "===        BLEU         ===\n",
      "bleu:                        0.0\n",
      "===       ROUGE-L       ===\n",
      "context-label rouge:         0.0588235294117647\n",
      "context-chatbot rouge:       0.0\n",
      "label-chatbot rouge:         0.0\n",
      "> Merged Metrics\n",
      "rouge mean:                  0.019607843137254898\n",
      "===         SAS         ===\n",
      "context-label sas:           0.1468764\n",
      "context-chatbot sas:         0.18231258\n",
      "label-chatbot sas:           0.21477439\n",
      "> Merged Metrics\n",
      "sas-mean:                    0.18132111926873526\n",
      "===       EMOTION       ===\n",
      "context emotions:            \n",
      " [('sadness', 0.006270761135965586), ('joy', 0.015778295695781708), ('love', 0.0008533367654308677), ('anger', 0.9678378701210022), ('fear', 0.008764984086155891), ('surprise', 0.000494705862365663)]\n",
      "label emotions:              \n",
      " [('sadness', 0.10040660947561264), ('joy', 0.08988485485315323), ('love', 0.004643112886697054), ('anger', 0.7096090912818909), ('fear', 0.0934947058558464), ('surprise', 0.001961600501090288)]\n",
      "chatbot emotions:            \n",
      " [('sadness', 0.061727896332740784), ('joy', 0.5121124982833862), ('love', 0.012885131873190403), ('anger', 0.09026123583316803), ('fear', 0.21414390206336975), ('surprise', 0.108869269490242)]\n",
      "===       DISTINCT      ===\n",
      "distinct:                    0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    print(\"##### Sample \" + str(i+1) + \" #####\")\n",
    "    context_sentence = df_char['ctx'][i]\n",
    "    chatbot_response = df_char['prd_greedy'][i]\n",
    "    label_response   = df_char['lbl'][i]\n",
    "    compute_sample_metrics(context_sentence, label_response, chatbot_response)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Set (Size 50) #####\n",
      "=== SEMANTIC SIMILARITY ===\n",
      "semantic similarity:         [0.07489894, 0.20145756, 0.08608877, 0.12081509083509445]\n",
      "===       PERPLEXITY     ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:42<00:00,  3.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity:                  247.60796924454095\n",
      "===         BLEU         ===\n",
      "bleu:                        0.0\n",
      "===        ROUGE-L       ===\n",
      "rouge:                       0.07438013490229321\n",
      "===        DISTINCT      ===\n",
      "distinct:                    0.11439184252887444\n",
      "===        EMOTION       ===\n",
      "chatbot emotions:            \n",
      " [('sadness', 0.3434286758565577), ('joy', 0.2214338393410435), ('love', 0.006544853251543827), ('anger', 0.33637171503651186), ('fear', 0.08191320944606559), ('surprise', 0.010307701138954144)]\n",
      "label emotions:              \n",
      " [('sadness', 0.13711809609201736), ('joy', 0.3487064220244065), ('love', 0.007899202467233407), ('anger', 0.40897555726842255), ('fear', 0.09460336329939309), ('surprise', 0.0026973685980192386)]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-cfa52417dce8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mlabel_responses\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_for_metrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lbl'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mset_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# print(chatbot_responses)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m compute_set_metrics(model, tokenizer,\n\u001b[0m\u001b[0;32m      9\u001b[0m                     context_sentences, label_responses, chatbot_responses)\n",
      "\u001b[1;32m<ipython-input-15-3607ef29aefd>\u001b[0m in \u001b[0;36mcompute_set_metrics\u001b[1;34m(model, tokenizer, context_sentences, label_responses, chatbot_responses, verbose, w, classifier_n_sentences)\u001b[0m\n\u001b[0;32m     54\u001b[0m     scores['semantic answer similarity'].append(metric.compute(predictions=context_sentences,\n\u001b[0;32m     55\u001b[0m                                                         references=chatbot_responses)['score'])\n\u001b[1;32m---> 56\u001b[1;33m     scores['semantic answer similarity'].append(metric.compute(predictions=label_responses,\n\u001b[0m\u001b[0;32m     57\u001b[0m                                                         references=chatbot_responses)['score'])\n\u001b[0;32m     58\u001b[0m     \u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'semantic answer similarity'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'semantic answer similarity'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBot\\BarneyBot\\Lib\\BBMetrics.py\u001b[0m in \u001b[0;36mcompute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    507\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"semantic answer similarity\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'predictions'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'predictions'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mlist\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'predictions'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 509\u001b[1;33m             \u001b[0mreferences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'references'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'references'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mlist\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'references'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    510\u001b[0m             single_outputs = np.diagonal(cosine_similarity(self.metric.encode(predictions),\n\u001b[0;32m    511\u001b[0m                                                            self.metric.encode(references)))\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m                 \u001b[0mout_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0moutput_value\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'token_embeddings'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[0mtrans_features\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'token_type_ids'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'token_type_ids'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0moutput_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mtrans_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m         \u001b[0moutput_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    848\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m         )\n\u001b[1;32m--> 850\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m    851\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    852\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    524\u001b[0m                 )\n\u001b[0;32m    525\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m                 layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m    527\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    410\u001b[0m         \u001b[1;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 412\u001b[1;33m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[0;32m    413\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m     ):\n\u001b[1;32m--> 339\u001b[1;33m         self_outputs = self.self(\n\u001b[0m\u001b[0;32m    340\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m     ):\n\u001b[1;32m--> 203\u001b[1;33m         \u001b[0mmixed_query_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[1;31m# If this is instantiated as a cross-attention module, the keys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "set_size = 50\n",
    "i = 30\n",
    "print(\"##### Set (Size \" + str(set_size) + \") #####\")\n",
    "context_sentences = list(df_char['ctx'][i:i+set_size])\n",
    "chatbot_responses = list(df_char['prd_greedy'][i:i+set_size])\n",
    "label_responses   = list(df_char['lbl'][i:i+set_size])\n",
    "compute_set_metrics(model, tokenizer,\n",
    "                    context_sentences, label_responses, chatbot_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"##### Full Test Set (Size \" + str(set_size) + \") #####\")\n",
    "context_sentences = list(df_char['ctx'])\n",
    "chatbot_responses = list(df_char['prd_greedy'])\n",
    "label_responses   = list(df_char['lbl'])\n",
    "compute_set_metrics(model, tokenizer,\n",
    "                    context_sentences, \n",
    "                    label_responses, \n",
    "                    chatbot_responses,\n",
    "                    classifier_n_sentences=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Between Character 1 & Character 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_small(sample_questions, model, generation_method):\n",
    "    print(\"Creating predictions\")\n",
    "    predictions = list()\n",
    "    for x in tqdm(sample_questions):\n",
    "        tokenized_question = tokenizer.encode(x + tokenizer.eos_token, return_tensors='tf')\n",
    "        max_length = 128 + tokenized_question.shape[1]\n",
    "        if generation_method == \"Greedy\":\n",
    "            generated_answer = model.generate(tokenized_question,\n",
    "                                pad_token_id=tokenizer.eos_token_id, max_length=max_length)[0].numpy().tolist()\n",
    "        elif generation_method == \"Beam Search\":\n",
    "            generated_answer = model.generate(tokenized_question,\n",
    "                                         pad_token_id=tokenizer.eos_token_id, max_length=max_length,\n",
    "                                         n_beams=n_beams)[0].numpy().tolist()\n",
    "        elif generation_method == \"Sampling\":\n",
    "            generated_answer = model.generate(tokenized_question,\n",
    "                                         pad_token_id=tokenizer.eos_token_id, max_length=max_length,\n",
    "                                         do_sample=True, top_k=top_k, top_p=top_p)[0].numpy().tolist()\n",
    "        predictions.append(generated_answer[len(tokenized_question[0]):])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_df = load_dataset('csv',\n",
    "                         data_files=os.path.join(base_folder, 'Data', 'common_dataset.csv'), \n",
    "                         cache_dir=os.path.join(base_folder, \"cache\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_1_greedy = get_predictions_small(common_df['context'], model, \"Greedy\")\n",
    "predictions_1_nbeams = get_predictions_small(common_df['context'], model, \"Beam Search\")\n",
    "predictions_1_sampling = get_predictions_small(common_df['context'], model, \"Sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_2_greedy = get_predictions_small(common_df['context'], model_2, \"Greedy\")\n",
    "predictions_2_nbeams = get_predictions_small(common_df['context'], model_2, \"Beam Search\")\n",
    "predictions_2_sampling = get_predictions_small(common_df['context'], model_2, \"Sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char_1 = get_dataframe_for_metrics(common_df, predictions_1_greedy, predictions_1_nbeams, predictions_1_sampling)\n",
    "df_char_2 = get_dataframe_for_metrics(common_df, predictions_2_greedy, predictions_2_nbeams, predictions_2_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"##### \" + character \"Vs. \" + character_2 + \" #####\")\n",
    "context_sentences   = list(df_for_metrics['ctx'])\n",
    "chatbot_responses   = list(df_char_1['prd_sampling'])\n",
    "chatbot_2_responses = list(df_char_2['prd_sampling'])\n",
    "compute_set_metrics(model, tokenizer,\n",
    "                    context_sentences, chatbot_2_responses, chatbot_responses, include_qualitative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Between Different Sampling Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"##### Greedy vs. N-Beams #####\")\n",
    "context_sentences = list(df_char['ctx'])\n",
    "greedy_responses  = list(df_char['prd_greedy'])\n",
    "nbeams_responses  = list(df_char['prd_nbeams'])\n",
    "compute_set_metrics(model, tokenizer,\n",
    "                    context_sentences,\n",
    "                    greedy_responses,\n",
    "                    nbeams_responses,\n",
    "                    classifier_n_sentences=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"##### Greedy vs. Sampling #####\")\n",
    "context_sentences   = list(df_char['ctx'])\n",
    "greedy_responses    = list(df_char['prd_greedy'])\n",
    "sampling_responses  = list(df_char['prd_sampling'])\n",
    "compute_set_metrics(model, tokenizer,\n",
    "                    context_sentences,\n",
    "                    greedy_responses,\n",
    "                    sampling_responses,\n",
    "                    classifier_n_sentences=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"##### N-Beams vs. Sampling #####\")\n",
    "context_sentences   = list(df_char['ctx'])\n",
    "nbeams_responses    = list(df_char['prd_nbeams'])\n",
    "sampling_responses  = list(df_char['prd_sampling'])\n",
    "compute_set_metrics(model, tokenizer,\n",
    "                    context_sentences,\n",
    "                    nbeams_responses,\n",
    "                    sampling_responses,\n",
    "                    classifier_n_sentences=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Between Non-Finetuned And Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_def_sampling = get_predictions_cached(sample_questions, model_def,\n",
    "                                                  os.path.join(in_folder_def, 'from_' + character + '_df_' + '_sampling.json'),\n",
    "                                                  \"Sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char_def = get_dataframe_for_metrics(character_hg['test'], None, None, predictions_def_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    print(\"##### Sample \" + str(i+1) + \" #####\")\n",
    "    context_sentence   = df_char['ctx'][i]\n",
    "    character_response = df_char['prd_sampling'][i]\n",
    "    default_response   = df_char_def['prd_sampling'][i]\n",
    "    compute_sample_metrics(context_sentence, default_response, character_response)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_size = 50\n",
    "i = 30\n",
    "print(\"##### Set (Size \" + str(set_size) + \") #####\")\n",
    "context_sentences   = list(df_char['ctx'][i:i+set_size])\n",
    "character_responses = list(df_char['prd_sampling'][i:i+set_size])\n",
    "default_responses   = list(df_char_def['prd_sampling'][i:i+set_size])\n",
    "compute_set_metrics(model, tokenizer,\n",
    "                    context_sentences, default_responses, character_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"##### Full Test Set (Size \" + str(set_size) + \") #####\")\n",
    "context_sentences   = list(df_char['ctx'])\n",
    "character_responses = list(df_char['prd_sampling'])\n",
    "default_responses   = list(df_char_def['prd_sampling'])\n",
    "compute_set_metrics(model, tokenizer,\n",
    "                    context_sentences, \n",
    "                    default_responses, \n",
    "                    character_responses,\n",
    "                    classifier_n_sentences=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

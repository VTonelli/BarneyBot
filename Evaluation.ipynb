{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data.data_dicts import character_dict, source_dict, random_state\n",
    "\n",
    "model_name = 'microsoft/DialoGPT-small'\n",
    "character = 'Sheldon' # 'Barney' | 'Sheldon' | 'Harry' | 'Fry' | 'Vader' | 'Joey' | 'Phoebe' | 'Bender' | Default'\n",
    "character_2 = 'Sheldon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount google drive\n",
    "import os\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive',force_remount=True)\n",
    "    base_folder = '/content/drive/My Drive/unibo/NLP_project/BarneyBot'\n",
    "    os.system(\"pip install datasets\")\n",
    "    os.system(\"pip install transformers\")\n",
    "    os.system(\"pip install rouge_score\")\n",
    "    os.system(\"pip install -U sentence-transformers\")\n",
    "else:\n",
    "    base_folder = os.getcwd()\n",
    "    \n",
    "in_folder = os.path.join(base_folder, 'Data', 'Characters', character)\n",
    "if not os.path.exists(in_folder):\n",
    "    os.makedirs(in_folder)\n",
    "out_folder = os.path.join(base_folder, 'Data', 'Characters', character)\n",
    "if not os.path.exists(out_folder):\n",
    "    os.makedirs(out_folder)\n",
    "    \n",
    "in_folder_2 = os.path.join(base_folder, 'Data', 'Characters', character_2)\n",
    "if not os.path.exists(in_folder_2):\n",
    "    os.makedirs(in_folder_2)\n",
    "out_folder_2 = os.path.join(base_folder, 'Data', 'Characters', character_2)\n",
    "if not os.path.exists(out_folder_2):\n",
    "    os.makedirs(out_folder_2)\n",
    "    \n",
    "in_folder_def = os.path.join(base_folder, 'Data', 'Characters', 'Default')\n",
    "if not os.path.exists(in_folder_def):\n",
    "    os.makedirs(in_folder_def)\n",
    "out_folder_def = os.path.join(base_folder, 'Data', 'Characters', 'Default')\n",
    "if not os.path.exists(out_folder_def):\n",
    "    os.makedirs(out_folder_def)\n",
    "    \n",
    "metrics_folder = os.path.join(base_folder, 'Metrics')\n",
    "if not os.path.exists(metrics_folder):\n",
    "    os.makedirs(metrics_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_json(filepath, filename, data):\n",
    "    if not os.path.exists(filepath):\n",
    "        os.makedirs(filepath, exist_ok=True)\n",
    "    with open(os.path.join(filepath, filename + \".json\"), 'w') as f:\n",
    "        f.write(json.dumps(data, indent=4))\n",
    "\n",
    "def load_from_json(filepath, filename):\n",
    "    if not os.path.exists(os.path.join(filepath, filename + '.json')):\n",
    "        return dict()\n",
    "    with open(os.path.join(filepath, filename + '.json'), 'r') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "def load_df(character):\n",
    "    dataset_path = os.path.join(base_folder, \"Data\", \"Characters\", character, character+'.csv')\n",
    "    \n",
    "    character_hg = load_dataset('csv', \n",
    "                                data_files=dataset_path, \n",
    "                                cache_dir=os.path.join(base_folder, \"cache\"))\n",
    "    \n",
    "    # 85% train / 10% test / 5% validation\n",
    "    train_test_hg = character_hg['train'].train_test_split(test_size=0.15, seed=random_state)\n",
    "    test_val = train_test_hg['test'].train_test_split(test_size=0.33, seed=random_state)\n",
    "    \n",
    "    \n",
    "    character_hg = DatasetDict({\n",
    "        'train': train_test_hg['train'],\n",
    "        'test': test_val['train'],\n",
    "        'val': test_val['test']\n",
    "    })\n",
    "    \n",
    "    return character_hg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_conv(row, tokenizer):\n",
    "    MAX_LENGTH = 512\n",
    "    row = list(reversed(list(row.values())))\n",
    "    model_inputs = tokenizer(row)\n",
    "    tokenizer_pad_token_id = tokenizer.encode('#')[0]\n",
    "    for i in range(len(model_inputs['input_ids'])):\n",
    "        model_inputs['input_ids'][i].append(tokenizer.eos_token_id)\n",
    "        model_inputs['attention_mask'][i].append(1)\n",
    "    model_inputs['input_ids'] = [item for sublist in model_inputs['input_ids'] for item in sublist]\n",
    "    model_inputs['attention_mask'] = [item for sublist in model_inputs['attention_mask'] for item in sublist]\n",
    "    if MAX_LENGTH > len(model_inputs['input_ids']):\n",
    "        model_inputs['input_ids'] += [tokenizer_pad_token_id] * (MAX_LENGTH - len(model_inputs['input_ids']))\n",
    "        model_inputs['attention_mask'] += [0] * (MAX_LENGTH - len(model_inputs['attention_mask']))\n",
    "    elif MAX_LENGTH < len(model_inputs['input_ids']):\n",
    "        model_inputs['input_ids'] = model_inputs['input_ids'][:MAX_LENGTH-1]\n",
    "        model_inputs['input_ids'][-1] = tokenizer.eos_token_id\n",
    "        model_inputs['attention_mask'] = model_inputs['attention_mask'][:MAX_LENGTH-1]\n",
    "        model_inputs['attention_mask'][-1] = 1\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    tokenizer.pad_token = '#'\n",
    "    model_inputs = construct_conv(examples, tokenizer)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-5d3fb72e84c157b9\n",
      "Reusing dataset csv (C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\cache\\csv\\default-5d3fb72e84c157b9\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d268b89e8b34d729183292c4852224b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\cache\\csv\\default-5d3fb72e84c157b9\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-0376ccc97696dc99.arrow and C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\cache\\csv\\default-5d3fb72e84c157b9\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-94a7895d9c950d70.arrow\n",
      "Loading cached split indices for dataset at C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\cache\\csv\\default-5d3fb72e84c157b9\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-077b138009d8d023.arrow and C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\cache\\csv\\default-5d3fb72e84c157b9\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-9473b74d41745f45.arrow\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"HF_DATASETS_CACHE\"] = os.path.join(base_folder, \"cache\")\n",
    "character_hg = load_df(character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_folder = os.path.join(out_folder, character_dict[character]['checkpoint_folder'])\n",
    "checkpoint_folder_2 = os.path.join(out_folder_2, character_dict[character_2]['checkpoint_folder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\Data\\Characters\\Sheldon\\sheldon_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at C:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\Data\\Characters\\Sheldon\\sheldon_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at microsoft/DialoGPT-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=os.path.join(base_folder, \"cache\"))\n",
    "tokenizer.pad_token = '#'\n",
    "\n",
    "model = TFAutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=checkpoint_folder)\n",
    "model_2 = TFAutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=checkpoint_folder_2)\n",
    "model_def = TFAutoModelForCausalLM.from_pretrained(model_name, cache_dir=os.path.join(base_folder, \"cache\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_questions = character_hg['test']['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_beams = 3\n",
    "top_k = 50\n",
    "top_p = 0.92\n",
    "\n",
    "def get_predictions_cached(sample_questions, model, filename, generation_method, override_predictions=False):\n",
    "    prediction_path = os.path.join(in_folder, filename)\n",
    "    if os.path.exists(prediction_path) and not override_predictions:\n",
    "        print(\"Loading predictions from stored file\")\n",
    "        with open(prediction_path, 'r') as file:\n",
    "            json_string = file.read()\n",
    "        predictions = json.loads(json_string)\n",
    "        print(\"Loaded predictions from stored file\")\n",
    "\n",
    "    else:\n",
    "        print(\"Creating predictions\")\n",
    "        predictions = list()\n",
    "        for x in tqdm(sample_questions):\n",
    "            tokenized_question = tokenizer.encode(x + tokenizer.eos_token, return_tensors='tf')\n",
    "            max_length = 128 + tokenized_question.shape[1]\n",
    "            if generation_method == \"Greedy\":\n",
    "                generated_answer = model.generate(tokenized_question,\n",
    "                                    pad_token_id=tokenizer.eos_token_id, max_length=max_length)[0].numpy().tolist()\n",
    "            elif generation_method == \"Beam Search\":\n",
    "                generated_answer = model.generate(tokenized_question,\n",
    "                                             pad_token_id=tokenizer.eos_token_id, max_length=max_length,\n",
    "                                             n_beams=n_beams)[0].numpy().tolist()\n",
    "            elif generation_method == \"Sampling\":\n",
    "                b = True\n",
    "                c = 0\n",
    "                while b:\n",
    "                    generated_answer = model.generate(tokenized_question,\n",
    "                                                 pad_token_id=tokenizer.eos_token_id, max_length=max_length,\n",
    "                                                 do_sample=True, top_k=top_k, top_p=top_p)[0].numpy().tolist()\n",
    "                    c += 1\n",
    "                    if len(generated_answer[len(tokenized_question[0]):])>1:\n",
    "                        b = False       \n",
    "                    if c>100: \n",
    "                        generated_answer[len(tokenized_question[0]):] = tokenizer.encode('hi') + [tokenizer.eos_token_id]\n",
    "                        break\n",
    "            \n",
    "            predictions.append(generated_answer[len(tokenized_question[0]):])\n",
    "\n",
    "        # Save predictions as a JSON file\n",
    "        output_string = json.dumps(predictions)\n",
    "        with open(prediction_path, 'w') as file:\n",
    "            file.write(output_string)\n",
    "        \n",
    "        assert all([len(p)>1 for p in predictions])\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading predictions from stored file\n",
      "Loaded predictions from stored file\n",
      "Loading predictions from stored file\n",
      "Loaded predictions from stored file\n",
      "Loading predictions from stored file\n",
      "Loaded predictions from stored file\n"
     ]
    }
   ],
   "source": [
    "predictions_greedy = get_predictions_cached(sample_questions, model,\n",
    "                                            character_dict[character]['prediction_filename'] + '_greedy.json',\n",
    "                                            \"Greedy\")\n",
    "predictions_nbeams = get_predictions_cached(sample_questions, model,\n",
    "                                            character_dict[character]['prediction_filename'] + '_nbeams.json',\n",
    "                                            \"Beam Search\")\n",
    "predictions_sampling = get_predictions_cached(sample_questions, model,\n",
    "                                              character_dict[character]['prediction_filename'] + '_sampling.json',\n",
    "                                              \"Sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe_for_metrics(data_test, predictions_greedy, predictions_nbeams, predictions_sampling):\n",
    "    i = 0\n",
    "    df = {'ctx':[], 'ctx_tk':[]}\n",
    "    has_labels = 'response' in data_test.features\n",
    "    if has_labels:\n",
    "        df['lbl'] = []\n",
    "        df['lbl_tk'] = []\n",
    "    if predictions_greedy:\n",
    "        df['prd_greedy'] = []\n",
    "        df['prd_greedy_tk'] = []\n",
    "    if predictions_nbeams:\n",
    "        df['prd_nbeams'] = []\n",
    "        df['prd_nbeams_tk'] = [] \n",
    "    if predictions_sampling:\n",
    "        df['prd_sampling'] = []\n",
    "        df['prd_sampling_tk'] = []\n",
    "    for sample in tqdm(data_test):\n",
    "        # encode the context and label sentences, add the eos_token and return a tensor\n",
    "        ctx_tk = tokenizer.encode(sample['context'] + tokenizer.eos_token, return_tensors='tf').numpy().tolist()\n",
    "        ctx = sample['context']\n",
    "        df['ctx_tk'].append(ctx_tk)\n",
    "        df['ctx'].append(ctx)\n",
    "        if has_labels:\n",
    "            lbl_tk = tokenizer.encode(sample['response'] + tokenizer.eos_token, return_tensors='tf').numpy().tolist()\n",
    "            lbl = sample['response']\n",
    "            df['lbl'].append(lbl)\n",
    "            df['lbl_tk'].append(lbl_tk)\n",
    "        if predictions_greedy:\n",
    "            prd_greedy_tk = predictions_greedy[i]\n",
    "            prd_greedy = tokenizer.decode(prd_greedy_tk, skip_special_tokens=True)\n",
    "            df['prd_greedy'].append(prd_greedy)\n",
    "            df['prd_greedy_tk'].append(prd_greedy_tk)\n",
    "        if predictions_nbeams:\n",
    "            prd_nbeams_tk = predictions_nbeams[i]\n",
    "            prd_nbeams = tokenizer.decode(prd_nbeams_tk, skip_special_tokens=True)\n",
    "            df['prd_nbeams'].append(prd_nbeams)\n",
    "            df['prd_nbeams_tk'].append(prd_nbeams_tk)\n",
    "        if predictions_sampling:\n",
    "            prd_sampling_tk = predictions_sampling[i]\n",
    "            prd_sampling = tokenizer.decode(prd_sampling_tk, skip_special_tokens=True)\n",
    "            df['prd_sampling'].append(prd_sampling)\n",
    "            df['prd_sampling_tk'].append(prd_sampling_tk)\n",
    "        i += 1\n",
    "    return pd.DataFrame(data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1170/1170 [00:00<00:00, 1724.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ctx</th>\n",
       "      <th>ctx_tk</th>\n",
       "      <th>lbl</th>\n",
       "      <th>lbl_tk</th>\n",
       "      <th>prd_greedy</th>\n",
       "      <th>prd_greedy_tk</th>\n",
       "      <th>prd_nbeams</th>\n",
       "      <th>prd_nbeams_tk</th>\n",
       "      <th>prd_sampling</th>\n",
       "      <th>prd_sampling_tk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oh, sweetie, Im sorry.</td>\n",
       "      <td>[[5812, 11, 6029, 494, 11, 1846, 7926, 13, 502...</td>\n",
       "      <td>She called me dumbass.</td>\n",
       "      <td>[[3347, 1444, 502, 13526, 562, 13, 50256]]</td>\n",
       "      <td>No, its okay.</td>\n",
       "      <td>[2949, 11, 663, 8788, 13, 50256]</td>\n",
       "      <td>No, its okay.</td>\n",
       "      <td>[2949, 11, 663, 8788, 13, 50256]</td>\n",
       "      <td>Uh, I think I just had a panic attack.</td>\n",
       "      <td>[34653, 11, 314, 892, 314, 655, 550, 257, 1361...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oh. Wow. Good job. Okay, um, can you do this?</td>\n",
       "      <td>[[5812, 13, 24755, 13, 4599, 1693, 13, 16805, ...</td>\n",
       "      <td>Well never know.</td>\n",
       "      <td>[[5779, 1239, 760, 13, 50256]]</td>\n",
       "      <td>No.</td>\n",
       "      <td>[2949, 13, 50256]</td>\n",
       "      <td>No.</td>\n",
       "      <td>[2949, 13, 50256]</td>\n",
       "      <td>Um, of course I can.</td>\n",
       "      <td>[37280, 11, 286, 1781, 314, 460, 13, 50256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thanks. And, Sheldon, I know tonights the nigh...</td>\n",
       "      <td>[[9690, 13, 843, 11, 34536, 11, 314, 760, 5680...</td>\n",
       "      <td>Oh, you shouldnt have.</td>\n",
       "      <td>[[5812, 11, 345, 815, 429, 423, 13, 50256]]</td>\n",
       "      <td>Thats great.</td>\n",
       "      <td>[817, 1381, 1049, 13, 50256]</td>\n",
       "      <td>Thats great.</td>\n",
       "      <td>[817, 1381, 1049, 13, 50256]</td>\n",
       "      <td>I understand. Its like the Greeks and Romans u...</td>\n",
       "      <td>[40, 1833, 13, 6363, 588, 262, 25059, 290, 224...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Too bad.</td>\n",
       "      <td>[[23307, 2089, 13, 50256]]</td>\n",
       "      <td>Youre going up Euclid Avenue?</td>\n",
       "      <td>[[1639, 260, 1016, 510, 48862, 312, 8878, 30, ...</td>\n",
       "      <td>Sheldon, Ive got a lot to figure out.</td>\n",
       "      <td>[3347, 25900, 11, 314, 303, 1392, 257, 1256, 2...</td>\n",
       "      <td>Sheldon, Ive got a lot to figure out.</td>\n",
       "      <td>[3347, 25900, 11, 314, 303, 1392, 257, 1256, 2...</td>\n",
       "      <td>Yeah, you were right, you were right.</td>\n",
       "      <td>[10995, 11, 345, 547, 826, 11, 345, 547, 826, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Okay, so youd lose most of Jar Jar, all the tr...</td>\n",
       "      <td>[[16454, 11, 523, 345, 67, 4425, 749, 286, 153...</td>\n",
       "      <td>Get rid of the trade route part? Then how woul...</td>\n",
       "      <td>[[3855, 5755, 286, 262, 3292, 6339, 636, 30, 3...</td>\n",
       "      <td>I dont know what you were worried about, but I...</td>\n",
       "      <td>[40, 17666, 760, 644, 345, 547, 7960, 546, 11,...</td>\n",
       "      <td>I dont know what you were worried about, but I...</td>\n",
       "      <td>[40, 17666, 760, 644, 345, 547, 7960, 546, 11,...</td>\n",
       "      <td>Hang on. Whats wrong with C-Span?</td>\n",
       "      <td>[39, 648, 319, 13, 28556, 2642, 351, 327, 12, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1165</th>\n",
       "      <td>Im available for experimentation.</td>\n",
       "      <td>[[3546, 1695, 329, 29315, 13, 50256]]</td>\n",
       "      <td>Thank you. Not necessary. We know everything t...</td>\n",
       "      <td>[[10449, 345, 13, 1892, 3306, 13, 775, 760, 22...</td>\n",
       "      <td>Oh, great.</td>\n",
       "      <td>[5812, 11, 1049, 13, 50256]</td>\n",
       "      <td>Oh, great.</td>\n",
       "      <td>[5812, 11, 1049, 13, 50256]</td>\n",
       "      <td>Yes. You want me to keep a notebook?</td>\n",
       "      <td>[5297, 13, 921, 765, 502, 284, 1394, 257, 2092...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166</th>\n",
       "      <td>Sheldon, why are you ignoring your sister?</td>\n",
       "      <td>[[3347, 25900, 11, 1521, 389, 345, 15482, 534,...</td>\n",
       "      <td>Im not ignoring my sister. Im ignoring all of ...</td>\n",
       "      <td>[[3546, 407, 15482, 616, 6621, 13, 1846, 15482...</td>\n",
       "      <td>Because shes my sister.</td>\n",
       "      <td>[8128, 673, 82, 616, 6621, 13, 50256]</td>\n",
       "      <td>Because shes my sister.</td>\n",
       "      <td>[8128, 673, 82, 616, 6621, 13, 50256]</td>\n",
       "      <td>Im ignoring her, Penny is just trying to butte...</td>\n",
       "      <td>[3546, 15482, 607, 11, 25965, 318, 655, 2111, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1167</th>\n",
       "      <td>Look, Sheldon, before you race off to the fert...</td>\n",
       "      <td>[[8567, 11, 34536, 11, 878, 345, 3234, 572, 28...</td>\n",
       "      <td>You mean dating?</td>\n",
       "      <td>[[1639, 1612, 10691, 30, 50256]]</td>\n",
       "      <td>Oh, I dont know. I mean, I could spend some ti...</td>\n",
       "      <td>[5812, 11, 314, 17666, 760, 13, 314, 1612, 11,...</td>\n",
       "      <td>Oh, I dont know. I mean, I could spend some ti...</td>\n",
       "      <td>[5812, 11, 314, 17666, 760, 13, 314, 1612, 11,...</td>\n",
       "      <td>Oh, I have a couple of questions about this.</td>\n",
       "      <td>[5812, 11, 314, 423, 257, 3155, 286, 2683, 546...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168</th>\n",
       "      <td>Sheldon, Im sorry. Ill be happy to reimburse y...</td>\n",
       "      <td>[[3347, 25900, 11, 1846, 7926, 13, 5821, 307, ...</td>\n",
       "      <td>You dont need to reimburse me because Im not p...</td>\n",
       "      <td>[[1639, 17666, 761, 284, 25903, 502, 780, 1846...</td>\n",
       "      <td>Sheldon, Im sorry.</td>\n",
       "      <td>[3347, 25900, 11, 1846, 7926, 13, 50256]</td>\n",
       "      <td>Sheldon, Im sorry.</td>\n",
       "      <td>[3347, 25900, 11, 1846, 7926, 13, 50256]</td>\n",
       "      <td>What?</td>\n",
       "      <td>[2061, 30, 50256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1169</th>\n",
       "      <td>Oh, God, I feel terrible.</td>\n",
       "      <td>[[5812, 11, 1793, 11, 314, 1254, 7818, 13, 502...</td>\n",
       "      <td>Do you have a stomach ache, too?</td>\n",
       "      <td>[[5211, 345, 423, 257, 11384, 936, 258, 11, 11...</td>\n",
       "      <td>Sheldon, I know youre upset about Amy, but I w...</td>\n",
       "      <td>[3347, 25900, 11, 314, 760, 345, 260, 9247, 54...</td>\n",
       "      <td>Sheldon, I know youre upset about Amy, but I w...</td>\n",
       "      <td>[3347, 25900, 11, 314, 760, 345, 260, 9247, 54...</td>\n",
       "      <td>What did you have in mind?</td>\n",
       "      <td>[2061, 750, 345, 423, 287, 2000, 30, 50256]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1170 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    ctx  \\\n",
       "0                                Oh, sweetie, Im sorry.   \n",
       "1         Oh. Wow. Good job. Okay, um, can you do this?   \n",
       "2     Thanks. And, Sheldon, I know tonights the nigh...   \n",
       "3                                              Too bad.   \n",
       "4     Okay, so youd lose most of Jar Jar, all the tr...   \n",
       "...                                                 ...   \n",
       "1165                  Im available for experimentation.   \n",
       "1166         Sheldon, why are you ignoring your sister?   \n",
       "1167  Look, Sheldon, before you race off to the fert...   \n",
       "1168  Sheldon, Im sorry. Ill be happy to reimburse y...   \n",
       "1169                          Oh, God, I feel terrible.   \n",
       "\n",
       "                                                 ctx_tk  \\\n",
       "0     [[5812, 11, 6029, 494, 11, 1846, 7926, 13, 502...   \n",
       "1     [[5812, 13, 24755, 13, 4599, 1693, 13, 16805, ...   \n",
       "2     [[9690, 13, 843, 11, 34536, 11, 314, 760, 5680...   \n",
       "3                            [[23307, 2089, 13, 50256]]   \n",
       "4     [[16454, 11, 523, 345, 67, 4425, 749, 286, 153...   \n",
       "...                                                 ...   \n",
       "1165              [[3546, 1695, 329, 29315, 13, 50256]]   \n",
       "1166  [[3347, 25900, 11, 1521, 389, 345, 15482, 534,...   \n",
       "1167  [[8567, 11, 34536, 11, 878, 345, 3234, 572, 28...   \n",
       "1168  [[3347, 25900, 11, 1846, 7926, 13, 5821, 307, ...   \n",
       "1169  [[5812, 11, 1793, 11, 314, 1254, 7818, 13, 502...   \n",
       "\n",
       "                                                    lbl  \\\n",
       "0                                She called me dumbass.   \n",
       "1                                      Well never know.   \n",
       "2                                Oh, you shouldnt have.   \n",
       "3                         Youre going up Euclid Avenue?   \n",
       "4     Get rid of the trade route part? Then how woul...   \n",
       "...                                                 ...   \n",
       "1165  Thank you. Not necessary. We know everything t...   \n",
       "1166  Im not ignoring my sister. Im ignoring all of ...   \n",
       "1167                                   You mean dating?   \n",
       "1168  You dont need to reimburse me because Im not p...   \n",
       "1169                   Do you have a stomach ache, too?   \n",
       "\n",
       "                                                 lbl_tk  \\\n",
       "0            [[3347, 1444, 502, 13526, 562, 13, 50256]]   \n",
       "1                        [[5779, 1239, 760, 13, 50256]]   \n",
       "2           [[5812, 11, 345, 815, 429, 423, 13, 50256]]   \n",
       "3     [[1639, 260, 1016, 510, 48862, 312, 8878, 30, ...   \n",
       "4     [[3855, 5755, 286, 262, 3292, 6339, 636, 30, 3...   \n",
       "...                                                 ...   \n",
       "1165  [[10449, 345, 13, 1892, 3306, 13, 775, 760, 22...   \n",
       "1166  [[3546, 407, 15482, 616, 6621, 13, 1846, 15482...   \n",
       "1167                   [[1639, 1612, 10691, 30, 50256]]   \n",
       "1168  [[1639, 17666, 761, 284, 25903, 502, 780, 1846...   \n",
       "1169  [[5211, 345, 423, 257, 11384, 936, 258, 11, 11...   \n",
       "\n",
       "                                             prd_greedy  \\\n",
       "0                                         No, its okay.   \n",
       "1                                                   No.   \n",
       "2                                          Thats great.   \n",
       "3                 Sheldon, Ive got a lot to figure out.   \n",
       "4     I dont know what you were worried about, but I...   \n",
       "...                                                 ...   \n",
       "1165                                         Oh, great.   \n",
       "1166                            Because shes my sister.   \n",
       "1167  Oh, I dont know. I mean, I could spend some ti...   \n",
       "1168                                 Sheldon, Im sorry.   \n",
       "1169  Sheldon, I know youre upset about Amy, but I w...   \n",
       "\n",
       "                                          prd_greedy_tk  \\\n",
       "0                      [2949, 11, 663, 8788, 13, 50256]   \n",
       "1                                     [2949, 13, 50256]   \n",
       "2                          [817, 1381, 1049, 13, 50256]   \n",
       "3     [3347, 25900, 11, 314, 303, 1392, 257, 1256, 2...   \n",
       "4     [40, 17666, 760, 644, 345, 547, 7960, 546, 11,...   \n",
       "...                                                 ...   \n",
       "1165                        [5812, 11, 1049, 13, 50256]   \n",
       "1166              [8128, 673, 82, 616, 6621, 13, 50256]   \n",
       "1167  [5812, 11, 314, 17666, 760, 13, 314, 1612, 11,...   \n",
       "1168           [3347, 25900, 11, 1846, 7926, 13, 50256]   \n",
       "1169  [3347, 25900, 11, 314, 760, 345, 260, 9247, 54...   \n",
       "\n",
       "                                             prd_nbeams  \\\n",
       "0                                         No, its okay.   \n",
       "1                                                   No.   \n",
       "2                                          Thats great.   \n",
       "3                 Sheldon, Ive got a lot to figure out.   \n",
       "4     I dont know what you were worried about, but I...   \n",
       "...                                                 ...   \n",
       "1165                                         Oh, great.   \n",
       "1166                            Because shes my sister.   \n",
       "1167  Oh, I dont know. I mean, I could spend some ti...   \n",
       "1168                                 Sheldon, Im sorry.   \n",
       "1169  Sheldon, I know youre upset about Amy, but I w...   \n",
       "\n",
       "                                          prd_nbeams_tk  \\\n",
       "0                      [2949, 11, 663, 8788, 13, 50256]   \n",
       "1                                     [2949, 13, 50256]   \n",
       "2                          [817, 1381, 1049, 13, 50256]   \n",
       "3     [3347, 25900, 11, 314, 303, 1392, 257, 1256, 2...   \n",
       "4     [40, 17666, 760, 644, 345, 547, 7960, 546, 11,...   \n",
       "...                                                 ...   \n",
       "1165                        [5812, 11, 1049, 13, 50256]   \n",
       "1166              [8128, 673, 82, 616, 6621, 13, 50256]   \n",
       "1167  [5812, 11, 314, 17666, 760, 13, 314, 1612, 11,...   \n",
       "1168           [3347, 25900, 11, 1846, 7926, 13, 50256]   \n",
       "1169  [3347, 25900, 11, 314, 760, 345, 260, 9247, 54...   \n",
       "\n",
       "                                           prd_sampling  \\\n",
       "0                Uh, I think I just had a panic attack.   \n",
       "1                                  Um, of course I can.   \n",
       "2     I understand. Its like the Greeks and Romans u...   \n",
       "3                 Yeah, you were right, you were right.   \n",
       "4                     Hang on. Whats wrong with C-Span?   \n",
       "...                                                 ...   \n",
       "1165               Yes. You want me to keep a notebook?   \n",
       "1166  Im ignoring her, Penny is just trying to butte...   \n",
       "1167       Oh, I have a couple of questions about this.   \n",
       "1168                                              What?   \n",
       "1169                         What did you have in mind?   \n",
       "\n",
       "                                        prd_sampling_tk  \n",
       "0     [34653, 11, 314, 892, 314, 655, 550, 257, 1361...  \n",
       "1           [37280, 11, 286, 1781, 314, 460, 13, 50256]  \n",
       "2     [40, 1833, 13, 6363, 588, 262, 25059, 290, 224...  \n",
       "3     [10995, 11, 345, 547, 826, 11, 345, 547, 826, ...  \n",
       "4     [39, 648, 319, 13, 28556, 2642, 351, 327, 12, ...  \n",
       "...                                                 ...  \n",
       "1165  [5297, 13, 921, 765, 502, 284, 1394, 257, 2092...  \n",
       "1166  [3546, 15482, 607, 11, 25965, 318, 655, 2111, ...  \n",
       "1167  [5812, 11, 314, 423, 257, 3155, 286, 2683, 546...  \n",
       "1168                                  [2061, 30, 50256]  \n",
       "1169        [2061, 750, 345, 423, 287, 2000, 30, 50256]  \n",
       "\n",
       "[1170 rows x 10 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_char = get_dataframe_for_metrics(character_hg['test'], predictions_greedy, predictions_nbeams, predictions_sampling)\n",
    "df_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics For Character 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ccl_sim(ctx_lbl, ctx_cht, lbl_cht):\n",
    "    return ((1 - abs(ctx_lbl - ctx_cht))**2 + lbl_cht**2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Lib.BBMetrics import BBMetric\n",
    "\n",
    "def compute_sample_metrics(context_sentence, label_response, chatbot_response, verbose=True,\n",
    "                           label_chatbot_symmetry=False):\n",
    "    scores = {}\n",
    "    lbl_text = 'label' if not label_chatbot_symmetry else 'chatbota'\n",
    "    cht_text = 'chatbot' if not label_chatbot_symmetry else 'chatbotb'\n",
    "    \n",
    "    scores['metadata'] = {}\n",
    "    scores['metadata']['ordering'] = ['context-'+lbl_text,\n",
    "                                      'context-'+cht_text,\n",
    "                                      cht_text+'-'+lbl_text,\n",
    "                                      'ccl']\n",
    "    \n",
    "    if verbose:\n",
    "        # prints the sample\n",
    "        print('* context:', context_sentence) \n",
    "        print('* ' + lbl_text  + ':  ', label_response)\n",
    "        print('* ' + cht_text  + ':', chatbot_response) \n",
    "    # 1) computes metrics for semantic similarity\n",
    "    metric = BBMetric.load_metric(\"semantic similarity\")\n",
    "    scores['semantic similarity'] = [metric.compute(sentences_a=context_sentence,\n",
    "                                                      sentences_b=label_response)['score']]\n",
    "    scores['semantic similarity'].append(metric.compute(sentences_a=context_sentence,\n",
    "                                                      sentences_b=chatbot_response)['score'])\n",
    "    scores['semantic similarity'].append(metric.compute(sentences_a=label_response,\n",
    "                                                      sentences_b=chatbot_response)['score'])\n",
    "    scores['semantic similarity'].append(ccl_sim(scores['semantic similarity'][0],\n",
    "                                                 scores['semantic similarity'][1],\n",
    "                                                 scores['semantic similarity'][2]))\n",
    "    if verbose:\n",
    "        print('=== SEMANTIC SIMILARITY ===')\n",
    "        print('context-'+lbl_text+' similarity:   ', scores['semantic similarity'][0])\n",
    "        print('context-'+cht_text+' similarity: ', scores['semantic similarity'][1])\n",
    "        print(cht_text+'-'+lbl_text+' similarity:   ', scores['semantic similarity'][2])\n",
    "        print('ccl-sim similarity:            ', scores['semantic similarity'][3])\n",
    "    # 2) computes metrics for bleu\n",
    "    metric = BBMetric.load_metric(\"bleu\")\n",
    "    scores['bleu'] = [metric.compute(predictions=label_response, references=context_sentence)['score']]\n",
    "    scores['bleu'].append(metric.compute(predictions=chatbot_response, references=context_sentence)['score'])\n",
    "    scores['bleu'].append(metric.compute(predictions=chatbot_response, references=label_response)['score'])\n",
    "    scores['bleu'].append(ccl_sim(scores['bleu'][0],\n",
    "                                  scores['bleu'][1],\n",
    "                                  scores['bleu'][2]))\n",
    "    if verbose:\n",
    "        print('===         BLEU         ===')\n",
    "        print('context-to-'+lbl_text+' bleu:      ', scores['bleu'][0])\n",
    "        print('context-to-'+cht_text+' bleu:    ', scores['bleu'][1])\n",
    "        print(lbl_text+'-to-'+cht_text+' bleu:      ', scores['bleu'][2])\n",
    "        print('ccl-sim bleu:            ', scores['bleu'][3])\n",
    "    # 3) computes metrics for rouge-L\n",
    "    metric = BBMetric.load_metric(\"rouge l\")\n",
    "    scores['rouge l'] = [metric.compute(predictions=label_response, references=context_sentence)['score']]\n",
    "    scores['rouge l'].append(metric.compute(predictions=chatbot_response, references=context_sentence)['score'])\n",
    "    scores['rouge l'].append(metric.compute(predictions=chatbot_response, references=label_response)['score'])\n",
    "    scores['rouge l'].append(ccl_sim(scores['rouge l'][0],\n",
    "                                     scores['rouge l'][1],\n",
    "                                     scores['rouge l'][2]))\n",
    "    if verbose:\n",
    "        print('===        ROUGE-L       ===')\n",
    "        print('context-to-'+lbl_text+' rouge:     ', scores['rouge l'][0])\n",
    "        print('context-to-'+cht_text+' rouge:   ', scores['rouge l'][1])\n",
    "        print(lbl_text+'-to-'+cht_text+' rouge:     ', scores['rouge l'][2])\n",
    "        print('ccl-sim rouge:            ', scores['rouge l'][3])\n",
    "    # 6) computes metrics for distinct\n",
    "    metric = BBMetric.load_metric(\"distinct\")\n",
    "    scores['distinct'] = [metric.compute(sentences=context_sentence)['score']]\n",
    "    scores['distinct'].append(metric.compute(sentences=label_response)['score'])\n",
    "    scores['distinct'].append(metric.compute(sentences=chatbot_response)['score'])\n",
    "    scores['distinct'].append(ccl_sim(scores['distinct'][0],\n",
    "                                      scores['distinct'][1],\n",
    "                                      scores['distinct'][2]))\n",
    "    if verbose:\n",
    "        print('===       DISTINCT      ===')\n",
    "        print('context distinct:          ', scores['distinct'][0])\n",
    "        print(lbl_text+' distinct:          ', scores['distinct'][1])\n",
    "        print(cht_text+' distinct:          ', scores['distinct'][2])\n",
    "        print('ccl-sim distinct:          ', scores['distinct'][3])\n",
    "    # 4) computes sas metric\n",
    "    metric = BBMetric.load_metric(\"semantic answer similarity\")\n",
    "    scores['semantic answer similarity'] = [metric.compute(predictions=context_sentence,\n",
    "                                                    references=label_response)['score']]\n",
    "    scores['semantic answer similarity'].append(metric.compute(predictions=context_sentence,\n",
    "                                                        references=chatbot_response)['score'])\n",
    "    scores['semantic answer similarity'].append(metric.compute(predictions=label_response,\n",
    "                                                        references=chatbot_response)['score'])\n",
    "    scores['semantic answer similarity'].append(ccl_sim(scores['semantic answer similarity'][0],\n",
    "                                                        scores['semantic answer similarity'][1],\n",
    "                                                        scores['semantic answer similarity'][2]))\n",
    "    if verbose:\n",
    "        print('===         SAS         ===')\n",
    "        print('context-'+lbl_text+' sas:          ', scores['semantic answer similarity'][0])\n",
    "        print('context-'+cht_text+' sas:        ', scores['semantic answer similarity'][1])\n",
    "        print(lbl_text+'-'+cht_text+' sas:          ', scores['semantic answer similarity'][2])\n",
    "        print('ccl-sim sas:               ', scores['semantic answer similarity'][3])\n",
    "    # 5) computes emotion metric\n",
    "    metric = BBMetric.load_metric(\"emotion\")\n",
    "    scores['emotion'] = [metric.compute(sentences=context_sentence)]\n",
    "    scores['emotion'].append(metric.compute(sentences=label_response))\n",
    "    scores['emotion'].append(metric.compute(sentences=chatbot_response))\n",
    "    scores['emotion'].append(sp.stats.stats.pearsonr(scores['emotion'][1]['score'],\n",
    "                                                     scores['emotion'][2]['score'])[0])\n",
    "    if verbose:\n",
    "        print('===       EMOTION       ===')\n",
    "        print('context emotions:            \\n', list(zip(scores['emotion'][0]['label'], scores['emotion'][0]['score'])))\n",
    "        print(lbl_text+' emotions:              \\n', list(zip(scores['emotion'][1]['label'], scores['emotion'][1]['score'])))\n",
    "        print(cht_text+' emotions:            \\n', list(zip(scores['emotion'][2]['label'], scores['emotion'][2]['score'])))\n",
    "        print(lbl_text+'-'+cht_text+'emotion corr:  \\n', scores['emotion'][3])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_set_metrics(model, tokenizer, context_sentences, label_responses, chatbot_responses, character, verbose=True,\n",
    "                        classifier_n_sentences=50, include_sentences=False, label_chatbot_symmetry=False):\n",
    "    scores = {}\n",
    "    \n",
    "    lbl_text = 'label' if not label_chatbot_symmetry else 'chatbota'\n",
    "    cht_text = 'chatbot' if not label_chatbot_symmetry else 'chatbotb'\n",
    "    \n",
    "    scores['metadata'] = {}\n",
    "    scores['metadata']['ordering'] = ['context-'+lbl_text,\n",
    "                                      'context-'+cht_text,\n",
    "                                      cht_text+'-'+lbl_text,\n",
    "                                      'ccl']\n",
    "    \n",
    "    # 0) computes metrics for perplexity\n",
    "    metric = BBMetric.load_metric(\"semantic similarity\")\n",
    "    scores['semantic similarity'] = [metric.compute(sentences_a=context_sentences,\n",
    "                                            sentences_b=label_responses)]\n",
    "    scores['semantic similarity'].append(metric.compute(sentences_a=context_sentences,\n",
    "                                            sentences_b=chatbot_responses)),\n",
    "    scores['semantic similarity'].append(metric.compute(sentences_a=label_responses,\n",
    "                                              sentences_b=chatbot_responses))\n",
    "    scores['semantic similarity'].append(ccl_sim(scores['semantic similarity'][0]['score'],\n",
    "                                                 scores['semantic similarity'][1]['score'],\n",
    "                                                 scores['semantic similarity'][2]['score']))\n",
    "    if verbose:\n",
    "        print('=== SEMANTIC SIMILARITY ===')\n",
    "        print('context-'+lbl_text+' similarity:   ', scores['semantic similarity'][0])\n",
    "        print('context-'+cht_text+' similarity: ', scores['semantic similarity'][1])\n",
    "        print(cht_text+'-'+lbl_text+' similarity:   ', scores['semantic similarity'][2])\n",
    "        print('ccl-sim similarity:            ', scores['semantic similarity'][3])\n",
    "    # 1) computes metrics for perplexity\n",
    "    if not label_chatbot_symmetry:\n",
    "        metric = BBMetric.load_metric(\"perplexity\")\n",
    "        scores['perplexity'] = metric.compute(model=model, tokenizer=tokenizer, sentences=chatbot_responses)['score_concat']\n",
    "        if verbose:\n",
    "            print('===       PERPLEXITY     ===')\n",
    "            print('chatbot perplexity:         ', scores['perplexity'])\n",
    "    elif verbose:\n",
    "        print(\"Symmetric mode, skipping Perplexity.\")\n",
    "    # 2) computes metrics for bleu\n",
    "    metric = BBMetric.load_metric(\"bleu\")\n",
    "    scores['bleu'] = [metric.compute(predictions=label_responses, references=context_sentences)]\n",
    "    scores['bleu'].append(metric.compute(predictions=chatbot_responses, references=context_sentences))\n",
    "    scores['bleu'].append(metric.compute(predictions=chatbot_responses, references=label_responses))\n",
    "    scores['bleu'].append(ccl_sim(scores['bleu'][0]['score'],\n",
    "                                  scores['bleu'][1]['score'],\n",
    "                                  scores['bleu'][2]['score']))\n",
    "    if verbose:\n",
    "        print('===         BLEU         ===')\n",
    "        print('context-to-'+lbl_text+' bleu:      ', scores['bleu'][0])\n",
    "        print('context-to-'+cht_text+' bleu:    ', scores['bleu'][1])\n",
    "        print(lbl_text+'-to-'+cht_text+' bleu:      ', scores['bleu'][2])\n",
    "        print('ccl-sim bleu:            ', scores['bleu'][3])\n",
    "    # 3) computes metrics for rouge-L\n",
    "    metric = BBMetric.load_metric(\"rouge l\")\n",
    "    scores['rouge l'] = [metric.compute(predictions=label_responses, references=context_sentences)]\n",
    "    scores['rouge l'].append(metric.compute(predictions=chatbot_responses, references=context_sentences))\n",
    "    scores['rouge l'].append(metric.compute(predictions=chatbot_responses, references=label_responses))\n",
    "    scores['rouge l'].append(ccl_sim(scores['rouge l'][0]['score'],\n",
    "                                     scores['rouge l'][1]['score'],\n",
    "                                     scores['rouge l'][2]['score']))\n",
    "    if verbose:\n",
    "        print('===        ROUGE-L       ===')\n",
    "        print('context-to-'+lbl_text+' rouge:     ', scores['rouge l'][0])\n",
    "        print('context-to-'+cht_text+' rouge:   ', scores['rouge l'][1])\n",
    "        print(lbl_text+'-to-'+cht_text+' rouge:     ', scores['rouge l'][2])\n",
    "        print('ccl-sim rouge:            ', scores['rouge l'][3])\n",
    "    # 4) computes metrics for distinct\n",
    "    metric = BBMetric.load_metric(\"distinct\")\n",
    "    scores['distinct'] = [metric.compute(sentences=context_sentences)]\n",
    "    scores['distinct'].append(metric.compute(sentences=label_responses))\n",
    "    scores['distinct'].append(metric.compute(sentences=chatbot_responses))\n",
    "    scores['distinct'].append(ccl_sim(scores['distinct'][0]['score'],\n",
    "                                      scores['distinct'][1]['score'],\n",
    "                                      scores['distinct'][2]['score']))\n",
    "    if verbose:\n",
    "        print('===       DISTINCT      ===')\n",
    "        print('context distinct:          ', scores['distinct'][0])\n",
    "        print(lbl_text+' distinct:          ', scores['distinct'][1])\n",
    "        print(cht_text+' distinct:          ', scores['distinct'][2])\n",
    "        print('ccl-sim distinct:          ', scores['distinct'][3])\n",
    "    # 6) computes emotion metric\n",
    "    metric = BBMetric.load_metric(\"emotion\")\n",
    "    scores['emotion'] = [metric.compute(sentences=context_sentences)]\n",
    "    scores['emotion'].append(metric.compute(sentences=label_responses))\n",
    "    scores['emotion'].append(metric.compute(sentences=chatbot_responses))\n",
    "    scores['emotion'].append(sp.stats.stats.pearsonr(scores['emotion'][1]['score'],\n",
    "                                                     scores['emotion'][2]['score'])[0])\n",
    "    if verbose:\n",
    "        print('===       EMOTION       ===')\n",
    "        print('context emotions:            \\n', list(zip(scores['emotion'][0]['label'], scores['emotion'][0]['score'])))\n",
    "        print(lbl_text+' emotions:              \\n', list(zip(scores['emotion'][1]['label'], scores['emotion'][1]['score'])))\n",
    "        print(cht_text+' emotions:            \\n', list(zip(scores['emotion'][2]['label'], scores['emotion'][2]['score'])))\n",
    "        print(lbl_text+'-'+cht_text+'emotion corr:  \\n', scores['emotion'][3])\n",
    "    # 8) computes sas metric\n",
    "    metric = BBMetric.load_metric(\"semantic answer similarity\")\n",
    "    scores['semantic answer similarity'] = [metric.compute(predictions=context_sentences,\n",
    "                                                    references=label_responses)]\n",
    "    scores['semantic answer similarity'].append(metric.compute(predictions=context_sentences,\n",
    "                                                        references=chatbot_responses))\n",
    "    scores['semantic answer similarity'].append(metric.compute(predictions=label_responses,\n",
    "                                                        references=chatbot_responses))\n",
    "    scores['semantic answer similarity'].append(ccl_sim(scores['semantic answer similarity'][0]['score'],\n",
    "                                                        scores['semantic answer similarity'][1]['score'],\n",
    "                                                        scores['semantic answer similarity'][2]['score']))\n",
    "    if verbose:\n",
    "        print('===         SAS         ===')\n",
    "        print('context-'+lbl_text+' sas:          ', scores['semantic answer similarity'][0])\n",
    "        print('context-'+cht_text+' sas:        ', scores['semantic answer similarity'][1])\n",
    "        print(lbl_text+'-'+cht_text+' sas:          ', scores['semantic answer similarity'][2])\n",
    "        print('ccl-sim sas:               ', scores['semantic answer similarity'][3])\n",
    "    # 9) computes metrics for semantic classifier\n",
    "    metric = BBMetric.load_metric(\"semantic classifier\")\n",
    "    start_time = time.time()\n",
    "    scores['semantic classifier'] = [metric.compute(character=character, character_dict=character_dict, \n",
    "                                                   base_folder=base_folder, sentences=label_responses,\n",
    "                                                   n_sentences=classifier_n_sentences)]\n",
    "    scores['semantic classifier'].append(metric.compute(character=character, character_dict=character_dict, \n",
    "                                                   base_folder=base_folder, sentences=chatbot_responses,\n",
    "                                                   n_sentences=classifier_n_sentences))\n",
    "    end_time = time.time()\n",
    "    if verbose:\n",
    "        print('=== SEMANTIC CLASSIFIER ===')\n",
    "        print('sem-classifier '+lbl_text+':                ', scores['semantic classifier'][0])\n",
    "        print('sem-classifier '+cht_text+':                  ', scores['semantic classifier'][1])\n",
    "        print('time elapsed computing semantic classifier:  {:.2f} s'.format(end_time - start_time))\n",
    "    if not label_chatbot_symmetry and os.path.exists(os.path.join(os.getcwd(), \"Data\", \"Characters\", character, \"humancoherence.csv\")):\n",
    "        scores['human'] = {}\n",
    "        metric = BBMetric.load_metric(\"human - coherence\")\n",
    "        scores['human']['coherence'] = metric.compute(filepath=os.path.join(os.getcwd(), \"Data\", \"Characters\",\n",
    "                                                                            character, \"humancoherence.csv\"))\n",
    "        metric = BBMetric.load_metric(\"human - style\")\n",
    "        scores['human']['style'] = metric.compute(filepath=os.path.join(os.getcwd(), \"Data\", \"Characters\",\n",
    "                                                                        character, \"humanstyle.csv\"))\n",
    "        metric = BBMetric.load_metric(\"human - consistency\")\n",
    "        scores['human']['consistency'] = metric.compute(filepath=os.path.join(os.getcwd(), \"Data\", \"Characters\",\n",
    "                                                                              character, \"humanconsistency.csv\"))\n",
    "        if verbose:\n",
    "            print('===    HUMAN METRICS    ===')\n",
    "            print('coherence:                 ', scores['human']['coherence'])\n",
    "            print('consistency:               ', scores['human']['consistency'])\n",
    "            print('style:                     ', scores['human']['style'])\n",
    "    elif verbose:\n",
    "        print(\"Symmetric mode, skipping Human metrics.\")\n",
    "    if include_sentences:\n",
    "        sentences_df = {}\n",
    "        sentences_df['context'] = context_sentences\n",
    "        sentences_df[lbl_text] = label_responses\n",
    "        sentences_df[cht_text] = chatbot_responses\n",
    "        scores['sentences'] = sentences_df\n",
    "        if verbose:\n",
    "            print('===      SENTENCES      ===')\n",
    "            for i in range(len(context_sentences)):\n",
    "                print(\"* context: \", context_sentences[i])\n",
    "                print(\"* \" + lbl_text + \":\", label_responses[i])\n",
    "                print(\"* \" + cht_text + \":\", chatbot_responses[i])\n",
    "                print()\n",
    "    elif verbose:\n",
    "        print(\"Skipping sentence outputting.\")\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for i in range(1):\n",
    "    print(\"##### Sample \" + str(i+1) + \" #####\")\n",
    "    context_sentence = df_char['ctx'][i]\n",
    "    chatbot_response = df_char['prd_greedy'][i]\n",
    "    label_response   = df_char['lbl'][i]\n",
    "    compute_sample_metrics(context_sentence, label_response, chatbot_response)\n",
    "    print()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "set_size = 10\n",
    "i = 30\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "print(\"##### Set (Size \" + str(set_size) + \") #####\")\n",
    "context_sentences = list(df_char['ctx'][i:i+set_size])\n",
    "chatbot_responses = list(df_char['prd_greedy'][i:i+set_size])\n",
    "label_responses   = list(df_char['lbl'][i:i+set_size])\n",
    "compute_set_metrics(model, tokenizer,\n",
    "                    context_sentences, label_responses, chatbot_responses, character)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"##### Full Test Set #####\")\n",
    "context_sentences = list(df_char['ctx'])\n",
    "chatbot_responses = list(df_char['prd_greedy'])\n",
    "label_responses   = list(df_char['lbl'])\n",
    "scores = compute_set_metrics(model, tokenizer,\n",
    "                    context_sentences, \n",
    "                    label_responses, \n",
    "                    chatbot_responses,\n",
    "                    character,\n",
    "                    classifier_n_sentences=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_json(metrics_folder, character+'_base_metrics', scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Between Different Sampling Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"##### Greedy vs. N-Beams #####\")\n",
    "context_sentences = list(df_char['ctx'])\n",
    "greedy_responses  = list(df_char['prd_greedy'])\n",
    "nbeams_responses  = list(df_char['prd_nbeams'])\n",
    "scores['greedy_vs_nbeams'] = compute_set_metrics(None, None,\n",
    "                                                 context_sentences,\n",
    "                                                 greedy_responses,\n",
    "                                                 nbeams_responses,\n",
    "                                                 character,\n",
    "                                                 classifier_n_sentences=75,\n",
    "                                                 label_chatbot_symmetry=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split == True:\n",
    "    save_as_json(metrics_folder, character+'_greedy_vs_nbeams_metrics', scores['greedy_vs_nbeams'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"##### Greedy vs. Sampling #####\")\n",
    "context_sentences   = list(df_char['ctx'])\n",
    "greedy_responses    = list(df_char['prd_greedy'])\n",
    "sampling_responses  = list(df_char['prd_sampling'])\n",
    "scores['greedy_vs_sampling'] = compute_set_metrics(None, None,\n",
    "                                                   context_sentences,\n",
    "                                                   greedy_responses,\n",
    "                                                   sampling_responses,\n",
    "                                                   character,\n",
    "                                                   classifier_n_sentences=75,\n",
    "                                                   label_chatbot_symmetry=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split == True:\n",
    "    save_as_json(metrics_folder, character+'_greedy_vs_sampling_metrics', scores['greedy_vs_sampling'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"##### N-Beams vs. Sampling #####\")\n",
    "context_sentences   = list(df_char['ctx'])\n",
    "nbeams_responses    = list(df_char['prd_nbeams'])\n",
    "sampling_responses  = list(df_char['prd_sampling'])\n",
    "scores['nbeams_vs_sampling'] = compute_set_metrics(None, None,\n",
    "                                                   context_sentences,\n",
    "                                                   nbeams_responses,\n",
    "                                                   sampling_responses,\n",
    "                                                   character,\n",
    "                                                   classifier_n_sentences=75,\n",
    "                                                   label_chatbot_symmetry=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split == True:\n",
    "    save_as_json(metrics_folder, character+'_nbeams_vs_sampling_metrics', scores['nbeams_vs_sampling'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split == True:    \n",
    "    scores = {}\n",
    "    scores['greedy_vs_nbeams'] = load_from_json(\n",
    "        filepath=metrics_folder,\n",
    "        filename=character+'_greedy_vs_nbeams_metrics'\n",
    "    )\n",
    "    scores['greedy_vs_sampling'] = load_from_json(\n",
    "        filepath=metrics_folder,\n",
    "        filename=character+'_greedy_vs_sampling_metrics'\n",
    "    )\n",
    "    scores['nbeams_vs_sampling'] = load_from_json(\n",
    "        filepath=metrics_folder,\n",
    "        filename=character+'_nbeams_vs_sampling_metrics'\n",
    "    )\n",
    "    \n",
    "    os.remove(os.path.join(\n",
    "        metrics_folder,\n",
    "        character+'_greedy_vs_nbeams_metrics.json'\n",
    "    ))\n",
    "    os.remove(os.path.join(\n",
    "        metrics_folder,\n",
    "        character+'_greedy_vs_sampling_metrics.json'\n",
    "    ))\n",
    "    os.remove(os.path.join(\n",
    "        metrics_folder,\n",
    "        character+'_nbeams_vs_sampling_metrics.json'\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_json(metrics_folder, character+'_sampling_comparison_metrics', scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Between Non-Finetuned And Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▋                                                                             | 39/1170 [00:43<22:00,  1.17s/it]"
     ]
    }
   ],
   "source": [
    "predictions_def_sampling = get_predictions_cached(sample_questions, model_def,\n",
    "                                                  os.path.join(in_folder_def, 'from_' + character + '_df_' + '_sampling.json'),\n",
    "                                                  \"Sampling\", override_predictions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_char_def = get_dataframe_for_metrics(character_hg['test'], None, None, predictions_def_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for i in range(1):\n",
    "    print(\"##### Sample \" + str(i+1) + \" #####\")\n",
    "    context_sentence   = df_char['ctx'][i]\n",
    "    character_response = df_char['prd_sampling'][i]\n",
    "    default_response   = df_char_def['prd_sampling'][i]\n",
    "    compute_sample_metrics(context_sentence, default_response, character_response, label_chatbot_symmetry=True)\n",
    "    print()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "set_size = 50\n",
    "i = 30\n",
    "print(\"##### Set (Size \" + str(set_size) + \") #####\")\n",
    "context_sentences   = list(df_char['ctx'][i:i+set_size])\n",
    "character_responses = list(df_char['prd_sampling'][i:i+set_size])\n",
    "default_responses   = list(df_char_def['prd_sampling'][i:i+set_size])\n",
    "compute_set_metrics(None, None,\n",
    "                    context_sentences, default_responses, character_responses, character, label_chatbot_symmetry=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"##### Full Test Set #####\")\n",
    "context_sentences   = list(df_char['ctx'])\n",
    "character_responses = list(df_char['prd_sampling'])\n",
    "default_responses   = list(df_char_def['prd_sampling'])\n",
    "scores = compute_set_metrics(None, None,\n",
    "                             context_sentences, \n",
    "                             default_responses, \n",
    "                             character_responses,\n",
    "                             character,\n",
    "                             classifier_n_sentences=75,\n",
    "                             label_chatbot_symmetry=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_json(metrics_folder, character+'_vs_nonfinetuned_metrics', scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Between Character 1 & Character 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_small(sample_questions, model, generation_method):\n",
    "    print(\"Creating predictions\")\n",
    "    predictions = list()\n",
    "    for x in tqdm(sample_questions):\n",
    "        tokenized_question = tokenizer.encode(x + tokenizer.eos_token, return_tensors='tf')\n",
    "        max_length = 128 + tokenized_question.shape[1]\n",
    "        if generation_method == \"Greedy\":\n",
    "            generated_answer = model.generate(tokenized_question,\n",
    "                                pad_token_id=tokenizer.eos_token_id, max_length=max_length)[0].numpy().tolist()\n",
    "        elif generation_method == \"Beam Search\":\n",
    "            generated_answer = model.generate(tokenized_question,\n",
    "                                         pad_token_id=tokenizer.eos_token_id, max_length=max_length,\n",
    "                                         n_beams=n_beams)[0].numpy().tolist()\n",
    "        elif generation_method == \"Sampling\":\n",
    "                b = True\n",
    "                c = 0\n",
    "                while b:\n",
    "                    generated_answer = model.generate(tokenized_question,\n",
    "                                                 pad_token_id=tokenizer.eos_token_id, max_length=max_length,\n",
    "                                                 do_sample=True, top_k=top_k, top_p=top_p)[0].numpy().tolist()\n",
    "                    \n",
    "                    c+= 1\n",
    "                    if len(generated_answer[len(tokenized_question[0]):])>1:\n",
    "                        b = False         \n",
    "                    if c>100: \n",
    "                        generated_answer[len(tokenized_question[0]):] = tokenizer.encode('hi') + [tokenizer.eos_token_id]\n",
    "                        break \n",
    "                        \n",
    "        predictions.append(generated_answer[len(tokenized_question[0]):])\n",
    "        \n",
    "        assert all([len(p)>1 for p in predictions])\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_common = load_dataset('csv',\n",
    "                         data_files=os.path.join(base_folder, 'Data', 'common_dataset.csv'), \n",
    "                         cache_dir=os.path.join(base_folder, \"cache\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_1_sampling = get_predictions_small(df_common['train']['context'], model, \"Sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_2_sampling = get_predictions_small(df_common['train']['context'], model_2, \"Sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_common_char_1 = get_dataframe_for_metrics(df_common['train'], None, None, predictions_1_sampling)\n",
    "df_common_char_2 = get_dataframe_for_metrics(df_common['train'], None, None, predictions_2_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"##### \" + character + \"  Vs. \" + character_2 + \" #####\")\n",
    "context_sentences   = list(df_common_char_1['ctx'])\n",
    "chatbot_responses   = list(df_common_char_1['prd_sampling'])\n",
    "chatbot_2_responses = list(df_common_char_2['prd_sampling'])\n",
    "scores = compute_set_metrics(None, None,\n",
    "                            context_sentences, chatbot_responses, chatbot_2_responses, None,\n",
    "                            include_sentences=True, label_chatbot_symmetry=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_json(metrics_folder, character+'_vs_'+character_2+'_metrics', scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

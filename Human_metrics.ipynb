{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d84a494",
   "metadata": {},
   "source": [
    "# Human Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77e92f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Lib.BBMetrics import BBMetric\n",
    "from transformers import TFAutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "from Data.data_dicts import character_dict, source_dict, random_state\n",
    "\n",
    "import os\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive',force_remount=True)\n",
    "    base_folder = '/content/drive/My Drive/unibo/NLP_project/BarneyBot'\n",
    "    os.system(\"pip install datasets\")\n",
    "    os.system(\"pip install transformers\")\n",
    "else:\n",
    "    base_folder = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45bb9f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small', cache_dir=os.path.join(os.getcwd(), \"cache\"))\n",
    "tokenizer.pad_token = '#'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ecaf833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_Character(character='Default'):\n",
    "    in_folder = os.path.join(base_folder, 'Data', 'Characters', character)\n",
    "    if not os.path.exists(in_folder):\n",
    "        os.makedirs(in_folder)\n",
    "    out_folder = os.path.join(base_folder, 'Data', 'Characters', character)\n",
    "    if not os.path.exists(out_folder):\n",
    "        os.makedirs(out_folder)\n",
    "    checkpoint_folder = os.path.join(out_folder, character_dict[character]['checkpoint_folder'])\n",
    "    model = TFAutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=checkpoint_folder)\n",
    "    \n",
    "    # compute human - coherence\n",
    "    print(\"Step 1) chat with\", character, \"\\n\\tPlease evaluate the chat coherence:\", flush=True)\n",
    "    metric = BBMetric.load_metric(\"human - coherence\")\n",
    "    metric.train(model=model, tokenizer=tokenizer,\n",
    "                 filepath=os.path.join(os.getcwd(), \"Data\", \"Characters\", character, \"humancoherence.csv\"),\n",
    "                 length=5) # length is optional, defaults to 5\n",
    "    metric.compute(filepath=os.path.join(os.getcwd(), \"Data\", \"Characters\", character, \"humancoherence.csv\"))\n",
    "    \n",
    "    # compute human - consistency\n",
    "    print(\"Step 2) chat with\", character, \"\\n\\tPlease evaluate the consistency of the chatbot responses:\", flush=True)\n",
    "    metric = BBMetric.load_metric(\"human - consistency\")\n",
    "    metric.train(model=model, tokenizer=tokenizer,\n",
    "                 filepath=os.path.join(os.getcwd(), \"Datasets\", \"Characters\", character, \"humanconsistency.csv\"))\n",
    "    metric.compute(filepath=os.path.join(os.getcwd(), \"Datasets\", \"Characters\", character, \"humanconsistency.csv\"))\n",
    "    \n",
    "    # compute human - style\n",
    "    print(\"Step 3) chat with\", character, \"\\n\\tPlease evaluate the style of the chatbot responses\", flush=True)\n",
    "    print(\"\\tDo you think they are responses that\", character, \"would say?\", flush=True)\n",
    "    metric = BBMetric.load_metric(\"human - style\")\n",
    "    metric.train(model=model, tokenizer=tokenizer,\n",
    "                 filepath=os.path.join(os.getcwd(), \"Datasets\", \"Characters\", character, \"humanstyle.csv\"),\n",
    "                 questions=barney_sentences)\n",
    "    metric.compute(filepath=os.path.join(os.getcwd(), \"Datasets\", \"Characters\", character, \"humanstyle.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69134ad",
   "metadata": {},
   "source": [
    "# Barney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef410226",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at C:\\Users\\User\\Documents\\GitHub\\Data\\Characters\\Barney\\barney_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1) chat with Barney \n",
      "\tPlease evaluate the chat coherence:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1692/3179349208.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0meval_Character\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcharacter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Barney'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1692/2384707376.py\u001b[0m in \u001b[0;36meval_Character\u001b[1;34m(character)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Step 1) chat with\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcharacter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\n\\tPlease evaluate the chat coherence:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mmetric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBBMetric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"human - coherence\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     metric.train(model=model, tokenizer=tokenizer,\n\u001b[0m\u001b[0;32m     15\u001b[0m                  \u001b[0mfilepath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Data\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Characters\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcharacter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"humancoherence.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                  length=5) # length is optional, defaults to 5\n",
      "\u001b[1;32m~\\Documents\\GitHub\\Lib\\BBMetrics.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    568\u001b[0m                               kwargs['from_saved_embeddings'] if 'from_saved_embeddings' in kwargs else True)\n\u001b[0;32m    569\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"human - coherence\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m             self.metric(kwargs['model'], kwargs['tokenizer'], kwargs['filepath'], True, \n\u001b[0m\u001b[0;32m    571\u001b[0m                         kwargs['length'] if 'length' in kwargs else 5)\n\u001b[0;32m    572\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"human - consistency\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\Lib\\BBMetrics.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(m, t, f, train, l)\u001b[0m\n\u001b[0;32m    441\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"human - coherence\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m             metric = BBMetric(name,\n\u001b[1;32m--> 443\u001b[1;33m                               lambda m, t, f, train, l: human_conversation(m, t, f, train, l))\n\u001b[0m\u001b[0;32m    444\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"human - consistency\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m             metric = BBMetric(name,\n",
      "\u001b[1;32m~\\Documents\\GitHub\\Lib\\BBMetrics.py\u001b[0m in \u001b[0;36mhuman_conversation\u001b[1;34m(model, tokenizer, filepath, train, length)\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m             \u001b[1;31m# encode the new user input, add the eos_token and return a tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m             \u001b[0muser_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\">> User:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m             \u001b[0mchat_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_sentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m             \u001b[0mnew_user_input_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_sentence\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meos_token\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1004\u001b[0m                 \u001b[1;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1005\u001b[0m             )\n\u001b[1;32m-> 1006\u001b[1;33m         return self._input_request(\n\u001b[0m\u001b[0;32m   1007\u001b[0m             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1008\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"shell\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1049\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interrupted by user\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid Message:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "eval_Character(character='Barney')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344f1562",
   "metadata": {},
   "source": [
    "# Sheldon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96beccbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_Character(character='Sheldon')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239f3931",
   "metadata": {},
   "source": [
    "# Harry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d279632a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_Character(character='Harry')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c783c09a",
   "metadata": {},
   "source": [
    "# Fry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b95a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_Character(character='Fry')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91140cbb",
   "metadata": {},
   "source": [
    "# Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81dc50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_Character(character='Vader')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4951c03",
   "metadata": {},
   "source": [
    "# Joey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd27926",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_Character(character='Joey')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd7e940",
   "metadata": {},
   "source": [
    "# Phoebe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6665b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_Character(character='Phoebe')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec57166b",
   "metadata": {},
   "source": [
    "# Bender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65313216",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_Character(character='Bender')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

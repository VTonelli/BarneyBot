{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dde1bf5a",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46f38c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True\n",
    "do_metric_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6aaa0c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pip install -r \"E:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBotGit\\BarneyBot\\requirements.txt\"\n"
     ]
    }
   ],
   "source": [
    "### Run environment setup\n",
    "import os\n",
    "import lib.BBSetup as BBSetup\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    BBSetup.colab_setup(mount_folder=r\"/content/drive/My Drive/unibo/NLP_project/BarneyBot\")\n",
    "except:\n",
    "    try:\n",
    "        BBSetup.anaconda_manual_setup(base_folder=r\"E:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBotGit\\BarneyBot\",\n",
    "                                      env_name=\"barneybot\")\n",
    "    except:\n",
    "        BBSetup.anaconda_auto_setup(base_folder=r\"E:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBotGit\\BarneyBot\")\n",
    "\n",
    "### Define folders\n",
    "base_folder = BBSetup.BASE_FOLDER\n",
    "in_folder = BBSetup.set_folder(os.path.join(base_folder, 'Data', 'Characters'))\n",
    "out_folder = BBSetup.set_folder(os.path.join(base_folder, 'Metrics', 'New'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64468996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Valerio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "### load_char_df() (hg dataset) ['test'] to get testset, containing contexts and response\n",
    "### get_chatbot_predictions() to get a type of predictions for a model\n",
    "from lib.BBDataLoad import load_char_df, get_chatbot_predictions, dialogpt_preprocess_function\n",
    "from datasets import load_dataset\n",
    "from transformers import TFAutoModelForCausalLM\n",
    "from lib.BBMetrics import BBMetric\n",
    "from lib.BBMetricResults import *\n",
    "\n",
    "from lib.BBData import character_dict, model_name, random_state\n",
    "characters = list(character_dict.keys())\n",
    "characters.remove('Default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96c27ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import structures from HuggingFace\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "                                          cache_dir=os.path.join(base_folder, \"cache\"))\n",
    "tokenizer.pad_token = '#'\n",
    "data_collator = DataCollatorForLanguageModeling(mlm=False, tokenizer=tokenizer, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17d247a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Metrics training.\n"
     ]
    }
   ],
   "source": [
    "if do_metric_training:\n",
    "    print(\"Training metrics\")\n",
    "    # Neural Chatbot Classifier\n",
    "    for char in tqdm(characters):\n",
    "        neural_classifier = BBMetric.load_metric(\"neural chatbot classifier\")\n",
    "        neural_classifier.train(character=char, random_state=random_state,\n",
    "                 source_encoded_path=None,\n",
    "                 source_path=os.path.join(base_folder, \"Data\", \"Sources\",\n",
    "                                          character_dict[char]['source'],\n",
    "                                          character_dict[char]['source'] + \".csv\"),\n",
    "                 source_save_path=os.path.join(base_folder, \"Data\", \"Characters\", char),\n",
    "                 save_path=os.path.join(base_folder, \"Data\", \"Characters\", char))\n",
    "    # Distilbert-Embedded Chatbot Classifier\n",
    "    bertembedded_classifier = BBMetric.load_metric(\"distilbert-embedded chatbot classifier\")\n",
    "    raise NotImplementedError(\"Will be soon!\")\n",
    "else:\n",
    "    print(\"Skipping Metrics training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea2ce54",
   "metadata": {},
   "source": [
    "# Cache System Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8285eb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a cache structure to avoid reloading stuff\n",
    "from types import SimpleNamespace\n",
    "\n",
    "cache = {\n",
    "    'dialogpt': {char: None for char in characters + [\"Base\"]},\n",
    "    'tokenizer': None,\n",
    "    'datacollator': None,\n",
    "    'trained_metric': {\n",
    "        'neural chatbot classifier': {char: None for char in characters},\n",
    "        'frequency chatbot classifier': None,\n",
    "        'distilbert-embedded chatbot classifier': None\n",
    "    },\n",
    "    'testset': {char + \"_df\": None for char in characters + [\"Common\"]},\n",
    "    'concat_and_encoded_testset': {char + \"_df\": None for char in characters + [\"Common\"]},\n",
    "    'predictions': {\n",
    "        char + \"_df\": { # Dataset\n",
    "            char: { # Chatbot\n",
    "                'greedy': None,\n",
    "                'nbeams': None,\n",
    "                'sampling': None\n",
    "            } for char in characters + [\"Base\"]\n",
    "        } for char in characters + [\"Common\"]\n",
    "    },\n",
    "}\n",
    "cache = SimpleNamespace(**cache)\n",
    "\n",
    "def load_cache_entry(value, entry):\n",
    "    pointer = cache\n",
    "    for i in range(len(entry)-1):\n",
    "        val = entry[i]\n",
    "        if isinstance(pointer, dict):\n",
    "            pointer = pointer[val]\n",
    "        elif isinstance(pointer, SimpleNamespace):\n",
    "            pointer = pointer.__dict__[val]\n",
    "        else:\n",
    "            raise Exception()\n",
    "    if not pointer[entry[-1]]:\n",
    "        pointer[entry[-1]] = value\n",
    "        if verbose:\n",
    "            print(\"Loaded cache at \" + str(entry))\n",
    "    return pointer[entry[-1]]\n",
    "\n",
    "def flush_cache_entries(entries):\n",
    "    for entry in entries:\n",
    "        pointer = cache\n",
    "        for i in range(len(entry)-1):\n",
    "            val = entry[i]\n",
    "            if isinstance(pointer, dict):\n",
    "                pointer = pointer[val]\n",
    "            elif isinstance(pointer, SimpleNamespace):\n",
    "                pointer = pointer.__dict__[val]\n",
    "            else:\n",
    "                raise Exception()\n",
    "        pointer[entry[-1]] = None\n",
    "        if verbose:\n",
    "            print(\"Flushed cache at \" + str(entry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96df347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cache_testset(character, base_folder):\n",
    "    if not cache.testset[character + \"_df\"]:\n",
    "        if character != \"Common\":\n",
    "            df = load_char_df(character, base_folder)['test']\n",
    "        else: \n",
    "            df = load_dataset('csv',\n",
    "                     data_files=os.path.join(base_folder, 'Data', 'common_dataset.csv'), \n",
    "                     cache_dir=os.path.join(base_folder, \"cache\"))['train']\n",
    "        load_cache_entry(df, ['testset', character + \"_df\"])\n",
    "    return cache.testset[character + \"_df\"]\n",
    "\n",
    "# For perplexity\n",
    "def get_cache_concat_and_encoded_testset(character, base_folder):\n",
    "    if not cache.concat_and_encoded_testset[character + \"_df\"]:\n",
    "        testset = get_cache_testset(character, base_folder)\n",
    "        concat_encoded_testset = testset.map(lambda row: dialogpt_preprocess_function(row,\n",
    "                                                                            cache.tokenizer),\n",
    "                                             batched=False)\n",
    "        concat_encoded_testset = concat_encoded_testset.to_tf_dataset(\n",
    "            columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "            shuffle=False,\n",
    "            batch_size=8,\n",
    "            collate_fn=cache.datacollator,\n",
    "        )\n",
    "        load_cache_entry(concat_encoded_testset, ['concat_and_encoded_testset', character + \"_df\"])\n",
    "    return cache.concat_and_encoded_testset[character + \"_df\"]\n",
    "\n",
    "def get_cache_predictions(dataset_from, character, base_folder, gen_type):\n",
    "    if not cache.predictions[dataset_from][character][gen_type]:\n",
    "        if dataset_from == character + \"_df\":\n",
    "            predictions_tk = get_chatbot_predictions(None, None,\n",
    "                  character_dict[character]['prediction_filename'] + '_' + gen_type + '.json',\n",
    "                  None, character, None, base_folder, override_predictions=False)\n",
    "        elif character == \"Base\":\n",
    "            predictions_tk = get_chatbot_predictions(None, None,\n",
    "                  'from_' + dataset_from + '__' + gen_type + '.json',\n",
    "                  None, 'Default', None, base_folder, override_predictions=False)\n",
    "        elif dataset_from == \"Common_df\" and character != \"Base\":\n",
    "            df = load_dataset('csv',\n",
    "                         data_files=os.path.join(base_folder, 'Data', 'common_dataset.csv'), \n",
    "                         cache_dir=os.path.join(base_folder, \"cache\"))\n",
    "            df = df.remove_columns(['source'])\n",
    "            model = get_cache_model(character)\n",
    "            predictions_tk = get_chatbot_predictions(df['train']['context/0'], model,\n",
    "                  \"\", gen_type, character, cache.tokenizer, base_folder, file_caching=False, override_predictions=False)            \n",
    "        else:\n",
    "            raise NotImplementedError(\"Unexpected predictions to load!\")\n",
    "        predictions = []\n",
    "        for line in predictions_tk:\n",
    "            predictions.append(cache.tokenizer.decode(line, skip_special_tokens=True))\n",
    "        load_cache_entry(predictions, ['predictions', dataset_from, character, gen_type])\n",
    "    return cache.predictions[dataset_from][character][gen_type]\n",
    "\n",
    "# For metrics worth caching, in particular the chatbot classifiers\n",
    "def get_cache_metric(metric_name, **kwargs):\n",
    "    classifier_char = None if 'classifier_char' not in kwargs else kwargs['classifier_char']\n",
    "    mode = 'c-tf-idf' if 'mode' not in kwargs else kwargs['mode']\n",
    "    if metric_name in cache.trained_metric:\n",
    "        if metric_name == \"neural chatbot classifier\":\n",
    "            if not cache.trained_metric[metric_name][classifier_char]:\n",
    "                cache.trained_metric[metric_name][classifier_char] = BBMetric.load_metric(metric_name)\n",
    "                cache.trained_metric[metric_name][classifier_char].compute( # Dummy round for caching\n",
    "                    character=classifier_char,\n",
    "                    load_path=os.path.join(base_folder, \"Data\", \"Characters\",\n",
    "                              classifier_char, character_dict[classifier_char]['classifier_folder']),\n",
    "                    sentences=[\"Hi\", \"Hello\", \"How\"])\n",
    "            return cache.trained_metric[metric_name][classifier_char]\n",
    "        elif metric_name == \"frequency chatbot classifier\":\n",
    "            if not cache.trained_metric[metric_name]:\n",
    "                cache.trained_metric[metric_name] = BBMetric.load_metric(metric_name)\n",
    "                cache.trained_metric[metric_name].train(\n",
    "                    characters_path=os.path.join(base_folder, \"Data\", \"Characters\"),\n",
    "                    mode=mode)\n",
    "            return cache.trained_metric[metric_name]\n",
    "        elif metric_name == \"distilbert-embedded chatbot classifier\":\n",
    "                raise NotImplementedError(\"Will be soon!\")\n",
    "    else:\n",
    "        return BBMetric.load_metric(metric_name)\n",
    "\n",
    "def get_cache_model(character):\n",
    "    if character == \"Base\":\n",
    "        model = TFAutoModelForCausalLM.from_pretrained(model_name, cache_dir=os.path.join(base_folder, \"cache\"))\n",
    "    else:\n",
    "        checkpoint_folder = os.path.join(in_folder, character, character_dict[character]['checkpoint_folder'])\n",
    "        model = TFAutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=checkpoint_folder)\n",
    "    model.compile()\n",
    "    cache.dialogpt[character] = model\n",
    "    return cache.dialogpt[character]\n",
    "\n",
    "cache.tokenizer = tokenizer\n",
    "cache.datacollator = data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc3e480",
   "metadata": {},
   "source": [
    "# Evaluation Process Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8dba10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_callable(reference_set, character, column):\n",
    "    if column == \"context/0\" or column == \"response\":\n",
    "        assert(reference_set == character + \"_df\")\n",
    "        return get_cache_testset(character, base_folder)[column]\n",
    "    else:\n",
    "        assert(reference_set == character + \"_df\" or \\\n",
    "               reference_set == \"Common_df\" or \\\n",
    "               (character == \"Base\" and column == \"sampling\"))\n",
    "        return get_cache_predictions(reference_set, character, base_folder, column)\n",
    "\n",
    "def perplexity_callable(reference_set, character):\n",
    "    return {\n",
    "        'model': get_cache_model(character),\n",
    "        'encoded_test_set': get_cache_concat_and_encoded_testset(reference_set.replace(\"_df\", \"\"),\n",
    "                                                                 base_folder)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15b49468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_round(queries):\n",
    "    actors_pprint_map = {\n",
    "        MetricActor.DATASET_CHAR: \"dataset\",\n",
    "        MetricActor.DATASET_CHARCONTEXT: \"dataset labels\",\n",
    "        MetricActor.DIALOGPT_GREEDY: \"dialogpt (greedy)\",\n",
    "        MetricActor.DIALOGPT_NBEAMS: \"dialogpt (nbeamns)\",\n",
    "        MetricActor.DIALOGPT_SAMPLE: \"dialogpt (sampling)\"\n",
    "    }\n",
    "    actor_to_column_map = {\n",
    "        MetricActor.DATASET_CHARCONTEXT: 'context/0',\n",
    "        MetricActor.DATASET_CHAR: 'response',\n",
    "        MetricActor.DIALOGPT_GREEDY: 'greedy',\n",
    "        MetricActor.DIALOGPT_NBEAMS: 'nbeams',\n",
    "        MetricActor.DIALOGPT_SAMPLE: 'sampling'\n",
    "    }\n",
    "    results = dict()\n",
    "    for i in range(len(queries)):\n",
    "        query = queries[i].copy() # Since there are destructive operations\n",
    "        print(\"#### Running Query \" + str(i+1) + \"/\" + str(len(queries)) + \" ####\")\n",
    "        if 'run' in query:\n",
    "            query['run'](**query['run_args'])\n",
    "        else:\n",
    "            print(\"Evaluating \" + query['metric_name'] + \\\n",
    "                  \" on reference set \" + query['reference_set'] + \" with:\")\n",
    "            for actor_type, actor in query['metric_actors'].items():\n",
    "                print(\"\\t\" + actor[1] + \" \" + actors_pprint_map[actor[0]] + \" as \" + actor_type)\n",
    "            # Get metric metadata data for outputting\n",
    "            query_output = dict()\n",
    "            query_output['metric_name'] = query['metric_name']\n",
    "            query_output['metric_version'] = 1 # It's 1 for all metrics we use, anyway\n",
    "            query_output['metric_attempt'] = 0 if 'metric_attempt' not in query \\\n",
    "                                               else query['metric_attempt']\n",
    "            query_output['metric_actors'] = query['metric_actors']\n",
    "            query_output['metric_params'] = query['metric_params']\n",
    "            query_output['context'] = {\n",
    "                \"dialogpt_size\": \"small\",\n",
    "                \"dialogpt_context_sentences\": 5,\n",
    "                \"dialogpt_nbeams_beams\": 3,\n",
    "                \"dialogpt_sample_top_p\": 0.92,\n",
    "                \"dialogpt_sample_top_k\": 50\n",
    "            }\n",
    "            query_output['metric_arity'] = get_metric_arity(query['metric_name'])\n",
    "            query_output['metric_determinism'] = get_metric_determinism(query['metric_name'],\n",
    "                                                                        query_output['metric_version'])\n",
    "            query_output['reference_set'] = query['reference_set']\n",
    "            query_hash = dict_hash({'metric_name': query_output['metric_name'],\n",
    "                                    'metric_version': query_output['metric_version'],\n",
    "                                    'reference_set': query_output['reference_set'],\n",
    "                                    'metric_attempt': query_output['metric_attempt'],\n",
    "                                    'metric_actors': query_output['metric_actors'],\n",
    "                                    'context': query_output['context'],\n",
    "                                    'metric_params': query_output['metric_params']})\n",
    "            for key in query['metric_actors'].keys(): # Lazy fix for \"_df\" suffix\n",
    "                if query['metric_actors'][key][0] == MetricActor.DATASET_CHARCONTEXT or \\\n",
    "                    query['metric_actors'][key][0] == MetricActor.DATASET_CHAR:\n",
    "                    query['metric_actors'][key] = (query['metric_actors'][key][0],\n",
    "                                                   query['metric_actors'][key][1].replace(\"_df\", \"\"))\n",
    "            # Compute the actual metric\n",
    "            if query['metric_name'] in ['google bleu', 'meteor', 'rouge l', 'mpnet embedding similarity',\n",
    "                            'emotion classifier', 'distinct', 'roberta crossencoding similarity',\n",
    "                            'repetitiveness', 'term error rate', 'bertscore', 'bleurt', 'bartscore',\n",
    "                            'word mover distance', 't5 grammar correction edit distance',\n",
    "                            'extended edit distance']:\n",
    "                args_map = {\n",
    "                    'predictor': 'predictions', 'reference': 'references', 'document': 'sentences',\n",
    "                    'document0': 'sentences_a', 'document1': 'sentences_b'\n",
    "                }\n",
    "                metric = get_cache_metric(query['metric_name'])\n",
    "                args_dict = {}\n",
    "                for actor_key, actor_pair in query['metric_actors'].items():\n",
    "                    args_dict[args_map[actor_key]] = sentence_callable(query['reference_set'],\n",
    "                                                                       actor_pair[1],\n",
    "                                                                       actor_to_column_map[actor_pair[0]])\n",
    "            elif query['metric_name'] == 'comet':\n",
    "                args_map = {\n",
    "                    'predictor': 'predictions', 'reference': 'references', 'document': 'sources'\n",
    "                }\n",
    "                metric = get_cache_metric(query['metric_name'])\n",
    "                args_dict = {}\n",
    "                for actor_key, actor_pair in query['metric_actors'].items():\n",
    "                    args_dict[args_map[actor_key]] = sentence_callable(query['reference_set'],\n",
    "                                                                       actor_pair[1],\n",
    "                                                                       actor_to_column_map[actor_pair[0]])\n",
    "            elif query['metric_name'] in ['perplexity']:\n",
    "                actor_pair = list(query['metric_actors'].values())[0]\n",
    "                metric = get_cache_metric(query['metric_name'])\n",
    "                args_dict = perplexity_callable(query['reference_set'],\n",
    "                                                actor_pair[1])\n",
    "            elif query['metric_name'] in ['frequency chatbot classifier',\n",
    "                                          'distilbert-embedded chatbot classifier']:\n",
    "                actor_pair = list(query['metric_actors'].values())[0]\n",
    "                metric = get_cache_metric(query['metric_name'])\n",
    "                args_dict = {\n",
    "                    'sentences': sentence_callable(query['reference_set'],\n",
    "                                                   actor_pair[1],\n",
    "                                                   actor_to_column_map[actor_pair[0]])\n",
    "                }\n",
    "            elif query['metric_name'] in ['neural chatbot classifier']:\n",
    "                actor_pair = list(query['metric_actors'].values())[0]\n",
    "                classifier_char = query['metric_params']['classifier_char']\n",
    "                args_dict = {\n",
    "                    'character': classifier_char,\n",
    "                    'load_path': os.path.join(base_folder, \"Data\", \"Characters\",\n",
    "                                  classifier_char, character_dict[classifier_char]['classifier_folder']),\n",
    "                }\n",
    "                metric = get_cache_metric(query['metric_name'],\n",
    "                                          classifier_char=classifier_char)\n",
    "                del query['metric_params']['classifier_char']\n",
    "                args_dict['sentences'] = sentence_callable(query['reference_set'],\n",
    "                                                           actor_pair[1],\n",
    "                                                           actor_to_column_map[actor_pair[0]])         \n",
    "            query_output['answer'] = metric.compute(**{**args_dict, **query['metric_params']})\n",
    "            results[query_hash] = query_output\n",
    "                \n",
    "        print()\n",
    "    print(\"Done.\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf531aa",
   "metadata": {},
   "source": [
    "# Run Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3be77e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric Name: See BBMetric.metrics_list\n",
    "# Metric Params: See optional and require params of each metric\n",
    "## NOTE: For neural chatbot classifier, add 'classifier_char' as a parameter\n",
    "# Metric Actors:\n",
    "## DATASET_CHARCONTEXT: (any character | \"Common\") + \"_df\"\n",
    "## DATASET_CHAR: (any character | \"Common\") + \"_df\"\n",
    "## DIALOGPT_GREEDY: any character | \"Base\"\n",
    "## DIALOGPT_NBEAMS: any character | \"Base\"\n",
    "## DIALOGPT_SAMPLE: any character | \"Base\"\n",
    "# Reference Set: (any character | \"Common\") + \"_df\"\n",
    "# Metric Attempt: Defaults to 0, add a number to save multiple runs of the same query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bf0116aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    {\n",
    "        'metric_name': 'bleurt',\n",
    "        'metric_actors': {\n",
    "            'predictor': (MetricActor.DATASET_CHAR, 'Vader_df'),\n",
    "            'reference': (MetricActor.DATASET_CHARCONTEXT, 'Vader_df'),\n",
    "        },\n",
    "        'reference_set': 'Vader_df',\n",
    "        'metric_params': {},\n",
    "        'metric_attempt': 0\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e5925112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Running Query 1/1 ####\n",
      "Evaluating bleurt (on reference set Vader_df) with:\n",
      "\tVader_df dataset as predictor\n",
      "\tVader_df dataset labels as reference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default BLEURT-Base checkpoint for sequence maximum length 128. You can use a bigger model for better results with e.g.: evaluate.load('bleurt', 'bleurt-large-512').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint C:\\Users\\Valerio\\.cache\\huggingface\\metrics\\bleurt\\default\\downloads\\extracted\\3d33e07d20dc36fda3d97eef6258f85c53f0aaa9906a4530ff316c246d91a357\\bleurt-base-128.\n",
      "INFO:tensorflow:Config file found, reading.\n",
      "INFO:tensorflow:Will load checkpoint bert_custom\n",
      "INFO:tensorflow:Loads full paths and checks that files exists.\n",
      "INFO:tensorflow:... name:bert_custom\n",
      "INFO:tensorflow:... vocab_file:vocab.txt\n",
      "INFO:tensorflow:... bert_config_file:bert_config.json\n",
      "INFO:tensorflow:... do_lower_case:True\n",
      "INFO:tensorflow:... max_seq_length:128\n",
      "INFO:tensorflow:Creating BLEURT scorer.\n",
      "INFO:tensorflow:Creating WordPiece tokenizer.\n",
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n",
      "INFO:tensorflow:Creating Eager Mode predictor.\n",
      "INFO:tensorflow:Loading model.\n",
      "INFO:tensorflow:BLEURT initialized.\n",
      "\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'9901c208f2f724ef76bc34e0c39bfaad': {'metric_name': 'bleurt',\n",
       "  'metric_version': 1,\n",
       "  'metric_attempt': 0,\n",
       "  'metric_actors': {'predictor': (<MetricActor.DATASET_CHAR: 1>, 'Vader'),\n",
       "   'reference': (<MetricActor.DATASET_CHARCONTEXT: 0>, 'Vader')},\n",
       "  'metric_params': {},\n",
       "  'context': {'dialogpt_size': 'small',\n",
       "   'dialogpt_context_sentences': 5,\n",
       "   'dialogpt_nbeams_beams': 3,\n",
       "   'dialogpt_sample_top_p': 0.92,\n",
       "   'dialogpt_sample_top_k': 50},\n",
       "  'metric_arity': <MetricArity.PAIRWISE: 2>,\n",
       "  'metric_determinism': <MetricDeterminism.NEURAL: 2>,\n",
       "  'reference_set': 'Vader_df',\n",
       "  'answer': {'score': -1.4269872084259987, 'std': 0.29314456560953456}}}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_round(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6590e05a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

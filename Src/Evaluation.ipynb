{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dde1bf5a",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46f38c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True\n",
    "use_cuda = False\n",
    "\n",
    "do_metric_training = False\n",
    "do_predictions = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6aaa0c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pip install -r \"E:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBotGit\\BarneyBot\\requirements.txt\"\n"
     ]
    }
   ],
   "source": [
    "### Run environment setup\n",
    "import os\n",
    "import lib.BBSetup as BBSetup\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    BBSetup.colab_setup(mount_folder=r\"/content/drive/My Drive/unibo/NLP_project/BarneyBot\")\n",
    "except:\n",
    "    try:\n",
    "        BBSetup.anaconda_manual_setup(base_folder=r\"E:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBotGit\\BarneyBot\",\n",
    "                                      env_name=\"barneybot\")\n",
    "    except:\n",
    "        BBSetup.anaconda_auto_setup(base_folder=r\"E:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBotGit\\BarneyBot\")\n",
    "\n",
    "### Define folders\n",
    "base_folder = BBSetup.BASE_FOLDER\n",
    "in_folder = BBSetup.set_folder(os.path.join(base_folder, 'Data', 'Characters'))\n",
    "out_folder = BBSetup.set_folder(os.path.join(base_folder, 'Metrics', 'New'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64468996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "E:\\Programs\\Anaconda\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tonel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "### load_char_df() (hg dataset) ['test'] to get testset, containing contexts and response\n",
    "### get_chatbot_predictions() to get a type of predictions for a model\n",
    "from lib.BBDataLoad import load_char_df, get_chatbot_predictions, dialogpt_preprocess_function\n",
    "from datasets import load_dataset\n",
    "from transformers import TFAutoModelForCausalLM\n",
    "from lib.BBMetrics import BBMetric\n",
    "from lib.BBMetricResults import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "from lib.BBData import character_dict, model_name, random_state\n",
    "import lib.BBData as BBData\n",
    "characters = list(character_dict.keys())\n",
    "characters.remove('Default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96c27ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import structures from HuggingFace\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "                                          cache_dir=os.path.join(base_folder, \"cache\"))\n",
    "tokenizer.pad_token = '#'\n",
    "data_collator = DataCollatorForLanguageModeling(mlm=False, tokenizer=tokenizer, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57b98f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(S):\n",
    "    if S == []:\n",
    "        return S\n",
    "    if isinstance(S[0], list):\n",
    "        return flatten(S[0]) + flatten(S[1:])\n",
    "    return S[:1] + flatten(S[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55849736",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_predictions:\n",
    "    print(\"Saving predictions to file\")\n",
    "    with tqdm(total=len(characters)*4) as pbar:\n",
    "        # Chatbot of a character on their own dataset\n",
    "        for char in characters:\n",
    "            checkpoint_folder = os.path.join(in_folder, char,\n",
    "                                             character_dict[char]['checkpoint_folder'])\n",
    "            model = TFAutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=checkpoint_folder)\n",
    "            model.compile()\n",
    "            samples = load_char_df(char)\n",
    "            for gen_type in ['greedy', 'nbeams', 'sampling']:\n",
    "                get_chatbot_predictions(samples['test']['context/0'], model,\n",
    "                              character_dict[char]['prediction_filename'] + '_' + gen_type + '.json',\n",
    "                              gen_type, char, cache.tokenizer, base_folder, override_predictions=True)\n",
    "                pbar.update(1)\n",
    "        # Base chatbot on each character's dataset\n",
    "        for char in characters:\n",
    "            model = TFAutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                           cache_dir=os.path.join(base_folder, \"cache\"))\n",
    "            model.compile()\n",
    "            samples = load_char_df(char)\n",
    "            get_chatbot_predictions(samples['test']['context/0'], model,\n",
    "                              'from_' + char + \"_df__sampling.json\", gen_type,\n",
    "                              \"Default\", cache.tokenizer, base_folder, override_predictions=True)\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17d247a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Metrics training.\n"
     ]
    }
   ],
   "source": [
    "if do_metric_training:\n",
    "    print(\"Training metrics\")\n",
    "    # Neural Chatbot Classifier\n",
    "    with tqdm(total=len(characters) + 2) as pbar:\n",
    "        for char in tqdm(characters):\n",
    "            neural_classifier = BBMetric.load_metric(\"neural chatbot classifier\")\n",
    "            neural_classifier.train(character=char, random_state=random_state,\n",
    "                     source_encoded_path=None,\n",
    "                     source_path=os.path.join(base_folder, \"Data\", \"Sources\",\n",
    "                                              character_dict[char]['source'],\n",
    "                                              character_dict[char]['source'] + \".csv\"),\n",
    "                     source_save_path=os.path.join(base_folder, \"Data\", \"Characters\", char),\n",
    "                     save_path=os.path.join(base_folder, \"Data\", \"Characters\", char))\n",
    "            pbar.update(1)\n",
    "        # Distilbert-Embedded Chatbot Classifier\n",
    "        bertembedded_classifier = BBMetric.load_metric(\"distilbert-embedded chatbot classifier\")\n",
    "        bertembedded_classifier.train(characters_path=os.path.join(base_folder, \"Data\", \"Characters\"),\n",
    "                                      save_path=os.path.join(base_folder, \"Data\", \"Metrics\", \n",
    "                                                             \"distilbert_embedder\"),\n",
    "                                      train_embedder=True,\n",
    "                                      verbose=True)\n",
    "        pbar.update(1)\n",
    "        characters_no_barney = characters.copy()\n",
    "        characters_no_barney.remove(\"Barney\")\n",
    "        bertembedded_classifier = BBMetric.load_metric(\"distilbert-embedded chatbot classifier\")\n",
    "        bertembedded_classifier.metric.set_characters(characters_no_barney)\n",
    "        bertembedded_classifier.train(characters_path=os.path.join(base_folder, \"Data\", \"Characters\"),\n",
    "                                      save_path=os.path.join(base_folder, \"Data\", \"Metrics\", \n",
    "                                                             \"distilbert_embedder_nobarney\"),\n",
    "                                      train_embedder=True,\n",
    "                                      verbose=True)\n",
    "        pbar.update(1)\n",
    "else:\n",
    "    print(\"Skipping Metrics training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea2ce54",
   "metadata": {},
   "source": [
    "# Cache System Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8285eb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a cache structure to avoid reloading stuff\n",
    "from types import SimpleNamespace\n",
    "\n",
    "cache = {\n",
    "    'dialogpt': {char: None for char in characters + [\"Base\"]},\n",
    "    'tokenizer': None,\n",
    "    'datacollator': None,\n",
    "    'trained_metric': {\n",
    "        'neural chatbot classifier': {char: None for char in characters},\n",
    "        'frequency chatbot classifier': {'c-tf-idf': None, 'tf-idf': None, 'word frequency': None},\n",
    "        'distilbert-embedded chatbot classifier': {'Full': None, 'No Barney': None}\n",
    "    },\n",
    "    'testset': {char + \"_df\": None for char in characters + [\"Common\"]},\n",
    "    'concat_and_encoded_testset': {char + \"_df\": None for char in characters + [\"Common\"]},\n",
    "    'predictions': {\n",
    "        char + \"_df\": { # Dataset\n",
    "            char: { # Chatbot\n",
    "                'greedy': None,\n",
    "                'nbeams': None,\n",
    "                'sampling': None\n",
    "            } for char in characters + [\"Base\"]\n",
    "        } for char in characters + [\"Common\"]\n",
    "    },\n",
    "}\n",
    "cache = SimpleNamespace(**cache)\n",
    "\n",
    "def load_cache_entry(value, entry):\n",
    "    pointer = cache\n",
    "    for i in range(len(entry)-1):\n",
    "        val = entry[i]\n",
    "        if isinstance(pointer, dict):\n",
    "            pointer = pointer[val]\n",
    "        elif isinstance(pointer, SimpleNamespace):\n",
    "            pointer = pointer.__dict__[val]\n",
    "        else:\n",
    "            raise Exception()\n",
    "    if not pointer[entry[-1]]:\n",
    "        pointer[entry[-1]] = value\n",
    "        if verbose:\n",
    "            print(\"Loaded cache at \" + str(entry))\n",
    "    return pointer[entry[-1]]\n",
    "\n",
    "def flush_cache_entries(entries):\n",
    "    for entry in entries:\n",
    "        pointer = cache\n",
    "        for i in range(len(entry)-1):\n",
    "            val = entry[i]\n",
    "            if isinstance(pointer, dict):\n",
    "                pointer = pointer[val]\n",
    "            elif isinstance(pointer, SimpleNamespace):\n",
    "                pointer = pointer.__dict__[val]\n",
    "            else:\n",
    "                raise Exception()\n",
    "        pointer[entry[-1]] = None\n",
    "        if verbose:\n",
    "            print(\"Flushed cache at \" + str(entry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96df347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cache_testset(character, base_folder):\n",
    "    if not cache.testset[character + \"_df\"]:\n",
    "        if character != \"Common\":\n",
    "            df = load_char_df(character, base_folder)['test']\n",
    "        else: \n",
    "            df = load_dataset('csv',\n",
    "                     data_files=os.path.join(base_folder, 'Data', 'Sources', 'common_dataset.csv'), \n",
    "                     cache_dir=os.path.join(base_folder, \"cache\"))['train']\n",
    "        load_cache_entry(df, ['testset', character + \"_df\"])\n",
    "    return cache.testset[character + \"_df\"]\n",
    "\n",
    "# For perplexity\n",
    "def get_cache_concat_and_encoded_testset(character, base_folder):\n",
    "    if not cache.concat_and_encoded_testset[character + \"_df\"]:\n",
    "        testset = get_cache_testset(character, base_folder)\n",
    "        concat_encoded_testset = testset.map(lambda row: dialogpt_preprocess_function(row,\n",
    "                                                                            cache.tokenizer),\n",
    "                                             batched=False)\n",
    "        concat_encoded_testset = concat_encoded_testset.to_tf_dataset(\n",
    "            columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "            shuffle=False,\n",
    "            batch_size=8,\n",
    "            collate_fn=cache.datacollator,\n",
    "        )\n",
    "        load_cache_entry(concat_encoded_testset, ['concat_and_encoded_testset', character + \"_df\"])\n",
    "    return cache.concat_and_encoded_testset[character + \"_df\"]\n",
    "\n",
    "def get_cache_predictions(dataset_from, character, base_folder, gen_type):\n",
    "    if not cache.predictions[dataset_from][character][gen_type]:\n",
    "        if dataset_from == character + \"_df\":\n",
    "            if character != \"Base\":\n",
    "                predictions_tk = get_chatbot_predictions(None, None,\n",
    "                      character_dict[character]['prediction_filename'] + '_' + gen_type + '.json',\n",
    "                      None, character, None, base_folder, override_predictions=False)\n",
    "            else:\n",
    "                predictions_tk = get_chatbot_predictions(None, None,\n",
    "                      'from_' + dataset_from + '__' + gen_type + '.json',\n",
    "                      None, 'Default', None, base_folder, override_predictions=False)\n",
    "        elif dataset_from == \"Common_df\":\n",
    "            df = load_dataset('csv',\n",
    "                         data_files=os.path.join(base_folder, 'Data', 'Sources', 'common_dataset.csv'), \n",
    "                         cache_dir=os.path.join(base_folder, \"cache\"))\n",
    "            df = df.remove_columns(['source'])\n",
    "            model = get_cache_model(character)\n",
    "            predictions_tk = get_chatbot_predictions(df['train']['context/0'], model,\n",
    "                  \"\", gen_type, character, cache.tokenizer, base_folder, file_caching=False, override_predictions=False)            \n",
    "        else:\n",
    "            raise NotImplementedError(\"Unexpected predictions to load!\")\n",
    "        predictions = []\n",
    "        for line in predictions_tk:\n",
    "            predictions.append(cache.tokenizer.decode(line, skip_special_tokens=True))\n",
    "        load_cache_entry(predictions, ['predictions', dataset_from, character, gen_type])\n",
    "    return cache.predictions[dataset_from][character][gen_type]\n",
    "\n",
    "# For metrics worth caching, in particular the chatbot classifiers\n",
    "def get_cache_metric(metric_name, **kwargs):\n",
    "    classifier_char = None if 'classifier_char' not in kwargs else kwargs['classifier_char']\n",
    "    mode = None if 'mode' not in kwargs else kwargs['mode']\n",
    "    with_barney = None if 'with_barney' not in kwargs else kwargs['with_barney']\n",
    "    with_barney = 'Full' if with_barney else 'No Barney'\n",
    "    if metric_name in cache.trained_metric:\n",
    "        if metric_name == \"neural chatbot classifier\":\n",
    "            if not cache.trained_metric[metric_name][classifier_char]:\n",
    "                cache.trained_metric[metric_name][classifier_char] = BBMetric.load_metric(metric_name)\n",
    "                cache.trained_metric[metric_name][classifier_char].compute( # Dummy round for caching\n",
    "                    character=classifier_char,\n",
    "                    load_path=os.path.join(base_folder, \"Data\", \"Characters\",\n",
    "                              classifier_char, character_dict[classifier_char]['classifier_folder']),\n",
    "                    sentences=[\"Hi\", \"Hello\", \"How\"])\n",
    "            return cache.trained_metric[metric_name][classifier_char]\n",
    "        elif metric_name == \"frequency chatbot classifier\":\n",
    "            if not cache.trained_metric[metric_name][mode]:\n",
    "                cache.trained_metric[metric_name][mode] = BBMetric.load_metric(metric_name)\n",
    "                cache.trained_metric[metric_name][mode].train(\n",
    "                    characters_path=os.path.join(base_folder, \"Data\", \"Characters\"),\n",
    "                    mode=mode)\n",
    "            return cache.trained_metric[metric_name][mode]\n",
    "        elif metric_name == \"distilbert-embedded chatbot classifier\":\n",
    "            if not cache.trained_metric[metric_name][with_barney]:\n",
    "                if with_barney == 'Full':\n",
    "                    cache.trained_metric[metric_name][with_barney] = BBMetric.load_metric(metric_name,\n",
    "                                embedder_path=os.path.join(base_folder, \"Data\", \"Metrics\", \n",
    "                                                           \"distilbert_embedder\"),\n",
    "                                from_pretrained=True, use_cuda=use_cuda)\n",
    "                    cache.trained_metric[metric_name][with_barney].train(\n",
    "                        characters_path=os.path.join(base_folder, \"Data\", \"Characters\"),\n",
    "                        save_path=None, train_embedder=False\n",
    "                    )\n",
    "                else:\n",
    "                    cache.trained_metric[metric_name][with_barney] = BBMetric.load_metric(metric_name,\n",
    "                                embedder_path=os.path.join(base_folder, \"Data\", \"Metrics\", \n",
    "                                                           \"distilbert_embedder_nobarney\"),\n",
    "                                from_pretrained=True, use_cuda=use_cuda)\n",
    "                    cache.trained_metric[metric_name][with_barney].train(\n",
    "                        characters_path=os.path.join(base_folder, \"Data\", \"Characters\"),\n",
    "                        save_path=None, train_embedder=False\n",
    "                    )\n",
    "            return cache.trained_metric[metric_name][with_barney]\n",
    "    else:\n",
    "        return BBMetric.load_metric(metric_name)\n",
    "\n",
    "def get_cache_model(character):\n",
    "    if character == \"Base\":\n",
    "        model = TFAutoModelForCausalLM.from_pretrained(model_name, cache_dir=os.path.join(base_folder, \"cache\"))\n",
    "    else:\n",
    "        checkpoint_folder = os.path.join(in_folder, character, character_dict[character]['checkpoint_folder'])\n",
    "        model = TFAutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=checkpoint_folder)\n",
    "    model.compile()\n",
    "    cache.dialogpt[character] = model\n",
    "    return cache.dialogpt[character]\n",
    "\n",
    "cache.tokenizer = tokenizer\n",
    "cache.datacollator = data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc3e480",
   "metadata": {},
   "source": [
    "# Evaluation Process Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8dba10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_callable(reference_set, character, column):\n",
    "    if column == \"context/0\" or column == \"response\":\n",
    "        assert(reference_set == character + \"_df\")\n",
    "        return get_cache_testset(character, base_folder)[column]\n",
    "    else:\n",
    "        assert(reference_set == character + \"_df\" or \\\n",
    "               reference_set == \"Common_df\" or \\\n",
    "               (character == \"Base\" and column == \"sampling\"))\n",
    "        return get_cache_predictions(reference_set, character, base_folder, column)\n",
    "\n",
    "def perplexity_callable(reference_set, character):\n",
    "    return {\n",
    "        'model': get_cache_model(character),\n",
    "        'encoded_test_set': get_cache_concat_and_encoded_testset(reference_set.replace(\"_df\", \"\"),\n",
    "                                                                 base_folder)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15b49468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_round(queries):\n",
    "    actors_pprint_map = {\n",
    "        MetricActor.DATASET_CHAR: \"dataset\",\n",
    "        MetricActor.DATASET_CHARCONTEXT: \"dataset labels\",\n",
    "        MetricActor.DIALOGPT_GREEDY: \"dialogpt (greedy)\",\n",
    "        MetricActor.DIALOGPT_NBEAMS: \"dialogpt (nbeamns)\",\n",
    "        MetricActor.DIALOGPT_SAMPLE: \"dialogpt (sampling)\"\n",
    "    }\n",
    "    actor_to_column_map = {\n",
    "        MetricActor.DATASET_CHARCONTEXT: 'context/0',\n",
    "        MetricActor.DATASET_CHAR: 'response',\n",
    "        MetricActor.DIALOGPT_GREEDY: 'greedy',\n",
    "        MetricActor.DIALOGPT_NBEAMS: 'nbeams',\n",
    "        MetricActor.DIALOGPT_SAMPLE: 'sampling'\n",
    "    }\n",
    "    results = dict()\n",
    "    for i in range(len(queries)):\n",
    "        try:\n",
    "            query = queries[i].copy() # Since there are destructive operations\n",
    "            print(\"#### Running Query \" + str(i+1) + \"/\" + str(len(queries)) + \" ####\")\n",
    "            if 'run' in query:\n",
    "                query['run'](**query['run_args'])\n",
    "            else:\n",
    "                print(\"Evaluating \" + query['metric_name'] + \\\n",
    "                      \" on reference set \" + query['reference_set'] + \" with:\")\n",
    "                for actor_type, actor in query['metric_actors'].items():\n",
    "                    print(\"\\t\" + actor[1] + \" \" + actors_pprint_map[actor[0]] + \" as \" + actor_type)\n",
    "                # Get metric metadata data for outputting\n",
    "                query_output = dict()\n",
    "                query_output['metric_name'] = query['metric_name']\n",
    "                query_output['metric_version'] = 1 if 'metric_version' not in query else query['metric_version']\n",
    "                query_output['metric_attempt'] = 0 if 'metric_attempt' not in query else query['metric_attempt']\n",
    "                query_output['metric_actors'] = query['metric_actors']\n",
    "                query_output['metric_params'] = query['metric_params']\n",
    "                query_output['context'] = {\n",
    "                    \"dialogpt_size\": \"small\",\n",
    "                    \"dialogpt_context_sentences\": BBData.context_n,\n",
    "                    \"dialogpt_nbeams_beams\": BBData.n_beams,\n",
    "                    \"dialogpt_sample_top_p\": BBData.top_p,\n",
    "                    \"dialogpt_sample_top_k\": BBData.top_k\n",
    "                }\n",
    "                query_output['metric_arity'] = get_metric_arity(query['metric_name'])\n",
    "                query_output['metric_determinism'] = get_metric_determinism(query['metric_name'],\n",
    "                                                                            query_output['metric_version'])\n",
    "                query_output['reference_set'] = query['reference_set']\n",
    "                query_hash = dict_hash({'metric_name': query_output['metric_name'],\n",
    "                                        'metric_version': query_output['metric_version'],\n",
    "                                        'reference_set': query_output['reference_set'],\n",
    "                                        'metric_attempt': query_output['metric_attempt'],\n",
    "                                        'metric_actors': query_output['metric_actors'],\n",
    "                                        'context': query_output['context'],\n",
    "                                        'metric_params': query_output['metric_params']})\n",
    "                for key in query['metric_actors'].keys(): # Lazy fix for \"_df\" suffix\n",
    "                    if query['metric_actors'][key][0] == MetricActor.DATASET_CHARCONTEXT or \\\n",
    "                        query['metric_actors'][key][0] == MetricActor.DATASET_CHAR:\n",
    "                        query['metric_actors'][key] = (query['metric_actors'][key][0],\n",
    "                                                       query['metric_actors'][key][1].replace(\"_df\", \"\"))\n",
    "                # Compute the actual metric\n",
    "                if query['metric_name'] in ['google bleu', 'meteor', 'rouge l', 'mpnet embedding similarity',\n",
    "                                'emotion classifier', 'distinct', 'roberta crossencoding similarity',\n",
    "                                'repetitiveness', 'term error rate', 'bertscore', 'bleurt', 'bartscore',\n",
    "                                'word mover distance', 't5 grammar correction edit distance',\n",
    "                                'extended edit distance', 'flesch-kincaid index']:\n",
    "                    args_map = {\n",
    "                        'predictor': 'predictions', 'reference': 'references', 'document': 'sentences',\n",
    "                        'document0': 'sentences_a', 'document1': 'sentences_b'\n",
    "                    }\n",
    "                    metric = get_cache_metric(query['metric_name'])\n",
    "                    args_dict = {}\n",
    "                    for actor_key, actor_pair in query['metric_actors'].items():\n",
    "                        args_dict[args_map[actor_key]] = sentence_callable(query['reference_set'],\n",
    "                                                                           actor_pair[1],\n",
    "                                                                           actor_to_column_map[actor_pair[0]])\n",
    "                elif query['metric_name'] == 'comet':\n",
    "                    args_map = {\n",
    "                        'predictor': 'predictions', 'reference': 'references', 'document': 'sources'\n",
    "                    }\n",
    "                    metric = get_cache_metric(query['metric_name'])\n",
    "                    args_dict = {}\n",
    "                    for actor_key, actor_pair in query['metric_actors'].items():    \n",
    "                        args_dict[args_map[actor_key]] = sentence_callable(query['reference_set'],\n",
    "                                                                           actor_pair[1],\n",
    "                                                                           actor_to_column_map[actor_pair[0]])\n",
    "                elif query['metric_name'] in ['perplexity']:\n",
    "                    actor_pair = list(query['metric_actors'].values())[0]\n",
    "                    metric = get_cache_metric(query['metric_name'])\n",
    "                    args_dict = perplexity_callable(query['reference_set'],\n",
    "                                                    actor_pair[1])\n",
    "                elif query['metric_name'] in ['frequency chatbot classifier']:\n",
    "                    actor_pair = list(query['metric_actors'].values())[0]\n",
    "                    metric = get_cache_metric(query['metric_name'],\n",
    "                                              mode=query['metric_params']['mode'])\n",
    "                    del query['metric_params']['mode']\n",
    "                    args_dict = {\n",
    "                        'sentences': sentence_callable(query['reference_set'],\n",
    "                                                       actor_pair[1],\n",
    "                                                       actor_to_column_map[actor_pair[0]])\n",
    "                    }\n",
    "                elif query['metric_name'] in ['distilbert-embedded chatbot classifier']:\n",
    "                    actor_pair = list(query['metric_actors'].values())[0]\n",
    "                    metric = get_cache_metric(query['metric_name'],\n",
    "                                              with_barney=query['metric_params']['with_barney'])\n",
    "                    del query['metric_params']['with_barney']\n",
    "                    args_dict = {\n",
    "                        'sentences': sentence_callable(query['reference_set'],\n",
    "                                                       actor_pair[1],\n",
    "                                                       actor_to_column_map[actor_pair[0]])\n",
    "                    }\n",
    "                elif query['metric_name'] in ['neural chatbot classifier']:\n",
    "                    actor_pair = list(query['metric_actors'].values())[0]\n",
    "                    classifier_char = query['metric_params']['classifier_char']\n",
    "                    args_dict = {\n",
    "                        'character': classifier_char,\n",
    "                        'load_path': os.path.join(base_folder, \"Data\", \"Characters\",\n",
    "                                      classifier_char, character_dict[classifier_char]['classifier_folder']),\n",
    "                    }\n",
    "                    metric = get_cache_metric(query['metric_name'],\n",
    "                                              classifier_char=classifier_char)\n",
    "                    del query['metric_params']['classifier_char']\n",
    "                    args_dict['sentences'] = sentence_callable(query['reference_set'],\n",
    "                                                               actor_pair[1],\n",
    "                                                               actor_to_column_map[actor_pair[0]])         \n",
    "                query_output['answer'] = metric.compute(**{**args_dict, **query['metric_params']})\n",
    "                results[query_hash] = query_output\n",
    "        except Exception as e:\n",
    "            print(\"Query failed due to \" + str(type(e)) + \" with message \" + str(e))\n",
    "        print()\n",
    "    print(\"Done.\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf531aa",
   "metadata": {},
   "source": [
    "# Example of Running an Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3be77e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric Name: See BBMetric.metrics_list\n",
    "# Metric Params: See optional and require params of each metric\n",
    "## NOTE: For neural chatbot classifier, add 'classifier_char' as a parameter\n",
    "# Metric Actors:\n",
    "## DATASET_CHARCONTEXT: (any character | \"Common\") + \"_df\"\n",
    "## DATASET_CHAR: (any character | \"Common\") + \"_df\"\n",
    "## DIALOGPT_GREEDY: any character | \"Base\"\n",
    "## DIALOGPT_NBEAMS: any character | \"Base\"\n",
    "## DIALOGPT_SAMPLE: any character | \"Base\"\n",
    "# Reference Set: (any character | \"Common\") + \"_df\"\n",
    "# Metric Attempt: Defaults to 0, add a number to save multiple runs of the same query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf0116aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    {\n",
    "        'metric_name': 'google bleu',\n",
    "        'metric_actors': {\n",
    "            'predictor': (MetricActor.DATASET_CHAR, 'Vader_df'),\n",
    "            'reference': (MetricActor.DATASET_CHARCONTEXT, 'Vader_df'),\n",
    "        },\n",
    "        'reference_set': 'Vader_df',\n",
    "        'metric_params': {},\n",
    "        'metric_attempt': 0\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5925112",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate_round(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a09bb1d",
   "metadata": {},
   "source": [
    "# Run Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f706444d",
   "metadata": {},
   "source": [
    "## Single Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c9fce35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a46d9aa129405187035489deaee3ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/768 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Programs\\Anaconda\\envs\\barneybot\\Lib\\site-packages\\huggingface_hub\\file_download.py:127: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\tonel\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1502bdc2a05b4712839d2119a36aecba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1330644cc51549a2aea1f4ceea33c8f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/291 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68c6e93245074dbe9abc990d149bcf81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df6b2fffde604ed693ac2ca2d8bbc6ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Programs\\Anaconda\\envs\\barneybot\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Running Query 1/26 ####\n",
      "Evaluating emotion classifier on reference set Barney_df with:\n",
      "\tBarney_df dataset as document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-d05c63f64527f593\n",
      "Found cached dataset csv (E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-d05c63f64527f593/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edcdb10906774d2fbf0daf3b73781b0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-d05c63f64527f593/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-e57fb526fe4a3ff3.arrow and E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-d05c63f64527f593/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-54c202230f70778c.arrow\n",
      "Loading cached split indices for dataset at E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-d05c63f64527f593/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-2b7ecddd83bcebad.arrow and E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-d05c63f64527f593/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-f85aa8d6528292e4.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cache at ['testset', 'Barney_df']\n",
      "\n",
      "#### Running Query 2/26 ####\n",
      "Evaluating emotion classifier on reference set Sheldon_df with:\n",
      "\tSheldon_df dataset as document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b47497d241584694\n",
      "Found cached dataset csv (E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-b47497d241584694/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce586f0c93b34c278a7cbdd752f5935b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-b47497d241584694/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-029452f8e061c873.arrow and E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-b47497d241584694/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-c14841bc6fd888d0.arrow\n",
      "Loading cached split indices for dataset at E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-b47497d241584694/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-a95f39f7704e7643.arrow and E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-b47497d241584694/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-3155a76a827528d3.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cache at ['testset', 'Sheldon_df']\n",
      "\n",
      "#### Running Query 3/26 ####\n",
      "Evaluating emotion classifier on reference set Harry_df with:\n",
      "\tHarry_df dataset as document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-8734ab070ca4d4a4\n",
      "Found cached dataset csv (E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-8734ab070ca4d4a4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfffa31e5aa64182a9bbc687bf78afe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-8734ab070ca4d4a4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-6f2c723a222e2152.arrow and E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-8734ab070ca4d4a4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-06309c8f05ebbafb.arrow\n",
      "Loading cached split indices for dataset at E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-8734ab070ca4d4a4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-065178db47afc42d.arrow and E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-8734ab070ca4d4a4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-b63cdf0d40382d55.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cache at ['testset', 'Harry_df']\n",
      "\n",
      "#### Running Query 4/26 ####\n",
      "Evaluating emotion classifier on reference set Fry_df with:\n",
      "\tFry_df dataset as document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-edae583082198a82\n",
      "Found cached dataset csv (E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-edae583082198a82/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d6c1aa83a744844950e324e8efdb097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-edae583082198a82/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-3a40bd254dc1230b.arrow and E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-edae583082198a82/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-76bce446c8545bcd.arrow\n",
      "Loading cached split indices for dataset at E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-edae583082198a82/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-0d49f749e9201c30.arrow and E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-edae583082198a82/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-2361c2a0ed34c32d.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cache at ['testset', 'Fry_df']\n",
      "\n",
      "#### Running Query 5/26 ####\n",
      "Evaluating emotion classifier on reference set Bender_df with:\n",
      "\tBender_df dataset as document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-36d673def5b55b14\n",
      "Found cached dataset csv (E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-36d673def5b55b14/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3c548e51c8541e788246d1df03d6a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-36d673def5b55b14/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-45e9a5cdc4703907.arrow and E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-36d673def5b55b14/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-3d73977ab48375f4.arrow\n",
      "Loading cached split indices for dataset at E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-36d673def5b55b14/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-29c1b91d58842d36.arrow and E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-36d673def5b55b14/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-15d3f645646c43ff.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cache at ['testset', 'Bender_df']\n",
      "\n",
      "#### Running Query 6/26 ####\n",
      "Evaluating emotion classifier on reference set Vader_df with:\n",
      "\tVader_df dataset as document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-3e37c23a51e9d556\n",
      "Found cached dataset csv (E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-3e37c23a51e9d556/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214025ccc80341599017c7eaa4d87a95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-3e37c23a51e9d556/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-6affe80547fc0f38.arrow and E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-3e37c23a51e9d556/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-6800bd34204a2976.arrow\n",
      "Loading cached split indices for dataset at E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-3e37c23a51e9d556/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-78893b2d23cae2d7.arrow and E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-3e37c23a51e9d556/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-38619cd96763b020.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cache at ['testset', 'Vader_df']\n",
      "\n",
      "#### Running Query 7/26 ####\n",
      "Evaluating emotion classifier on reference set Joey_df with:\n",
      "\tJoey_df dataset as document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-bc30ecf942c9a05e\n",
      "Found cached dataset csv (E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-bc30ecf942c9a05e/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a12b2a88119643ac9bb0825c06df4807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-bc30ecf942c9a05e/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-ba044a5350a0b4ee.arrow and E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-bc30ecf942c9a05e/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-27a2b109b2a74d44.arrow\n",
      "Loading cached split indices for dataset at E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-bc30ecf942c9a05e/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-5b9e07b7a11ce3d7.arrow and E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-bc30ecf942c9a05e/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-5910ff0b0ae92adb.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cache at ['testset', 'Joey_df']\n",
      "\n",
      "#### Running Query 8/26 ####\n",
      "Evaluating emotion classifier on reference set Phoebe_df with:\n",
      "\tPhoebe_df dataset as document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-437509a5e3c6484b\n",
      "Found cached dataset csv (E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-437509a5e3c6484b/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c9054c72ada49ac9b5f18819b6eec43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-437509a5e3c6484b/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-f3dedb84963f27fc.arrow and E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-437509a5e3c6484b/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-f69b60477b60f2fe.arrow\n",
      "Loading cached split indices for dataset at E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-437509a5e3c6484b/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-ae641b942c4f1782.arrow and E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-437509a5e3c6484b/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-1b02de643a9ce81e.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cache at ['testset', 'Phoebe_df']\n",
      "\n",
      "#### Running Query 9/26 ####\n",
      "Evaluating emotion classifier on reference set Common_df with:\n",
      "\tCommon_df dataset as document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-9ad41ba7d956ebd3\n",
      "Found cached dataset csv (E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/cache/csv/default-9ad41ba7d956ebd3/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "696cd79ec08d46dd80231e01c9b483f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cache at ['testset', 'Common_df']\n",
      "\n",
      "#### Running Query 10/26 ####\n",
      "Evaluating emotion classifier on reference set Barney_df with:\n",
      "\tBarney dialogpt (sampling) as document\n",
      "Loading predictions from stored file\n",
      "Loaded predictions from stored file\n",
      "Loaded cache at ['predictions', 'Barney_df', 'Barney', 'sampling']\n",
      "\n",
      "#### Running Query 11/26 ####\n",
      "Evaluating emotion classifier on reference set Sheldon_df with:\n",
      "\tSheldon dialogpt (sampling) as document\n",
      "Loading predictions from stored file\n",
      "Loaded predictions from stored file\n",
      "Loaded cache at ['predictions', 'Sheldon_df', 'Sheldon', 'sampling']\n",
      "\n",
      "#### Running Query 12/26 ####\n",
      "Evaluating emotion classifier on reference set Harry_df with:\n",
      "\tHarry dialogpt (sampling) as document\n",
      "Loading predictions from stored file\n",
      "Loaded predictions from stored file\n",
      "Loaded cache at ['predictions', 'Harry_df', 'Harry', 'sampling']\n",
      "\n",
      "#### Running Query 13/26 ####\n",
      "Evaluating emotion classifier on reference set Fry_df with:\n",
      "\tFry dialogpt (sampling) as document\n",
      "Loading predictions from stored file\n",
      "Loaded predictions from stored file\n",
      "Loaded cache at ['predictions', 'Fry_df', 'Fry', 'sampling']\n",
      "\n",
      "#### Running Query 14/26 ####\n",
      "Evaluating emotion classifier on reference set Bender_df with:\n",
      "\tBender dialogpt (sampling) as document\n",
      "Loading predictions from stored file\n",
      "Loaded predictions from stored file\n",
      "Loaded cache at ['predictions', 'Bender_df', 'Bender', 'sampling']\n",
      "\n",
      "#### Running Query 15/26 ####\n",
      "Evaluating emotion classifier on reference set Vader_df with:\n",
      "\tVader dialogpt (sampling) as document\n",
      "Loading predictions from stored file\n",
      "Loaded predictions from stored file\n",
      "Loaded cache at ['predictions', 'Vader_df', 'Vader', 'sampling']\n",
      "\n",
      "#### Running Query 16/26 ####\n",
      "Evaluating emotion classifier on reference set Joey_df with:\n",
      "\tJoey dialogpt (sampling) as document\n",
      "Loading predictions from stored file\n",
      "Loaded predictions from stored file\n",
      "Loaded cache at ['predictions', 'Joey_df', 'Joey', 'sampling']\n",
      "\n",
      "#### Running Query 17/26 ####\n",
      "Evaluating emotion classifier on reference set Phoebe_df with:\n",
      "\tPhoebe dialogpt (sampling) as document\n",
      "Loading predictions from stored file\n",
      "Loaded predictions from stored file\n",
      "Loaded cache at ['predictions', 'Phoebe_df', 'Phoebe', 'sampling']\n",
      "\n",
      "#### Running Query 18/26 ####\n",
      "Evaluating emotion classifier on reference set Common_df with:\n",
      "\tBarney dialogpt (sampling) as document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-9ad41ba7d956ebd3\n",
      "Found cached dataset csv (E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/cache/csv/default-9ad41ba7d956ebd3/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6baf4a6dae294d199684e90db5eff778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at E:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBotGit\\BarneyBot\\Data\\Characters\\Barney\\barney_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/35 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  3%|██▎                                                                                | 1/35 [00:09<05:35,  9.87s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  6%|████▋                                                                              | 2/35 [00:11<02:52,  5.22s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  9%|███████                                                                            | 3/35 [00:14<02:13,  4.18s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 11%|█████████▍                                                                         | 4/35 [00:16<01:44,  3.36s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 14%|███████████▊                                                                       | 5/35 [00:18<01:25,  2.84s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 17%|██████████████▏                                                                    | 6/35 [00:20<01:15,  2.61s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 20%|████████████████▌                                                                  | 7/35 [00:22<01:07,  2.41s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 23%|██████████████████▉                                                                | 8/35 [00:24<00:57,  2.11s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 26%|█████████████████████▎                                                             | 9/35 [00:28<01:13,  2.81s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 29%|███████████████████████▍                                                          | 10/35 [00:33<01:24,  3.37s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 31%|█████████████████████████▊                                                        | 11/35 [00:35<01:10,  2.93s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 34%|████████████████████████████                                                      | 12/35 [00:38<01:11,  3.11s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 37%|██████████████████████████████▍                                                   | 13/35 [00:46<01:37,  4.45s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 40%|████████████████████████████████▊                                                 | 14/35 [00:51<01:36,  4.60s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 43%|███████████████████████████████████▏                                              | 15/35 [00:55<01:30,  4.52s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 46%|█████████████████████████████████████▍                                            | 16/35 [00:59<01:19,  4.19s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 49%|███████████████████████████████████████▊                                          | 17/35 [01:05<01:27,  4.87s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 51%|██████████████████████████████████████████▏                                       | 18/35 [01:07<01:05,  3.88s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 54%|████████████████████████████████████████████▌                                     | 19/35 [01:08<00:50,  3.17s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 57%|██████████████████████████████████████████████▊                                   | 20/35 [01:09<00:39,  2.62s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 60%|█████████████████████████████████████████████████▏                                | 21/35 [01:11<00:31,  2.25s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 63%|███████████████████████████████████████████████████▌                              | 22/35 [01:13<00:28,  2.17s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 66%|█████████████████████████████████████████████████████▉                            | 23/35 [01:15<00:24,  2.07s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 69%|████████████████████████████████████████████████████████▏                         | 24/35 [01:16<00:21,  1.93s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 71%|██████████████████████████████████████████████████████████▌                       | 25/35 [01:18<00:17,  1.74s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 74%|████████████████████████████████████████████████████████████▉                     | 26/35 [01:20<00:17,  1.90s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 77%|███████████████████████████████████████████████████████████████▎                  | 27/35 [01:24<00:20,  2.50s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████████████████████████▌                | 28/35 [01:27<00:19,  2.80s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 83%|███████████████████████████████████████████████████████████████████▉              | 29/35 [01:28<00:13,  2.24s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 86%|██████████████████████████████████████████████████████████████████████▎           | 30/35 [01:34<00:16,  3.25s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 89%|████████████████████████████████████████████████████████████████████████▋         | 31/35 [01:35<00:11,  2.77s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 91%|██████████████████████████████████████████████████████████████████████████▉       | 32/35 [01:40<00:09,  3.26s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 94%|█████████████████████████████████████████████████████████████████████████████▎    | 33/35 [01:41<00:05,  2.58s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 97%|███████████████████████████████████████████████████████████████████████████████▋  | 34/35 [01:42<00:02,  2.24s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 35/35 [01:44<00:00,  2.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cache at ['predictions', 'Common_df', 'Barney', 'sampling']\n",
      "\n",
      "#### Running Query 19/26 ####\n",
      "Evaluating emotion classifier on reference set Common_df with:\n",
      "\tSheldon dialogpt (sampling) as document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-9ad41ba7d956ebd3\n",
      "Found cached dataset csv (E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/cache/csv/default-9ad41ba7d956ebd3/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6931a1c361984db3aef9504099d3c4a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at E:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBotGit\\BarneyBot\\Data\\Characters\\Sheldon\\sheldon_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/35 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  3%|██▎                                                                                | 1/35 [00:08<05:01,  8.86s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  6%|████▋                                                                              | 2/35 [00:09<02:15,  4.10s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  9%|███████                                                                            | 3/35 [00:15<02:33,  4.80s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 11%|█████████▍                                                                         | 4/35 [00:16<01:49,  3.53s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 14%|███████████▊                                                                       | 5/35 [00:23<02:23,  4.78s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 17%|██████████████▏                                                                    | 6/35 [00:27<02:08,  4.43s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 20%|████████████████▌                                                                  | 7/35 [00:30<01:46,  3.80s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 23%|██████████████████▉                                                                | 8/35 [00:34<01:50,  4.11s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 26%|█████████████████████▎                                                             | 9/35 [00:42<02:17,  5.30s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 29%|███████████████████████▍                                                          | 10/35 [00:46<02:02,  4.90s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 31%|█████████████████████████▊                                                        | 11/35 [00:49<01:45,  4.38s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 34%|████████████████████████████                                                      | 12/35 [00:50<01:15,  3.27s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 37%|██████████████████████████████▍                                                   | 13/35 [00:53<01:08,  3.13s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 40%|████████████████████████████████▊                                                 | 14/35 [00:54<00:50,  2.43s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 43%|███████████████████████████████████▏                                              | 15/35 [00:55<00:43,  2.17s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 46%|█████████████████████████████████████▍                                            | 16/35 [00:58<00:43,  2.30s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 49%|███████████████████████████████████████▊                                          | 17/35 [01:04<01:01,  3.39s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 51%|██████████████████████████████████████████▏                                       | 18/35 [01:09<01:04,  3.80s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 54%|████████████████████████████████████████████▌                                     | 19/35 [01:11<00:54,  3.44s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 57%|██████████████████████████████████████████████▊                                   | 20/35 [01:12<00:40,  2.68s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 60%|█████████████████████████████████████████████████▏                                | 21/35 [01:16<00:43,  3.07s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 63%|███████████████████████████████████████████████████▌                              | 22/35 [01:18<00:36,  2.81s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 66%|█████████████████████████████████████████████████████▉                            | 23/35 [01:19<00:26,  2.19s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 69%|████████████████████████████████████████████████████████▏                         | 24/35 [01:22<00:27,  2.52s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 71%|██████████████████████████████████████████████████████████▌                       | 25/35 [01:30<00:40,  4.09s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 74%|████████████████████████████████████████████████████████████▉                     | 26/35 [01:33<00:32,  3.62s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 77%|███████████████████████████████████████████████████████████████▎                  | 27/35 [01:36<00:27,  3.39s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████████████████████████▌                | 28/35 [01:38<00:21,  3.09s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 83%|███████████████████████████████████████████████████████████████████▉              | 29/35 [01:41<00:17,  2.95s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 86%|██████████████████████████████████████████████████████████████████████▎           | 30/35 [01:44<00:15,  3.00s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 89%|████████████████████████████████████████████████████████████████████████▋         | 31/35 [01:49<00:15,  3.77s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 91%|██████████████████████████████████████████████████████████████████████████▉       | 32/35 [01:56<00:14,  4.76s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 94%|█████████████████████████████████████████████████████████████████████████████▎    | 33/35 [01:57<00:07,  3.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 97%|███████████████████████████████████████████████████████████████████████████████▋  | 34/35 [02:02<00:03,  3.91s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 35/35 [02:05<00:00,  3.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cache at ['predictions', 'Common_df', 'Sheldon', 'sampling']\n",
      "\n",
      "#### Running Query 20/26 ####\n",
      "Evaluating emotion classifier on reference set Common_df with:\n",
      "\tHarry dialogpt (sampling) as document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-9ad41ba7d956ebd3\n",
      "Found cached dataset csv (E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/cache/csv/default-9ad41ba7d956ebd3/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91333d139ab046269544041687477cc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at E:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBotGit\\BarneyBot\\Data\\Characters\\Harry\\harry_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/35 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  3%|██▎                                                                                | 1/35 [00:00<00:25,  1.36it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  6%|████▋                                                                              | 2/35 [00:02<00:49,  1.50s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  9%|███████                                                                            | 3/35 [00:03<00:40,  1.27s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 11%|█████████▍                                                                         | 4/35 [00:05<00:51,  1.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 14%|███████████▊                                                                       | 5/35 [00:07<00:53,  1.77s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 17%|██████████████▏                                                                    | 6/35 [00:10<00:54,  1.87s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 20%|████████████████▌                                                                  | 7/35 [00:10<00:43,  1.54s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 23%|██████████████████▉                                                                | 8/35 [00:14<00:58,  2.18s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 26%|█████████████████████▎                                                             | 9/35 [00:16<00:55,  2.14s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 29%|███████████████████████▍                                                          | 10/35 [00:17<00:45,  1.82s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 31%|█████████████████████████▊                                                        | 11/35 [00:19<00:42,  1.78s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 34%|████████████████████████████                                                      | 12/35 [00:20<00:35,  1.54s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 37%|██████████████████████████████▍                                                   | 13/35 [00:21<00:30,  1.36s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 40%|████████████████████████████████▊                                                 | 14/35 [00:22<00:27,  1.31s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 43%|███████████████████████████████████▏                                              | 15/35 [00:24<00:27,  1.39s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 46%|█████████████████████████████████████▍                                            | 16/35 [00:25<00:24,  1.31s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 49%|███████████████████████████████████████▊                                          | 17/35 [00:26<00:21,  1.20s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 51%|██████████████████████████████████████████▏                                       | 18/35 [00:27<00:21,  1.25s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 54%|████████████████████████████████████████████▌                                     | 19/35 [00:29<00:21,  1.34s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 57%|██████████████████████████████████████████████▊                                   | 20/35 [00:31<00:25,  1.71s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 60%|█████████████████████████████████████████████████▏                                | 21/35 [00:36<00:37,  2.71s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 63%|███████████████████████████████████████████████████▌                              | 22/35 [00:38<00:31,  2.42s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 66%|█████████████████████████████████████████████████████▉                            | 23/35 [00:39<00:25,  2.11s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 69%|████████████████████████████████████████████████████████▏                         | 24/35 [00:41<00:21,  1.91s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 71%|██████████████████████████████████████████████████████████▌                       | 25/35 [00:43<00:21,  2.12s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 74%|████████████████████████████████████████████████████████████▉                     | 26/35 [00:45<00:18,  2.07s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 77%|███████████████████████████████████████████████████████████████▎                  | 27/35 [00:47<00:15,  1.92s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████████████████████████▌                | 28/35 [00:50<00:15,  2.19s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 83%|███████████████████████████████████████████████████████████████████▉              | 29/35 [00:52<00:13,  2.29s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 86%|██████████████████████████████████████████████████████████████████████▎           | 30/35 [00:56<00:13,  2.66s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 89%|████████████████████████████████████████████████████████████████████████▋         | 31/35 [00:58<00:10,  2.51s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 91%|██████████████████████████████████████████████████████████████████████████▉       | 32/35 [00:59<00:06,  2.10s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 94%|█████████████████████████████████████████████████████████████████████████████▎    | 33/35 [01:01<00:04,  2.01s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 97%|███████████████████████████████████████████████████████████████████████████████▋  | 34/35 [01:03<00:02,  2.13s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 35/35 [01:04<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cache at ['predictions', 'Common_df', 'Harry', 'sampling']\n",
      "\n",
      "#### Running Query 21/26 ####\n",
      "Evaluating emotion classifier on reference set Common_df with:\n",
      "\tFry dialogpt (sampling) as document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-9ad41ba7d956ebd3\n",
      "Found cached dataset csv (E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/cache/csv/default-9ad41ba7d956ebd3/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace68320fdf34c86b20a85628153d366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at E:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBotGit\\BarneyBot\\Data\\Characters\\Fry\\fry_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/35 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  3%|██▎                                                                                | 1/35 [00:04<02:20,  4.13s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  6%|████▋                                                                              | 2/35 [00:15<04:34,  8.31s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  9%|███████                                                                            | 3/35 [00:18<03:13,  6.03s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 11%|█████████▍                                                                         | 4/35 [00:22<02:36,  5.06s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 14%|███████████▊                                                                       | 5/35 [00:26<02:27,  4.90s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 17%|██████████████▏                                                                    | 6/35 [00:29<01:56,  4.03s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 20%|████████████████▌                                                                  | 7/35 [00:32<01:44,  3.72s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 23%|██████████████████▉                                                                | 8/35 [00:37<01:55,  4.28s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 26%|█████████████████████▎                                                             | 9/35 [00:44<02:12,  5.10s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 29%|███████████████████████▍                                                          | 10/35 [00:47<01:47,  4.29s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 31%|█████████████████████████▊                                                        | 11/35 [00:50<01:37,  4.04s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 34%|████████████████████████████                                                      | 12/35 [00:53<01:21,  3.53s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 37%|██████████████████████████████▍                                                   | 13/35 [00:58<01:29,  4.07s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 40%|████████████████████████████████▊                                                 | 14/35 [01:01<01:19,  3.78s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 43%|███████████████████████████████████▏                                              | 15/35 [01:08<01:35,  4.77s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 46%|█████████████████████████████████████▍                                            | 16/35 [01:10<01:17,  4.07s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 49%|███████████████████████████████████████▊                                          | 17/35 [01:13<01:02,  3.49s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 51%|██████████████████████████████████████████▏                                       | 18/35 [01:15<00:55,  3.24s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 54%|████████████████████████████████████████████▌                                     | 19/35 [01:16<00:41,  2.61s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 57%|██████████████████████████████████████████████▊                                   | 20/35 [01:19<00:39,  2.61s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 60%|█████████████████████████████████████████████████▏                                | 21/35 [01:20<00:30,  2.18s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 63%|███████████████████████████████████████████████████▌                              | 22/35 [01:25<00:37,  2.87s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 66%|█████████████████████████████████████████████████████▉                            | 23/35 [01:28<00:37,  3.15s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 69%|████████████████████████████████████████████████████████▏                         | 24/35 [01:39<00:58,  5.35s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 71%|██████████████████████████████████████████████████████████▌                       | 25/35 [01:45<00:56,  5.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 74%|████████████████████████████████████████████████████████████▉                     | 26/35 [01:52<00:54,  6.06s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 77%|███████████████████████████████████████████████████████████████▎                  | 27/35 [01:54<00:36,  4.60s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████████████████████████▌                | 28/35 [02:01<00:39,  5.61s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 83%|███████████████████████████████████████████████████████████████████▉              | 29/35 [02:04<00:27,  4.53s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 86%|██████████████████████████████████████████████████████████████████████▎           | 30/35 [02:05<00:18,  3.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 89%|████████████████████████████████████████████████████████████████████████▋         | 31/35 [02:11<00:17,  4.25s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 91%|██████████████████████████████████████████████████████████████████████████▉       | 32/35 [02:20<00:17,  5.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 94%|█████████████████████████████████████████████████████████████████████████████▎    | 33/35 [02:22<00:09,  4.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 97%|███████████████████████████████████████████████████████████████████████████████▋  | 34/35 [02:25<00:04,  4.12s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 35/35 [02:28<00:00,  4.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cache at ['predictions', 'Common_df', 'Fry', 'sampling']\n",
      "\n",
      "#### Running Query 22/26 ####\n",
      "Evaluating emotion classifier on reference set Common_df with:\n",
      "\tBender dialogpt (sampling) as document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-9ad41ba7d956ebd3\n",
      "Found cached dataset csv (E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/cache/csv/default-9ad41ba7d956ebd3/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "318d2696bf2b427fb7a516155048fffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at E:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBotGit\\BarneyBot\\Data\\Characters\\Bender\\bender_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/35 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  3%|██▎                                                                                | 1/35 [00:03<02:06,  3.72s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  6%|████▋                                                                              | 2/35 [00:05<01:32,  2.79s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  9%|███████                                                                            | 3/35 [00:09<01:38,  3.06s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 11%|█████████▍                                                                         | 4/35 [00:12<01:43,  3.32s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 14%|███████████▊                                                                       | 5/35 [00:16<01:45,  3.52s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 17%|██████████████▏                                                                    | 6/35 [00:18<01:28,  3.04s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 20%|████████████████▌                                                                  | 7/35 [00:21<01:25,  3.04s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 23%|██████████████████▉                                                                | 8/35 [00:25<01:25,  3.17s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 26%|█████████████████████▎                                                             | 9/35 [00:29<01:28,  3.41s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 29%|███████████████████████▍                                                          | 10/35 [00:39<02:13,  5.34s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 31%|█████████████████████████▊                                                        | 11/35 [00:40<01:41,  4.22s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 34%|████████████████████████████                                                      | 12/35 [00:45<01:41,  4.42s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 37%|██████████████████████████████▍                                                   | 13/35 [00:46<01:16,  3.46s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 40%|████████████████████████████████▊                                                 | 14/35 [00:48<01:03,  3.01s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 43%|███████████████████████████████████▏                                              | 15/35 [00:57<01:33,  4.68s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 46%|█████████████████████████████████████▍                                            | 16/35 [01:01<01:23,  4.38s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 49%|███████████████████████████████████████▊                                          | 17/35 [01:03<01:06,  3.72s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 51%|██████████████████████████████████████████▏                                       | 18/35 [01:07<01:05,  3.86s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 54%|████████████████████████████████████████████▌                                     | 19/35 [01:09<00:52,  3.30s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 57%|██████████████████████████████████████████████▊                                   | 20/35 [01:11<00:42,  2.84s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 60%|█████████████████████████████████████████████████▏                                | 21/35 [01:13<00:38,  2.72s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 63%|███████████████████████████████████████████████████▌                              | 22/35 [01:16<00:36,  2.81s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 66%|█████████████████████████████████████████████████████▉                            | 23/35 [01:21<00:39,  3.31s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 69%|████████████████████████████████████████████████████████▏                         | 24/35 [01:23<00:32,  2.96s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 71%|██████████████████████████████████████████████████████████▌                       | 25/35 [01:26<00:31,  3.19s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 74%|████████████████████████████████████████████████████████████▉                     | 26/35 [01:27<00:22,  2.47s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 77%|███████████████████████████████████████████████████████████████▎                  | 27/35 [01:28<00:15,  1.97s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████████████████████████▌                | 28/35 [01:31<00:15,  2.27s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 83%|███████████████████████████████████████████████████████████████████▉              | 29/35 [01:34<00:15,  2.51s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 86%|██████████████████████████████████████████████████████████████████████▎           | 30/35 [01:38<00:14,  2.92s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 89%|████████████████████████████████████████████████████████████████████████▋         | 31/35 [01:40<00:10,  2.61s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 91%|██████████████████████████████████████████████████████████████████████████▉       | 32/35 [01:47<00:12,  4.07s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 94%|█████████████████████████████████████████████████████████████████████████████▎    | 33/35 [01:52<00:08,  4.38s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 97%|███████████████████████████████████████████████████████████████████████████████▋  | 34/35 [01:55<00:03,  3.71s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 35/35 [01:57<00:00,  3.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cache at ['predictions', 'Common_df', 'Bender', 'sampling']\n",
      "\n",
      "#### Running Query 23/26 ####\n",
      "Evaluating emotion classifier on reference set Common_df with:\n",
      "\tVader dialogpt (sampling) as document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-9ad41ba7d956ebd3\n",
      "Found cached dataset csv (E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/cache/csv/default-9ad41ba7d956ebd3/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca33a60559bc425d8a510a9d0f4c2d05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at E:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBotGit\\BarneyBot\\Data\\Characters\\Vader\\vader_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/35 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  3%|██▎                                                                                | 1/35 [00:01<00:39,  1.16s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  6%|████▋                                                                              | 2/35 [00:06<02:00,  3.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  9%|███████                                                                            | 3/35 [00:09<01:49,  3.41s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 11%|█████████▍                                                                         | 4/35 [00:14<02:05,  4.04s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 14%|███████████▊                                                                       | 5/35 [00:19<02:12,  4.42s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 17%|██████████████▏                                                                    | 6/35 [00:21<01:46,  3.66s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 20%|████████████████▌                                                                  | 7/35 [00:23<01:21,  2.93s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 23%|██████████████████▉                                                                | 8/35 [00:25<01:08,  2.53s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 26%|█████████████████████▎                                                             | 9/35 [00:33<01:51,  4.30s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 29%|███████████████████████▍                                                          | 10/35 [00:36<01:40,  4.01s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 31%|█████████████████████████▊                                                        | 11/35 [00:38<01:17,  3.23s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 34%|████████████████████████████                                                      | 12/35 [00:39<01:03,  2.74s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 37%|██████████████████████████████▍                                                   | 13/35 [00:40<00:50,  2.28s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 40%|████████████████████████████████▊                                                 | 14/35 [00:42<00:41,  1.97s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 43%|███████████████████████████████████▏                                              | 15/35 [00:43<00:36,  1.80s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 46%|█████████████████████████████████████▍                                            | 16/35 [00:46<00:43,  2.27s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 49%|███████████████████████████████████████▊                                          | 17/35 [00:51<00:54,  3.04s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 51%|██████████████████████████████████████████▏                                       | 18/35 [00:57<01:05,  3.87s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 54%|████████████████████████████████████████████▌                                     | 19/35 [01:00<00:58,  3.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 57%|██████████████████████████████████████████████▊                                   | 20/35 [01:04<00:53,  3.55s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 60%|█████████████████████████████████████████████████▏                                | 21/35 [01:06<00:45,  3.26s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 63%|███████████████████████████████████████████████████▌                              | 22/35 [01:08<00:36,  2.81s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 66%|█████████████████████████████████████████████████████▉                            | 23/35 [01:10<00:31,  2.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 69%|████████████████████████████████████████████████████████▏                         | 24/35 [01:12<00:24,  2.27s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 71%|██████████████████████████████████████████████████████████▌                       | 25/35 [01:16<00:28,  2.86s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 74%|████████████████████████████████████████████████████████████▉                     | 26/35 [01:18<00:24,  2.74s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 77%|███████████████████████████████████████████████████████████████▎                  | 27/35 [01:19<00:17,  2.15s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████████████████████████▌                | 28/35 [01:21<00:14,  2.12s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 83%|███████████████████████████████████████████████████████████████████▉              | 29/35 [01:24<00:15,  2.50s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 86%|██████████████████████████████████████████████████████████████████████▎           | 30/35 [01:28<00:13,  2.76s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 89%|████████████████████████████████████████████████████████████████████████▋         | 31/35 [01:28<00:08,  2.15s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 91%|██████████████████████████████████████████████████████████████████████████▉       | 32/35 [01:31<00:06,  2.15s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 94%|█████████████████████████████████████████████████████████████████████████████▎    | 33/35 [01:33<00:04,  2.33s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 97%|███████████████████████████████████████████████████████████████████████████████▋  | 34/35 [01:38<00:03,  3.05s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 35/35 [01:39<00:00,  2.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cache at ['predictions', 'Common_df', 'Vader', 'sampling']\n",
      "\n",
      "#### Running Query 24/26 ####\n",
      "Evaluating emotion classifier on reference set Common_df with:\n",
      "\tJoey dialogpt (sampling) as document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-9ad41ba7d956ebd3\n",
      "Found cached dataset csv (E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/cache/csv/default-9ad41ba7d956ebd3/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e44335eb8e854374bc92a2715b070e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at E:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBotGit\\BarneyBot\\Data\\Characters\\Joey\\joey_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/35 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  3%|██▎                                                                                | 1/35 [00:02<01:08,  2.02s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  6%|████▋                                                                              | 2/35 [00:05<01:28,  2.69s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  9%|███████                                                                            | 3/35 [00:07<01:24,  2.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 11%|█████████▍                                                                         | 4/35 [00:09<01:07,  2.19s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 14%|███████████▊                                                                       | 5/35 [00:10<00:50,  1.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 17%|██████████████▏                                                                    | 6/35 [00:10<00:39,  1.35s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 20%|████████████████▌                                                                  | 7/35 [00:12<00:43,  1.56s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 23%|██████████████████▉                                                                | 8/35 [00:27<02:38,  5.86s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 26%|█████████████████████▎                                                             | 9/35 [00:29<01:59,  4.60s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 29%|███████████████████████▍                                                          | 10/35 [00:32<01:40,  4.04s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 31%|█████████████████████████▊                                                        | 11/35 [00:38<01:49,  4.57s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 34%|████████████████████████████                                                      | 12/35 [00:42<01:43,  4.52s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 37%|██████████████████████████████▍                                                   | 13/35 [00:43<01:15,  3.45s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 40%|████████████████████████████████▊                                                 | 14/35 [00:45<01:02,  2.97s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 43%|███████████████████████████████████▏                                              | 15/35 [00:46<00:46,  2.31s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 46%|█████████████████████████████████████▍                                            | 16/35 [00:51<01:03,  3.34s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 49%|███████████████████████████████████████▊                                          | 17/35 [00:52<00:46,  2.60s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 51%|██████████████████████████████████████████▏                                       | 18/35 [00:54<00:37,  2.22s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 54%|████████████████████████████████████████████▌                                     | 19/35 [00:55<00:30,  1.93s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 57%|██████████████████████████████████████████████▊                                   | 20/35 [01:01<00:47,  3.16s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 60%|█████████████████████████████████████████████████▏                                | 21/35 [01:02<00:34,  2.45s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 63%|███████████████████████████████████████████████████▌                              | 22/35 [01:03<00:27,  2.13s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 66%|█████████████████████████████████████████████████████▉                            | 23/35 [01:06<00:27,  2.32s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 69%|████████████████████████████████████████████████████████▏                         | 24/35 [01:11<00:35,  3.21s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 71%|██████████████████████████████████████████████████████████▌                       | 25/35 [01:16<00:38,  3.84s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 74%|████████████████████████████████████████████████████████████▉                     | 26/35 [01:19<00:29,  3.30s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 77%|███████████████████████████████████████████████████████████████▎                  | 27/35 [01:21<00:23,  2.95s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████████████████████████▌                | 28/35 [01:24<00:21,  3.02s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 83%|███████████████████████████████████████████████████████████████████▉              | 29/35 [01:25<00:14,  2.37s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 86%|██████████████████████████████████████████████████████████████████████▎           | 30/35 [01:27<00:11,  2.34s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 89%|████████████████████████████████████████████████████████████████████████▋         | 31/35 [01:28<00:07,  1.93s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 91%|██████████████████████████████████████████████████████████████████████████▉       | 32/35 [01:29<00:04,  1.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 94%|█████████████████████████████████████████████████████████████████████████████▎    | 33/35 [01:34<00:05,  2.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 97%|███████████████████████████████████████████████████████████████████████████████▋  | 34/35 [01:41<00:03,  3.94s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 35/35 [01:45<00:00,  3.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cache at ['predictions', 'Common_df', 'Joey', 'sampling']\n",
      "\n",
      "#### Running Query 25/26 ####\n",
      "Evaluating emotion classifier on reference set Common_df with:\n",
      "\tPhoebe dialogpt (sampling) as document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-9ad41ba7d956ebd3\n",
      "Found cached dataset csv (E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/cache/csv/default-9ad41ba7d956ebd3/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce0539db4674104a614c4e1065671c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at E:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBotGit\\BarneyBot\\Data\\Characters\\Phoebe\\phoebe_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/35 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  3%|██▎                                                                                | 1/35 [00:12<07:13, 12.74s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  6%|████▋                                                                              | 2/35 [00:14<03:27,  6.30s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  9%|███████                                                                            | 3/35 [00:16<02:12,  4.13s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 11%|█████████▍                                                                         | 4/35 [00:20<02:05,  4.06s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 14%|███████████▊                                                                       | 5/35 [00:25<02:13,  4.45s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 17%|██████████████▏                                                                    | 6/35 [00:25<01:32,  3.19s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 20%|████████████████▌                                                                  | 7/35 [00:28<01:21,  2.93s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 23%|██████████████████▉                                                                | 8/35 [00:33<01:42,  3.78s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 26%|█████████████████████▎                                                             | 9/35 [00:35<01:21,  3.14s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 29%|███████████████████████▍                                                          | 10/35 [00:40<01:31,  3.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 31%|█████████████████████████▊                                                        | 11/35 [00:43<01:26,  3.60s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 34%|████████████████████████████                                                      | 12/35 [00:49<01:33,  4.06s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 37%|██████████████████████████████▍                                                   | 13/35 [00:49<01:07,  3.09s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 40%|████████████████████████████████▊                                                 | 14/35 [00:54<01:15,  3.59s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 43%|███████████████████████████████████▏                                              | 15/35 [01:00<01:25,  4.26s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 46%|█████████████████████████████████████▍                                            | 16/35 [01:03<01:12,  3.83s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 49%|███████████████████████████████████████▊                                          | 17/35 [01:04<00:54,  3.02s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 51%|██████████████████████████████████████████▏                                       | 18/35 [01:06<00:48,  2.83s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 54%|████████████████████████████████████████████▌                                     | 19/35 [01:09<00:43,  2.75s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 57%|██████████████████████████████████████████████▊                                   | 20/35 [01:11<00:39,  2.62s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 60%|█████████████████████████████████████████████████▏                                | 21/35 [01:13<00:34,  2.49s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 63%|███████████████████████████████████████████████████▌                              | 22/35 [01:28<01:20,  6.20s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 66%|█████████████████████████████████████████████████████▉                            | 23/35 [01:30<00:59,  4.99s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 69%|████████████████████████████████████████████████████████▏                         | 24/35 [01:31<00:41,  3.74s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 71%|██████████████████████████████████████████████████████████▌                       | 25/35 [01:33<00:31,  3.12s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 74%|████████████████████████████████████████████████████████████▉                     | 26/35 [01:43<00:47,  5.24s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 77%|███████████████████████████████████████████████████████████████▎                  | 27/35 [01:44<00:31,  3.91s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████████████████████████▌                | 28/35 [01:48<00:26,  3.85s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 83%|███████████████████████████████████████████████████████████████████▉              | 29/35 [01:50<00:19,  3.28s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 86%|██████████████████████████████████████████████████████████████████████▎           | 30/35 [01:52<00:14,  2.97s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 89%|████████████████████████████████████████████████████████████████████████▋         | 31/35 [01:53<00:10,  2.56s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 91%|██████████████████████████████████████████████████████████████████████████▉       | 32/35 [01:57<00:08,  2.88s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 94%|█████████████████████████████████████████████████████████████████████████████▎    | 33/35 [01:58<00:04,  2.26s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 97%|███████████████████████████████████████████████████████████████████████████████▋  | 34/35 [02:01<00:02,  2.43s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 35/35 [02:09<00:00,  3.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cache at ['predictions', 'Common_df', 'Phoebe', 'sampling']\n",
      "\n",
      "#### Running Query 26/26 ####\n",
      "Evaluating emotion classifier on reference set Common_df with:\n",
      "\tBase dialogpt (sampling) as document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-9ad41ba7d956ebd3\n",
      "Found cached dataset csv (E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/cache/csv/default-9ad41ba7d956ebd3/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea3da4e0d7b46e48c12813c8f851abe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at microsoft/DialoGPT-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/35 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  3%|██▎                                                                                | 1/35 [00:02<01:12,  2.12s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  6%|████▋                                                                              | 2/35 [00:04<01:09,  2.12s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  9%|███████                                                                            | 3/35 [00:09<01:58,  3.69s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 11%|█████████▍                                                                         | 4/35 [00:14<02:09,  4.17s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 14%|███████████▊                                                                       | 5/35 [00:16<01:34,  3.13s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 17%|██████████████▏                                                                    | 6/35 [00:19<01:32,  3.18s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 20%|████████████████▌                                                                  | 7/35 [00:20<01:09,  2.50s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 23%|██████████████████▉                                                                | 8/35 [00:23<01:14,  2.76s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 26%|█████████████████████▎                                                             | 9/35 [00:27<01:23,  3.20s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 29%|███████████████████████▍                                                          | 10/35 [00:32<01:34,  3.79s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 31%|█████████████████████████▊                                                        | 11/35 [00:35<01:21,  3.41s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 34%|████████████████████████████                                                      | 12/35 [00:37<01:11,  3.13s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 37%|██████████████████████████████▍                                                   | 13/35 [00:40<01:06,  3.04s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 40%|████████████████████████████████▊                                                 | 14/35 [00:45<01:14,  3.55s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 43%|███████████████████████████████████▏                                              | 15/35 [00:47<00:58,  2.92s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 46%|█████████████████████████████████████▍                                            | 16/35 [00:51<01:06,  3.51s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 49%|███████████████████████████████████████▊                                          | 17/35 [00:54<01:00,  3.33s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 51%|██████████████████████████████████████████▏                                       | 18/35 [01:00<01:06,  3.93s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 54%|████████████████████████████████████████████▌                                     | 19/35 [01:03<01:01,  3.84s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 57%|██████████████████████████████████████████████▊                                   | 20/35 [01:05<00:49,  3.31s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 60%|█████████████████████████████████████████████████▏                                | 21/35 [01:11<00:56,  4.07s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 63%|███████████████████████████████████████████████████▌                              | 22/35 [01:12<00:40,  3.14s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 66%|█████████████████████████████████████████████████████▉                            | 23/35 [01:16<00:41,  3.44s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 69%|████████████████████████████████████████████████████████▏                         | 24/35 [01:18<00:31,  2.85s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 71%|██████████████████████████████████████████████████████████▌                       | 25/35 [01:22<00:32,  3.21s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 74%|████████████████████████████████████████████████████████████▉                     | 26/35 [01:24<00:26,  2.94s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 77%|███████████████████████████████████████████████████████████████▎                  | 27/35 [01:25<00:18,  2.30s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████████████████████████▌                | 28/35 [01:28<00:16,  2.39s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 83%|███████████████████████████████████████████████████████████████████▉              | 29/35 [01:29<00:13,  2.18s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 86%|██████████████████████████████████████████████████████████████████████▎           | 30/35 [01:31<00:09,  1.98s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 89%|████████████████████████████████████████████████████████████████████████▋         | 31/35 [01:32<00:07,  1.89s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 91%|██████████████████████████████████████████████████████████████████████████▉       | 32/35 [01:34<00:05,  1.77s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 94%|█████████████████████████████████████████████████████████████████████████████▎    | 33/35 [01:35<00:03,  1.69s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 97%|███████████████████████████████████████████████████████████████████████████████▋  | 34/35 [01:37<00:01,  1.55s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 35/35 [01:42<00:00,  2.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cache at ['predictions', 'Common_df', 'Base', 'sampling']\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#'distinct', 'repetitiveness', 't5 grammar correction edit distance', 'flesch-kincaid index'\n",
    "#'frequency chatbot classifier'\n",
    "\n",
    "# 'distilbert-embedded chatbot classifier'\n",
    "\n",
    "for metric in ['emotion classifier']:\n",
    "    metric_pretty = BBMetric.load_metric(metric).pretty_name\n",
    "    metric_params = dict()\n",
    "    if metric == \"distilbert-embedded chatbot classifier\":\n",
    "        metric_params = {'with_barney': True}\n",
    "    elif metric == \"frequency chatbot classifier\":\n",
    "        metric_params = {'mode': 'c-tf-idf'}\n",
    "    results = evaluate_round([\n",
    "        {\n",
    "            'metric_name': metric,\n",
    "            'metric_actors': {\n",
    "                'document': (MetricActor.DATASET_CHAR, char + '_df')\n",
    "            },\n",
    "            'reference_set': char + '_df',\n",
    "            'metric_params': metric_params.copy(),\n",
    "            'metric_attempt': 0\n",
    "        } for char in characters + [\"Common\"]\n",
    "    ] + [\n",
    "        {\n",
    "            'metric_name': metric,\n",
    "            'metric_actors': {\n",
    "                'document': (MetricActor.DIALOGPT_SAMPLE, char)\n",
    "            },\n",
    "            'reference_set': char + '_df',\n",
    "            'metric_params': metric_params.copy(),\n",
    "            'metric_attempt': 0\n",
    "        } for char in characters\n",
    "    ] + [\n",
    "        {\n",
    "            'metric_name': metric,\n",
    "            'metric_actors': {\n",
    "                'document': (MetricActor.DIALOGPT_SAMPLE, char)\n",
    "            },\n",
    "            'reference_set': 'Common_df',\n",
    "            'metric_params': metric_params.copy(),\n",
    "            'metric_attempt': 0\n",
    "        } for char in characters + [\"Base\"]\n",
    "    ])\n",
    "    metric_dict = load_metric_by_name(out_folder, metric_pretty)\n",
    "    metric_dict = {**metric_dict, **results}\n",
    "    save_metric_by_name(out_folder, metric_pretty, metric_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a7e1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'neural chatbot classifier'\n",
    "metric_pretty = BBMetric.load_metric(metric).pretty_name\n",
    "metric_params = dict()\n",
    "results = evaluate_round(flatten([[\n",
    "        {\n",
    "            'metric_name': metric,\n",
    "            'metric_actors': {\n",
    "                'document': (MetricActor.DATASET_CHAR, char + '_df')\n",
    "            },\n",
    "            'reference_set': char + '_df',\n",
    "            'metric_params': {'classifier_char': char},\n",
    "            'metric_attempt': 0\n",
    "        },\n",
    "        {\n",
    "            'metric_name': metric,\n",
    "            'metric_actors': {\n",
    "                'document': (MetricActor.DIALOGPT_SAMPLE, char)\n",
    "            },\n",
    "            'reference_set': char + '_df',\n",
    "            'metric_params': {'classifier_char': char},\n",
    "            'metric_attempt': 0\n",
    "        },\n",
    "        {\n",
    "            'metric_name': metric,\n",
    "            'metric_actors': {\n",
    "                'document': (MetricActor.DIALOGPT_SAMPLE, char)\n",
    "            },\n",
    "            'reference_set': 'Common_df',\n",
    "            'metric_params': {'classifier_char': char},\n",
    "            'metric_attempt': 0\n",
    "        },\n",
    "        {\n",
    "            'run': flush_cache_entries,\n",
    "            'run_args': {\n",
    "                'entries': [['trained_metric', 'neural chatbot classifier', char]]\n",
    "            }\n",
    "        }\n",
    "] for char in characters\n",
    "]))\n",
    "metric_dict = load_metric_by_name(out_folder, metric_pretty)\n",
    "metric_dict = {**metric_dict, **results}\n",
    "save_metric_by_name(out_folder, metric_pretty, metric_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "22640fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Running Query 1/9 ####\n",
      "Evaluating perplexity on reference set Common_df with:\n",
      "\tBarney dialogpt (sampling) as document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at E:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBotGit\\BarneyBot\\Data\\Characters\\Barney\\barney_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n",
      "WARNING:datasets.builder:Using custom data configuration default-9ad41ba7d956ebd3\n",
      "WARNING:datasets.builder:Found cached dataset csv (E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/cache/csv/default-9ad41ba7d956ebd3/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c024bbf68734081ac04593dc947e85a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cache at ['testset', 'Common_df']\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\\tfgpt2_main_layer\n",
      "......vars\n",
      ".........0\n",
      "...layers\\tfgpt2_main_layer\\drop\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block\\attn\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block\\attn\\attn_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block\\attn\\c_attn\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block\\attn\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block\\attn\\resid_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block\\ln_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block\\ln_2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block\\mlp\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block\\mlp\\c_fc\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block\\mlp\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block\\mlp\\dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_1\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_10\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_10\\attn\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_10\\attn\\attn_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_10\\attn\\c_attn\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_10\\attn\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_10\\attn\\resid_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_10\\ln_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_10\\ln_2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_10\\mlp\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_10\\mlp\\c_fc\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_10\\mlp\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_10\\mlp\\dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_11\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_11\\attn\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_11\\attn\\attn_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_11\\attn\\c_attn\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_11\\attn\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_11\\attn\\resid_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_11\\ln_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_11\\ln_2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_11\\mlp\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_11\\mlp\\c_fc\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_11\\mlp\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_11\\mlp\\dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_1\\attn\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_1\\attn\\attn_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_1\\attn\\c_attn\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_1\\attn\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_1\\attn\\resid_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_1\\ln_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_1\\ln_2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_1\\mlp\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_1\\mlp\\c_fc\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_1\\mlp\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_1\\mlp\\dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_2\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_2\\attn\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_2\\attn\\attn_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_2\\attn\\c_attn\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_2\\attn\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_2\\attn\\resid_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_2\\ln_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_2\\ln_2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_2\\mlp\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_2\\mlp\\c_fc\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_2\\mlp\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_2\\mlp\\dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_3\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_3\\attn\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_3\\attn\\attn_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_3\\attn\\c_attn\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_3\\attn\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_3\\attn\\resid_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_3\\ln_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_3\\ln_2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_3\\mlp\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_3\\mlp\\c_fc\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_3\\mlp\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_3\\mlp\\dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_4\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_4\\attn\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_4\\attn\\attn_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_4\\attn\\c_attn\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_4\\attn\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_4\\attn\\resid_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_4\\ln_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_4\\ln_2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_4\\mlp\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_4\\mlp\\c_fc\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_4\\mlp\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_4\\mlp\\dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_5\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_5\\attn\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_5\\attn\\attn_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_5\\attn\\c_attn\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_5\\attn\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_5\\attn\\resid_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_5\\ln_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_5\\ln_2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_5\\mlp\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_5\\mlp\\c_fc\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_5\\mlp\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_5\\mlp\\dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_6\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_6\\attn\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_6\\attn\\attn_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_6\\attn\\c_attn\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_6\\attn\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_6\\attn\\resid_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_6\\ln_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_6\\ln_2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_6\\mlp\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_6\\mlp\\c_fc\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_6\\mlp\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_6\\mlp\\dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_7\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_7\\attn\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_7\\attn\\attn_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_7\\attn\\c_attn\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_7\\attn\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_7\\attn\\resid_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_7\\ln_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_7\\ln_2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_7\\mlp\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_7\\mlp\\c_fc\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_7\\mlp\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_7\\mlp\\dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_8\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_8\\attn\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_8\\attn\\attn_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_8\\attn\\c_attn\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_8\\attn\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_8\\attn\\resid_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_8\\ln_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_8\\ln_2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_8\\mlp\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_8\\mlp\\c_fc\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_8\\mlp\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_8\\mlp\\dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_9\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_9\\attn\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_9\\attn\\attn_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_9\\attn\\c_attn\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_9\\attn\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_9\\attn\\resid_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_9\\ln_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_9\\ln_2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_9\\mlp\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_9\\mlp\\c_fc\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_9\\mlp\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_9\\mlp\\dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\ln_f\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\wte\n",
      "......vars\n",
      ".........0\n",
      "...optimizer\n",
      "......vars\n",
      ".........0\n",
      "...vars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2022-12-24 18:22:21         2297\n",
      "metadata.json                                  2022-12-24 18:22:21           64\n",
      "variables.h5                                   2022-12-24 18:22:24    498110488\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\\tfgpt2_main_layer\n",
      "......vars\n",
      ".........0\n",
      "...layers\\tfgpt2_main_layer\\drop\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block\\attn\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block\\attn\\attn_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block\\attn\\c_attn\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block\\attn\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block\\attn\\resid_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block\\ln_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block\\ln_2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block\\mlp\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block\\mlp\\c_fc\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block\\mlp\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block\\mlp\\dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_1\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_10\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_10\\attn\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_10\\attn\\attn_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_10\\attn\\c_attn\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_10\\attn\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_10\\attn\\resid_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_10\\ln_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_10\\ln_2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_10\\mlp\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_10\\mlp\\c_fc\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_10\\mlp\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_10\\mlp\\dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_11\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_11\\attn\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_11\\attn\\attn_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_11\\attn\\c_attn\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_11\\attn\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_11\\attn\\resid_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_11\\ln_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_11\\ln_2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_11\\mlp\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_11\\mlp\\c_fc\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_11\\mlp\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_11\\mlp\\dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_1\\attn\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_1\\attn\\attn_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_1\\attn\\c_attn\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_1\\attn\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_1\\attn\\resid_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_1\\ln_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_1\\ln_2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_1\\mlp\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_1\\mlp\\c_fc\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_1\\mlp\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_1\\mlp\\dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_2\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_2\\attn\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_2\\attn\\attn_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_2\\attn\\c_attn\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_2\\attn\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_2\\attn\\resid_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_2\\ln_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_2\\ln_2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_2\\mlp\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_2\\mlp\\c_fc\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_2\\mlp\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_2\\mlp\\dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_3\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_3\\attn\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_3\\attn\\attn_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_3\\attn\\c_attn\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_3\\attn\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_3\\attn\\resid_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_3\\ln_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_3\\ln_2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_3\\mlp\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_3\\mlp\\c_fc\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_3\\mlp\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_3\\mlp\\dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_4\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_4\\attn\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_4\\attn\\attn_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_4\\attn\\c_attn\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_4\\attn\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_4\\attn\\resid_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_4\\ln_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_4\\ln_2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_4\\mlp\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_4\\mlp\\c_fc\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_4\\mlp\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_4\\mlp\\dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_5\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_5\\attn\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_5\\attn\\attn_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_5\\attn\\c_attn\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_5\\attn\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_5\\attn\\resid_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_5\\ln_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_5\\ln_2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_5\\mlp\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_5\\mlp\\c_fc\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_5\\mlp\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_5\\mlp\\dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_6\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_6\\attn\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_6\\attn\\attn_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_6\\attn\\c_attn\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_6\\attn\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_6\\attn\\resid_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_6\\ln_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_6\\ln_2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_6\\mlp\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_6\\mlp\\c_fc\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_6\\mlp\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_6\\mlp\\dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_7\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_7\\attn\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_7\\attn\\attn_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_7\\attn\\c_attn\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_7\\attn\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_7\\attn\\resid_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_7\\ln_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_7\\ln_2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_7\\mlp\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_7\\mlp\\c_fc\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_7\\mlp\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_7\\mlp\\dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_8\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_8\\attn\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_8\\attn\\attn_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_8\\attn\\c_attn\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_8\\attn\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_8\\attn\\resid_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_8\\ln_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_8\\ln_2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_8\\mlp\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_8\\mlp\\c_fc\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_8\\mlp\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_8\\mlp\\dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_9\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_9\\attn\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_9\\attn\\attn_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_9\\attn\\c_attn\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_9\\attn\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_9\\attn\\resid_dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_9\\ln_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_9\\ln_2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_9\\mlp\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_9\\mlp\\c_fc\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_9\\mlp\\c_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\h\\tf_block_9\\mlp\\dropout\n",
      "......vars\n",
      "...layers\\tfgpt2_main_layer\\ln_f\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\tfgpt2_main_layer\\wte\n",
      "......vars\n",
      ".........0\n",
      "...optimizer\n",
      "......vars\n",
      ".........0\n",
      "...vars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2022-12-24 18:22:30         2299\n",
      "metadata.json                                  2022-12-24 18:22:30           64\n",
      "variables.h5                                   2022-12-24 18:22:35    498110488\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7863c8a9851c4309a855f9c7614bee62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cache at ['concat_and_encoded_testset', 'Common_df']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:58<00:00, 11.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Running Query 2/9 ####\n",
      "Evaluating perplexity on reference set Common_df with:\n",
      "\tSheldon dialogpt (sampling) as document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at E:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBotGit\\BarneyBot\\Data\\Characters\\Sheldon\\sheldon_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:58<00:00, 11.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Running Query 3/9 ####\n",
      "Evaluating perplexity on reference set Common_df with:\n",
      "\tHarry dialogpt (sampling) as document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at E:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBotGit\\BarneyBot\\Data\\Characters\\Harry\\harry_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:56<00:00, 11.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Running Query 4/9 ####\n",
      "Evaluating perplexity on reference set Common_df with:\n",
      "\tFry dialogpt (sampling) as document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at E:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBotGit\\BarneyBot\\Data\\Characters\\Fry\\fry_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:51<00:00, 10.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Running Query 5/9 ####\n",
      "Evaluating perplexity on reference set Common_df with:\n",
      "\tBender dialogpt (sampling) as document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at E:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBotGit\\BarneyBot\\Data\\Characters\\Bender\\bender_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:50<00:00, 10.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Running Query 6/9 ####\n",
      "Evaluating perplexity on reference set Common_df with:\n",
      "\tVader dialogpt (sampling) as document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at E:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBotGit\\BarneyBot\\Data\\Characters\\Vader\\vader_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:53<00:00, 10.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Running Query 7/9 ####\n",
      "Evaluating perplexity on reference set Common_df with:\n",
      "\tJoey dialogpt (sampling) as document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at E:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBotGit\\BarneyBot\\Data\\Characters\\Joey\\joey_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:51<00:00, 10.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Running Query 8/9 ####\n",
      "Evaluating perplexity on reference set Common_df with:\n",
      "\tPhoebe dialogpt (sampling) as document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at E:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBotGit\\BarneyBot\\Data\\Characters\\Phoebe\\phoebe_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:51<00:00, 10.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Running Query 9/9 ####\n",
      "Evaluating perplexity on reference set Common_df with:\n",
      "\tBase dialogpt (sampling) as document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at microsoft/DialoGPT-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:50<00:00, 10.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "metric = 'perplexity'\n",
    "metric_pretty = BBMetric.load_metric(metric).pretty_name\n",
    "metric_params = dict()\n",
    "results = evaluate_round([\n",
    "    {\n",
    "        'metric_name': metric,\n",
    "        'metric_actors': {\n",
    "            'predictor': (MetricActor.DIALOGPT_SAMPLE, charpair[0])\n",
    "        },\n",
    "        'reference_set': charpair[1] + '_df',\n",
    "        'metric_params': {},\n",
    "        'metric_attempt': 0\n",
    "    } for charpair in [('Joey', 'Phoebe'), ('Joey', 'Sheldon'), ('Bender', 'Fry'), ('Bender', 'Barney'),\n",
    "                       ('Barney', 'Harry')]\n",
    "] + [\n",
    "    {\n",
    "        'metric_name': metric,\n",
    "        'metric_actors': {\n",
    "            'document': (MetricActor.DIALOGPT_SAMPLE, char)\n",
    "        },\n",
    "        'reference_set': char + '_df',\n",
    "        'metric_params': metric_params.copy(),\n",
    "        'metric_attempt': 0\n",
    "    } for char in characters\n",
    "] + [\n",
    "    {\n",
    "        'metric_name': metric,\n",
    "        'metric_actors': {\n",
    "            'document': (MetricActor.DIALOGPT_SAMPLE, \"Base\")\n",
    "        },\n",
    "        'reference_set': char + '_df',\n",
    "        'metric_params': metric_params.copy(),\n",
    "        'metric_attempt': 0\n",
    "    } for char in characters\n",
    "] + [\n",
    "    {\n",
    "        'metric_name': metric,\n",
    "        'metric_actors': {\n",
    "            'document': (MetricActor.DIALOGPT_SAMPLE, char)\n",
    "        },\n",
    "        'reference_set': 'Common_df',\n",
    "        'metric_params': metric_params.copy(),\n",
    "        'metric_attempt': 0\n",
    "    } for char in characters + [\"Base\"]\n",
    "])\n",
    "metric_dict = load_metric_by_name(out_folder, metric_pretty)\n",
    "metric_dict = {**metric_dict, **results}\n",
    "save_metric_by_name(out_folder, metric_pretty, metric_dict)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987bfcb3",
   "metadata": {},
   "source": [
    "# COMET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16f17605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d07ceae1d1b4d39a43b2e8e46d601d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eamt22-cometinho-da.tar.gz: 307MB [00:58, 5.25MB/s]                                                                    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf4e4228edc14d1494ecb326fefb9f3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dfbe0d8b14c458086ffa8462b4db1dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/430 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "041877e08a9f42469d9e96b8e0135c15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa7789026e9d4b58bf0a79bedee659a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11ccffa7e13a4ad4ae286abbe1c8aaad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:comet.models.base:Path lightning_logs/cometinho_part-i/checkpoints/epoch=0-step=899999.ckpt does not exist!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Running Query 1/8 ####\n",
      "Evaluating comet on reference set Barney_df with:\n",
      "\tBarney_df dataset labels as document\n",
      "\tBarney_df dataset as reference\n",
      "\tBarney dialogpt (sampling) as predictor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:comet.models.base:Path lightning_logs/cometinho_part-i/checkpoints/epoch=0-step=899999.ckpt does not exist!\n",
      "E:\\Programs\\Anaconda\\envs\\barneybot\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Running Query 2/8 ####\n",
      "Evaluating comet on reference set Sheldon_df with:\n",
      "\tSheldon_df dataset labels as document\n",
      "\tSheldon_df dataset as reference\n",
      "\tSheldon dialogpt (sampling) as predictor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:comet.models.base:Path lightning_logs/cometinho_part-i/checkpoints/epoch=0-step=899999.ckpt does not exist!\n",
      "E:\\Programs\\Anaconda\\envs\\barneybot\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Running Query 3/8 ####\n",
      "Evaluating comet on reference set Harry_df with:\n",
      "\tHarry_df dataset labels as document\n",
      "\tHarry_df dataset as reference\n",
      "\tHarry dialogpt (sampling) as predictor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:comet.models.base:Path lightning_logs/cometinho_part-i/checkpoints/epoch=0-step=899999.ckpt does not exist!\n",
      "E:\\Programs\\Anaconda\\envs\\barneybot\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Running Query 4/8 ####\n",
      "Evaluating comet on reference set Fry_df with:\n",
      "\tFry_df dataset labels as document\n",
      "\tFry_df dataset as reference\n",
      "\tFry dialogpt (sampling) as predictor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:comet.models.base:Path lightning_logs/cometinho_part-i/checkpoints/epoch=0-step=899999.ckpt does not exist!\n",
      "E:\\Programs\\Anaconda\\envs\\barneybot\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Running Query 5/8 ####\n",
      "Evaluating comet on reference set Bender_df with:\n",
      "\tBender_df dataset labels as document\n",
      "\tBender_df dataset as reference\n",
      "\tBender dialogpt (sampling) as predictor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:comet.models.base:Path lightning_logs/cometinho_part-i/checkpoints/epoch=0-step=899999.ckpt does not exist!\n",
      "E:\\Programs\\Anaconda\\envs\\barneybot\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Running Query 6/8 ####\n",
      "Evaluating comet on reference set Vader_df with:\n",
      "\tVader_df dataset labels as document\n",
      "\tVader_df dataset as reference\n",
      "\tVader dialogpt (sampling) as predictor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:comet.models.base:Path lightning_logs/cometinho_part-i/checkpoints/epoch=0-step=899999.ckpt does not exist!\n",
      "E:\\Programs\\Anaconda\\envs\\barneybot\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Running Query 7/8 ####\n",
      "Evaluating comet on reference set Joey_df with:\n",
      "\tJoey_df dataset labels as document\n",
      "\tJoey_df dataset as reference\n",
      "\tJoey dialogpt (sampling) as predictor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:comet.models.base:Path lightning_logs/cometinho_part-i/checkpoints/epoch=0-step=899999.ckpt does not exist!\n",
      "E:\\Programs\\Anaconda\\envs\\barneybot\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Running Query 8/8 ####\n",
      "Evaluating comet on reference set Phoebe_df with:\n",
      "\tPhoebe_df dataset labels as document\n",
      "\tPhoebe_df dataset as reference\n",
      "\tPhoebe dialogpt (sampling) as predictor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:comet.models.base:Path lightning_logs/cometinho_part-i/checkpoints/epoch=0-step=899999.ckpt does not exist!\n",
      "E:\\Programs\\Anaconda\\envs\\barneybot\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "metric = \"comet\"\n",
    "metric_pretty = BBMetric.load_metric(metric).pretty_name\n",
    "metric_params = dict()\n",
    "results = evaluate_round([\n",
    "    {\n",
    "        'metric_name': metric,\n",
    "        'metric_actors': {\n",
    "            'document': (MetricActor.DATASET_CHARCONTEXT, char + '_df'),\n",
    "            'reference': (MetricActor.DATASET_CHAR, char + \"_df\"),\n",
    "            'predictor': (MetricActor.DIALOGPT_SAMPLE, char)\n",
    "        },\n",
    "        'reference_set': char + '_df',\n",
    "        'metric_params': {},\n",
    "        'metric_attempt': 0\n",
    "    } for char in characters\n",
    "])\n",
    "metric_dict = load_metric_by_name(out_folder, metric_pretty)\n",
    "metric_dict = {**metric_dict, **results}\n",
    "save_metric_by_name(out_folder, metric_pretty, metric_dict)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fddbe56",
   "metadata": {},
   "source": [
    "# Pairwise Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4ba20ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Running Query 1/13 ####\n",
      "Evaluating extended edit distance on reference set Barney_df with:\n",
      "\tBarney_df dataset as document0\n",
      "\tBarney dialogpt (sampling) as document1\n",
      "\n",
      "#### Running Query 2/13 ####\n",
      "Evaluating extended edit distance on reference set Sheldon_df with:\n",
      "\tSheldon_df dataset as document0\n",
      "\tSheldon dialogpt (sampling) as document1\n",
      "\n",
      "#### Running Query 3/13 ####\n",
      "Evaluating extended edit distance on reference set Harry_df with:\n",
      "\tHarry_df dataset as document0\n",
      "\tHarry dialogpt (sampling) as document1\n",
      "\n",
      "#### Running Query 4/13 ####\n",
      "Evaluating extended edit distance on reference set Fry_df with:\n",
      "\tFry_df dataset as document0\n",
      "\tFry dialogpt (sampling) as document1\n",
      "\n",
      "#### Running Query 5/13 ####\n",
      "Evaluating extended edit distance on reference set Bender_df with:\n",
      "\tBender_df dataset as document0\n",
      "\tBender dialogpt (sampling) as document1\n",
      "\n",
      "#### Running Query 6/13 ####\n",
      "Evaluating extended edit distance on reference set Vader_df with:\n",
      "\tVader_df dataset as document0\n",
      "\tVader dialogpt (sampling) as document1\n",
      "\n",
      "#### Running Query 7/13 ####\n",
      "Evaluating extended edit distance on reference set Joey_df with:\n",
      "\tJoey_df dataset as document0\n",
      "\tJoey dialogpt (sampling) as document1\n",
      "\n",
      "#### Running Query 8/13 ####\n",
      "Evaluating extended edit distance on reference set Phoebe_df with:\n",
      "\tPhoebe_df dataset as document0\n",
      "\tPhoebe dialogpt (sampling) as document1\n",
      "\n",
      "#### Running Query 9/13 ####\n",
      "Evaluating extended edit distance on reference set Common_df with:\n",
      "\tPhoebe dialogpt (sampling) as document0\n",
      "\tJoey dialogpt (sampling) as document1\n",
      "\n",
      "#### Running Query 10/13 ####\n",
      "Evaluating extended edit distance on reference set Common_df with:\n",
      "\tSheldon dialogpt (sampling) as document0\n",
      "\tJoey dialogpt (sampling) as document1\n",
      "\n",
      "#### Running Query 11/13 ####\n",
      "Evaluating extended edit distance on reference set Common_df with:\n",
      "\tFry dialogpt (sampling) as document0\n",
      "\tBender dialogpt (sampling) as document1\n",
      "\n",
      "#### Running Query 12/13 ####\n",
      "Evaluating extended edit distance on reference set Common_df with:\n",
      "\tBarney dialogpt (sampling) as document0\n",
      "\tBender dialogpt (sampling) as document1\n",
      "\n",
      "#### Running Query 13/13 ####\n",
      "Evaluating extended edit distance on reference set Common_df with:\n",
      "\tHarry dialogpt (sampling) as document0\n",
      "\tBarney dialogpt (sampling) as document1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done.\n",
      "#### Running Query 1/13 ####\n",
      "Evaluating word mover distance on reference set Barney_df with:\n",
      "\tBarney_df dataset as document0\n",
      "\tBarney dialogpt (sampling) as document1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Running Query 2/13 ####\n",
      "Evaluating word mover distance on reference set Sheldon_df with:\n",
      "\tSheldon_df dataset as document0\n",
      "\tSheldon dialogpt (sampling) as document1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Running Query 3/13 ####\n",
      "Evaluating word mover distance on reference set Harry_df with:\n",
      "\tHarry_df dataset as document0\n",
      "\tHarry dialogpt (sampling) as document1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Running Query 4/13 ####\n",
      "Evaluating word mover distance on reference set Fry_df with:\n",
      "\tFry_df dataset as document0\n",
      "\tFry dialogpt (sampling) as document1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Running Query 5/13 ####\n",
      "Evaluating word mover distance on reference set Bender_df with:\n",
      "\tBender_df dataset as document0\n",
      "\tBender dialogpt (sampling) as document1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Running Query 6/13 ####\n",
      "Evaluating word mover distance on reference set Vader_df with:\n",
      "\tVader_df dataset as document0\n",
      "\tVader dialogpt (sampling) as document1\n",
      "\n",
      "#### Running Query 7/13 ####\n",
      "Evaluating word mover distance on reference set Joey_df with:\n",
      "\tJoey_df dataset as document0\n",
      "\tJoey dialogpt (sampling) as document1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Running Query 8/13 ####\n",
      "Evaluating word mover distance on reference set Phoebe_df with:\n",
      "\tPhoebe_df dataset as document0\n",
      "\tPhoebe dialogpt (sampling) as document1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Running Query 9/13 ####\n",
      "Evaluating word mover distance on reference set Common_df with:\n",
      "\tPhoebe dialogpt (sampling) as document0\n",
      "\tJoey dialogpt (sampling) as document1\n",
      "\n",
      "#### Running Query 10/13 ####\n",
      "Evaluating word mover distance on reference set Common_df with:\n",
      "\tSheldon dialogpt (sampling) as document0\n",
      "\tJoey dialogpt (sampling) as document1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Running Query 11/13 ####\n",
      "Evaluating word mover distance on reference set Common_df with:\n",
      "\tFry dialogpt (sampling) as document0\n",
      "\tBender dialogpt (sampling) as document1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Running Query 12/13 ####\n",
      "Evaluating word mover distance on reference set Common_df with:\n",
      "\tBarney dialogpt (sampling) as document0\n",
      "\tBender dialogpt (sampling) as document1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Running Query 13/13 ####\n",
      "Evaluating word mover distance on reference set Common_df with:\n",
      "\tHarry dialogpt (sampling) as document0\n",
      "\tBarney dialogpt (sampling) as document1\n",
      "\n",
      "Done.\n",
      "#### Running Query 1/13 ####\n",
      "Evaluating mpnet embedding similarity on reference set Barney_df with:\n",
      "\tBarney_df dataset as document0\n",
      "\tBarney dialogpt (sampling) as document1\n",
      "\n",
      "#### Running Query 2/13 ####\n",
      "Evaluating mpnet embedding similarity on reference set Sheldon_df with:\n",
      "\tSheldon_df dataset as document0\n",
      "\tSheldon dialogpt (sampling) as document1\n",
      "\n",
      "#### Running Query 3/13 ####\n",
      "Evaluating mpnet embedding similarity on reference set Harry_df with:\n",
      "\tHarry_df dataset as document0\n",
      "\tHarry dialogpt (sampling) as document1\n",
      "\n",
      "#### Running Query 4/13 ####\n",
      "Evaluating mpnet embedding similarity on reference set Fry_df with:\n",
      "\tFry_df dataset as document0\n",
      "\tFry dialogpt (sampling) as document1\n",
      "\n",
      "#### Running Query 5/13 ####\n",
      "Evaluating mpnet embedding similarity on reference set Bender_df with:\n",
      "\tBender_df dataset as document0\n",
      "\tBender dialogpt (sampling) as document1\n",
      "\n",
      "#### Running Query 6/13 ####\n",
      "Evaluating mpnet embedding similarity on reference set Vader_df with:\n",
      "\tVader_df dataset as document0\n",
      "\tVader dialogpt (sampling) as document1\n",
      "\n",
      "#### Running Query 7/13 ####\n",
      "Evaluating mpnet embedding similarity on reference set Joey_df with:\n",
      "\tJoey_df dataset as document0\n",
      "\tJoey dialogpt (sampling) as document1\n",
      "\n",
      "#### Running Query 8/13 ####\n",
      "Evaluating mpnet embedding similarity on reference set Phoebe_df with:\n",
      "\tPhoebe_df dataset as document0\n",
      "\tPhoebe dialogpt (sampling) as document1\n",
      "\n",
      "#### Running Query 9/13 ####\n",
      "Evaluating mpnet embedding similarity on reference set Common_df with:\n",
      "\tPhoebe dialogpt (sampling) as document0\n",
      "\tJoey dialogpt (sampling) as document1\n",
      "\n",
      "#### Running Query 10/13 ####\n",
      "Evaluating mpnet embedding similarity on reference set Common_df with:\n",
      "\tSheldon dialogpt (sampling) as document0\n",
      "\tJoey dialogpt (sampling) as document1\n",
      "\n",
      "#### Running Query 11/13 ####\n",
      "Evaluating mpnet embedding similarity on reference set Common_df with:\n",
      "\tFry dialogpt (sampling) as document0\n",
      "\tBender dialogpt (sampling) as document1\n",
      "\n",
      "#### Running Query 12/13 ####\n",
      "Evaluating mpnet embedding similarity on reference set Common_df with:\n",
      "\tBarney dialogpt (sampling) as document0\n",
      "\tBender dialogpt (sampling) as document1\n",
      "\n",
      "#### Running Query 13/13 ####\n",
      "Evaluating mpnet embedding similarity on reference set Common_df with:\n",
      "\tHarry dialogpt (sampling) as document0\n",
      "\tBarney dialogpt (sampling) as document1\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# 'rouge l', 'google bleu', 'meteor', 'bertscore', 'bleurt', 'term error rate',\n",
    "# 'bleurt', 'bertscore', 'roberta crossencoding similarity'\n",
    "\n",
    "for metric in ['bartscore']:\n",
    "    metric_pretty = BBMetric.load_metric(metric).pretty_name\n",
    "    results = evaluate_round([\n",
    "        {\n",
    "            'metric_name': metric,\n",
    "            'metric_actors': {\n",
    "                'reference': (MetricActor.DATASET_CHAR, char + \"_df\"),\n",
    "                'predictor': (MetricActor.DIALOGPT_SAMPLE, char)\n",
    "            },\n",
    "            'reference_set': char + '_df',\n",
    "            'metric_params': {},\n",
    "            'metric_attempt': 0\n",
    "        } for char in characters\n",
    "    ] + [\n",
    "        {\n",
    "            'metric_name': metric,\n",
    "            'metric_actors': {\n",
    "                'reference': (MetricActor.DIALOGPT_SAMPLE, charpair[1]),\n",
    "                'predictor': (MetricActor.DIALOGPT_SAMPLE, charpair[0])\n",
    "            },\n",
    "            'reference_set': 'Common_df',\n",
    "            'metric_params': {},\n",
    "            'metric_attempt': 0\n",
    "        } for charpair in [('Joey', 'Phoebe'), ('Joey', 'Sheldon'), ('Bender', 'Fry'), ('Bender', 'Barney')]        \n",
    "    ])\n",
    "    metric_dict = load_metric_by_name(out_folder, metric_pretty)\n",
    "    metric_dict = {**metric_dict, **results}\n",
    "    save_metric_by_name(out_folder, metric_pretty, metric_dict)\n",
    "\n",
    "'''\n",
    "for metric in ['extended edit distance', 'word mover distance', 'mpnet embedding similarity']:\n",
    "    metric_pretty = BBMetric.load_metric(metric).pretty_name\n",
    "    results = evaluate_round([\n",
    "        {\n",
    "            'metric_name': metric,\n",
    "            'metric_actors': {\n",
    "                'document0': (MetricActor.DATASET_CHAR, char + \"_df\"),\n",
    "                'document1': (MetricActor.DIALOGPT_SAMPLE, char)\n",
    "            },\n",
    "            'reference_set': char + '_df',\n",
    "            'metric_params': {},\n",
    "            'metric_attempt': 0\n",
    "        } for char in characters\n",
    "    ] + [\n",
    "        {\n",
    "            'metric_name': metric,\n",
    "            'metric_actors': {\n",
    "                'document0': (MetricActor.DIALOGPT_SAMPLE, charpair[1]),\n",
    "                'document1': (MetricActor.DIALOGPT_SAMPLE, charpair[0])\n",
    "            },\n",
    "            'reference_set': 'Common_df',\n",
    "            'metric_params': {},\n",
    "            'metric_attempt': 0\n",
    "        } for charpair in [('Joey', 'Phoebe'), ('Joey', 'Sheldon'), ('Bender', 'Fry'), ('Bender', 'Barney'),\n",
    "                           ('Barney', 'Harry')]        \n",
    "    ])\n",
    "    metric_dict = load_metric_by_name(out_folder, metric_pretty)\n",
    "    metric_dict = {**metric_dict, **results}\n",
    "    save_metric_by_name(out_folder, metric_pretty, metric_dict)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bbd0c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

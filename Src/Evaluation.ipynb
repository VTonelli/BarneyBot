{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dde1bf5a",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46f38c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True\n",
    "use_cuda = False\n",
    "\n",
    "do_metric_training = False\n",
    "do_predictions = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6aaa0c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pip install -r \"E:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBotGit\\BarneyBot\\requirements.txt\"\n"
     ]
    }
   ],
   "source": [
    "### Run environment setup\n",
    "import os\n",
    "import lib.BBSetup as BBSetup\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    BBSetup.colab_setup(mount_folder=r\"/content/drive/My Drive/unibo/NLP_project/BarneyBot\")\n",
    "except:\n",
    "    try:\n",
    "        BBSetup.anaconda_manual_setup(base_folder=r\"E:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBotGit\\BarneyBot\",\n",
    "                                      env_name=\"barneybot\")\n",
    "    except:\n",
    "        BBSetup.anaconda_auto_setup(base_folder=r\"E:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBotGit\\BarneyBot\")\n",
    "\n",
    "### Define folders\n",
    "base_folder = BBSetup.BASE_FOLDER\n",
    "in_folder = BBSetup.set_folder(os.path.join(base_folder, 'Data', 'Characters'))\n",
    "out_folder = BBSetup.set_folder(os.path.join(base_folder, 'Metrics', 'New'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64468996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "E:\\Programs\\Anaconda\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tonel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "### load_char_df() (hg dataset) ['test'] to get testset, containing contexts and response\n",
    "### get_chatbot_predictions() to get a type of predictions for a model\n",
    "from lib.BBDataLoad import load_char_df, get_chatbot_predictions, dialogpt_preprocess_function\n",
    "from datasets import load_dataset\n",
    "from transformers import TFAutoModelForCausalLM\n",
    "from lib.BBMetrics import BBMetric\n",
    "from lib.BBMetricResults import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "from lib.BBData import character_dict, model_name, random_state\n",
    "import lib.BBData as BBData\n",
    "characters = list(character_dict.keys())\n",
    "characters.remove('Default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96c27ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import structures from HuggingFace\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "                                          cache_dir=os.path.join(base_folder, \"cache\"))\n",
    "tokenizer.pad_token = '#'\n",
    "data_collator = DataCollatorForLanguageModeling(mlm=False, tokenizer=tokenizer, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57b98f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(S):\n",
    "    if S == []:\n",
    "        return S\n",
    "    if isinstance(S[0], list):\n",
    "        return flatten(S[0]) + flatten(S[1:])\n",
    "    return S[:1] + flatten(S[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55849736",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_predictions:\n",
    "    print(\"Saving predictions to file\")\n",
    "    with tqdm(total=len(characters)*4) as pbar:\n",
    "        # Chatbot of a character on their own dataset\n",
    "        for char in characters:\n",
    "            checkpoint_folder = os.path.join(in_folder, char,\n",
    "                                             character_dict[char]['checkpoint_folder'])\n",
    "            model = TFAutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=checkpoint_folder)\n",
    "            model.compile()\n",
    "            samples = load_char_df(char)\n",
    "            for gen_type in ['greedy', 'nbeams', 'sampling']:\n",
    "                get_chatbot_predictions(samples['test']['context/0'], model,\n",
    "                              character_dict[char]['prediction_filename'] + '_' + gen_type + '.json',\n",
    "                              gen_type, char, cache.tokenizer, base_folder, override_predictions=True)\n",
    "                pbar.update(1)\n",
    "        # Base chatbot on each character's dataset\n",
    "        for char in characters:\n",
    "            model = TFAutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                           cache_dir=os.path.join(base_folder, \"cache\"))\n",
    "            model.compile()\n",
    "            samples = load_char_df(char)\n",
    "            get_chatbot_predictions(samples['test']['context/0'], model,\n",
    "                              'from_' + char + \"_df__sampling.json\", gen_type,\n",
    "                              \"Default\", cache.tokenizer, base_folder, override_predictions=True)\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17d247a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Metrics training.\n"
     ]
    }
   ],
   "source": [
    "if do_metric_training:\n",
    "    print(\"Training metrics\")\n",
    "    # Neural Chatbot Classifier\n",
    "    with tqdm(total=len(characters) + 2) as pbar:\n",
    "        for char in tqdm(characters):\n",
    "            neural_classifier = BBMetric.load_metric(\"neural chatbot classifier\")\n",
    "            neural_classifier.train(character=char, random_state=random_state,\n",
    "                     source_encoded_path=None,\n",
    "                     source_path=os.path.join(base_folder, \"Data\", \"Sources\",\n",
    "                                              character_dict[char]['source'],\n",
    "                                              character_dict[char]['source'] + \".csv\"),\n",
    "                     source_save_path=os.path.join(base_folder, \"Data\", \"Characters\", char),\n",
    "                     save_path=os.path.join(base_folder, \"Data\", \"Characters\", char))\n",
    "            pbar.update(1)\n",
    "        # Distilbert-Embedded Chatbot Classifier\n",
    "        bertembedded_classifier = BBMetric.load_metric(\"distilbert-embedded chatbot classifier\")\n",
    "        bertembedded_classifier.train(characters_path=os.path.join(base_folder, \"Data\", \"Characters\"),\n",
    "                                      save_path=os.path.join(base_folder, \"Data\", \"Metrics\", \n",
    "                                                             \"distilbert_embedder\"),\n",
    "                                      train_embedder=True,\n",
    "                                      verbose=True)\n",
    "        pbar.update(1)\n",
    "        characters_no_barney = characters.copy()\n",
    "        characters_no_barney.remove(\"Barney\")\n",
    "        bertembedded_classifier = BBMetric.load_metric(\"distilbert-embedded chatbot classifier\")\n",
    "        bertembedded_classifier.metric.set_characters(characters_no_barney)\n",
    "        bertembedded_classifier.train(characters_path=os.path.join(base_folder, \"Data\", \"Characters\"),\n",
    "                                      save_path=os.path.join(base_folder, \"Data\", \"Metrics\", \n",
    "                                                             \"distilbert_embedder_nobarney\"),\n",
    "                                      train_embedder=True,\n",
    "                                      verbose=True)\n",
    "        pbar.update(1)\n",
    "else:\n",
    "    print(\"Skipping Metrics training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea2ce54",
   "metadata": {},
   "source": [
    "# Cache System Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8285eb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a cache structure to avoid reloading stuff\n",
    "from types import SimpleNamespace\n",
    "\n",
    "cache = {\n",
    "    'dialogpt': {char: None for char in characters + [\"Base\"]},\n",
    "    'tokenizer': None,\n",
    "    'datacollator': None,\n",
    "    'trained_metric': {\n",
    "        'neural chatbot classifier': {char: None for char in characters},\n",
    "        'frequency chatbot classifier': {'c-tf-idf': None, 'tf-idf': None, 'word frequency': None},\n",
    "        'distilbert-embedded chatbot classifier': {'Full': None, 'No Barney': None}\n",
    "    },\n",
    "    'testset': {char + \"_df\": None for char in characters + [\"Common\"]},\n",
    "    'concat_and_encoded_testset': {char + \"_df\": None for char in characters + [\"Common\"]},\n",
    "    'predictions': {\n",
    "        char + \"_df\": { # Dataset\n",
    "            char: { # Chatbot\n",
    "                'greedy': None,\n",
    "                'nbeams': None,\n",
    "                'sampling': None\n",
    "            } for char in characters + [\"Base\"]\n",
    "        } for char in characters + [\"Common\"]\n",
    "    },\n",
    "}\n",
    "cache = SimpleNamespace(**cache)\n",
    "\n",
    "def load_cache_entry(value, entry):\n",
    "    pointer = cache\n",
    "    for i in range(len(entry)-1):\n",
    "        val = entry[i]\n",
    "        if isinstance(pointer, dict):\n",
    "            pointer = pointer[val]\n",
    "        elif isinstance(pointer, SimpleNamespace):\n",
    "            pointer = pointer.__dict__[val]\n",
    "        else:\n",
    "            raise Exception()\n",
    "    if not pointer[entry[-1]]:\n",
    "        pointer[entry[-1]] = value\n",
    "        if verbose:\n",
    "            print(\"Loaded cache at \" + str(entry))\n",
    "    return pointer[entry[-1]]\n",
    "\n",
    "def flush_cache_entries(entries):\n",
    "    for entry in entries:\n",
    "        pointer = cache\n",
    "        for i in range(len(entry)-1):\n",
    "            val = entry[i]\n",
    "            if isinstance(pointer, dict):\n",
    "                pointer = pointer[val]\n",
    "            elif isinstance(pointer, SimpleNamespace):\n",
    "                pointer = pointer.__dict__[val]\n",
    "            else:\n",
    "                raise Exception()\n",
    "        pointer[entry[-1]] = None\n",
    "        if verbose:\n",
    "            print(\"Flushed cache at \" + str(entry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96df347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cache_testset(character, base_folder):\n",
    "    if not cache.testset[character + \"_df\"]:\n",
    "        if character != \"Common\":\n",
    "            df = load_char_df(character, base_folder)['test']\n",
    "        else: \n",
    "            df = load_dataset('csv',\n",
    "                     data_files=os.path.join(base_folder, 'Data', 'Sources', 'common_dataset.csv'), \n",
    "                     cache_dir=os.path.join(base_folder, \"cache\"))['train']\n",
    "        load_cache_entry(df, ['testset', character + \"_df\"])\n",
    "    return cache.testset[character + \"_df\"]\n",
    "\n",
    "# For perplexity\n",
    "def get_cache_concat_and_encoded_testset(character, base_folder):\n",
    "    if not cache.concat_and_encoded_testset[character + \"_df\"]:\n",
    "        testset = get_cache_testset(character, base_folder)\n",
    "        concat_encoded_testset = testset.map(lambda row: dialogpt_preprocess_function(row,\n",
    "                                                                            cache.tokenizer),\n",
    "                                             batched=False)\n",
    "        concat_encoded_testset = concat_encoded_testset.to_tf_dataset(\n",
    "            columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "            shuffle=False,\n",
    "            batch_size=8,\n",
    "            collate_fn=cache.datacollator,\n",
    "        )\n",
    "        load_cache_entry(concat_encoded_testset, ['concat_and_encoded_testset', character + \"_df\"])\n",
    "    return cache.concat_and_encoded_testset[character + \"_df\"]\n",
    "\n",
    "def get_cache_predictions(dataset_from, character, base_folder, gen_type):\n",
    "    if not cache.predictions[dataset_from][character][gen_type]:\n",
    "        if dataset_from == character + \"_df\":\n",
    "            if character != \"Base\":\n",
    "                predictions_tk = get_chatbot_predictions(None, None,\n",
    "                      character_dict[character]['prediction_filename'] + '_' + gen_type + '.json',\n",
    "                      None, character, None, base_folder, override_predictions=False)\n",
    "            else:\n",
    "                predictions_tk = get_chatbot_predictions(None, None,\n",
    "                      'from_' + dataset_from + '__' + gen_type + '.json',\n",
    "                      None, 'Default', None, base_folder, override_predictions=False)\n",
    "        elif dataset_from == \"Common_df\":\n",
    "            df = load_dataset('csv',\n",
    "                         data_files=os.path.join(base_folder, 'Data', 'Sources', 'common_dataset.csv'), \n",
    "                         cache_dir=os.path.join(base_folder, \"cache\"))\n",
    "            df = df.remove_columns(['source'])\n",
    "            model = get_cache_model(character)\n",
    "            predictions_tk = get_chatbot_predictions(df['train']['context/0'], model,\n",
    "                  \"\", gen_type, character, cache.tokenizer, base_folder, file_caching=False, override_predictions=False)            \n",
    "        else:\n",
    "            raise NotImplementedError(\"Unexpected predictions to load!\")\n",
    "        predictions = []\n",
    "        for line in predictions_tk:\n",
    "            predictions.append(cache.tokenizer.decode(line, skip_special_tokens=True))\n",
    "        load_cache_entry(predictions, ['predictions', dataset_from, character, gen_type])\n",
    "    return cache.predictions[dataset_from][character][gen_type]\n",
    "\n",
    "# For metrics worth caching, in particular the chatbot classifiers\n",
    "def get_cache_metric(metric_name, **kwargs):\n",
    "    classifier_char = None if 'classifier_char' not in kwargs else kwargs['classifier_char']\n",
    "    mode = None if 'mode' not in kwargs else kwargs['mode']\n",
    "    with_barney = None if 'with_barney' not in kwargs else kwargs['with_barney']\n",
    "    with_barney = 'Full' if with_barney else 'No Barney'\n",
    "    if metric_name in cache.trained_metric:\n",
    "        if metric_name == \"neural chatbot classifier\":\n",
    "            if not cache.trained_metric[metric_name][classifier_char]:\n",
    "                cache.trained_metric[metric_name][classifier_char] = BBMetric.load_metric(metric_name)\n",
    "                cache.trained_metric[metric_name][classifier_char].compute( # Dummy round for caching\n",
    "                    character=classifier_char,\n",
    "                    load_path=os.path.join(base_folder, \"Data\", \"Characters\",\n",
    "                              classifier_char, character_dict[classifier_char]['classifier_folder']),\n",
    "                    sentences=[\"Hi\", \"Hello\", \"How\"])\n",
    "            return cache.trained_metric[metric_name][classifier_char]\n",
    "        elif metric_name == \"frequency chatbot classifier\":\n",
    "            if not cache.trained_metric[metric_name][mode]:\n",
    "                cache.trained_metric[metric_name][mode] = BBMetric.load_metric(metric_name)\n",
    "                cache.trained_metric[metric_name][mode].train(\n",
    "                    characters_path=os.path.join(base_folder, \"Data\", \"Characters\"),\n",
    "                    mode=mode)\n",
    "            return cache.trained_metric[metric_name][mode]\n",
    "        elif metric_name == \"distilbert-embedded chatbot classifier\":\n",
    "            if not cache.trained_metric[metric_name][with_barney]:\n",
    "                if with_barney == 'Full':\n",
    "                    cache.trained_metric[metric_name][with_barney] = BBMetric.load_metric(metric_name,\n",
    "                                embedder_path=os.path.join(base_folder, \"Data\", \"Metrics\", \n",
    "                                                           \"distilbert_embedder\"),\n",
    "                                from_pretrained=True, use_cuda=use_cuda)\n",
    "                    cache.trained_metric[metric_name][with_barney].train(\n",
    "                        characters_path=os.path.join(base_folder, \"Data\", \"Characters\"),\n",
    "                        save_path=None, train_embedder=False\n",
    "                    )\n",
    "                else:\n",
    "                    cache.trained_metric[metric_name][with_barney] = BBMetric.load_metric(metric_name,\n",
    "                                embedder_path=os.path.join(base_folder, \"Data\", \"Metrics\", \n",
    "                                                           \"distilbert_embedder_nobarney\"),\n",
    "                                from_pretrained=True, use_cuda=use_cuda)\n",
    "                    cache.trained_metric[metric_name][with_barney].train(\n",
    "                        characters_path=os.path.join(base_folder, \"Data\", \"Characters\"),\n",
    "                        save_path=None, train_embedder=False\n",
    "                    )\n",
    "            return cache.trained_metric[metric_name][with_barney]\n",
    "    else:\n",
    "        return BBMetric.load_metric(metric_name)\n",
    "\n",
    "def get_cache_model(character):\n",
    "    if character == \"Base\":\n",
    "        model = TFAutoModelForCausalLM.from_pretrained(model_name, cache_dir=os.path.join(base_folder, \"cache\"))\n",
    "    else:\n",
    "        checkpoint_folder = os.path.join(in_folder, character, character_dict[character]['checkpoint_folder'])\n",
    "        model = TFAutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=checkpoint_folder)\n",
    "    model.compile()\n",
    "    cache.dialogpt[character] = model\n",
    "    return cache.dialogpt[character]\n",
    "\n",
    "cache.tokenizer = tokenizer\n",
    "cache.datacollator = data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc3e480",
   "metadata": {},
   "source": [
    "# Evaluation Process Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8dba10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_callable(reference_set, character, column):\n",
    "    if column == \"context/0\" or column == \"response\":\n",
    "        assert(reference_set == character + \"_df\")\n",
    "        return get_cache_testset(character, base_folder)[column]\n",
    "    else:\n",
    "        assert(reference_set == character + \"_df\" or \\\n",
    "               reference_set == \"Common_df\" or \\\n",
    "               (character == \"Base\" and column == \"sampling\"))\n",
    "        return get_cache_predictions(reference_set, character, base_folder, column)\n",
    "\n",
    "def perplexity_callable(reference_set, character):\n",
    "    return {\n",
    "        'model': get_cache_model(character),\n",
    "        'encoded_test_set': get_cache_concat_and_encoded_testset(reference_set.replace(\"_df\", \"\"),\n",
    "                                                                 base_folder)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15b49468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_round(queries):\n",
    "    actors_pprint_map = {\n",
    "        MetricActor.DATASET_CHAR: \"dataset\",\n",
    "        MetricActor.DATASET_CHARCONTEXT: \"dataset labels\",\n",
    "        MetricActor.DIALOGPT_GREEDY: \"dialogpt (greedy)\",\n",
    "        MetricActor.DIALOGPT_NBEAMS: \"dialogpt (nbeamns)\",\n",
    "        MetricActor.DIALOGPT_SAMPLE: \"dialogpt (sampling)\"\n",
    "    }\n",
    "    actor_to_column_map = {\n",
    "        MetricActor.DATASET_CHARCONTEXT: 'context/0',\n",
    "        MetricActor.DATASET_CHAR: 'response',\n",
    "        MetricActor.DIALOGPT_GREEDY: 'greedy',\n",
    "        MetricActor.DIALOGPT_NBEAMS: 'nbeams',\n",
    "        MetricActor.DIALOGPT_SAMPLE: 'sampling'\n",
    "    }\n",
    "    results = dict()\n",
    "    for i in range(len(queries)):\n",
    "        try:\n",
    "            query = queries[i].copy() # Since there are destructive operations\n",
    "            print(\"#### Running Query \" + str(i+1) + \"/\" + str(len(queries)) + \" ####\")\n",
    "            if 'run' in query:\n",
    "                query['run'](**query['run_args'])\n",
    "            else:\n",
    "                print(\"Evaluating \" + query['metric_name'] + \\\n",
    "                      \" on reference set \" + query['reference_set'] + \" with:\")\n",
    "                for actor_type, actor in query['metric_actors'].items():\n",
    "                    print(\"\\t\" + actor[1] + \" \" + actors_pprint_map[actor[0]] + \" as \" + actor_type)\n",
    "                # Get metric metadata data for outputting\n",
    "                query_output = dict()\n",
    "                query_output['metric_name'] = query['metric_name']\n",
    "                query_output['metric_version'] = 1 if 'metric_version' not in query else query['metric_version']\n",
    "                query_output['metric_attempt'] = 0 if 'metric_attempt' not in query else query['metric_attempt']\n",
    "                query_output['metric_actors'] = query['metric_actors']\n",
    "                query_output['metric_params'] = query['metric_params']\n",
    "                query_output['context'] = {\n",
    "                    \"dialogpt_size\": \"small\",\n",
    "                    \"dialogpt_context_sentences\": BBData.context_n,\n",
    "                    \"dialogpt_nbeams_beams\": BBData.n_beams,\n",
    "                    \"dialogpt_sample_top_p\": BBData.top_p,\n",
    "                    \"dialogpt_sample_top_k\": BBData.top_k\n",
    "                }\n",
    "                query_output['metric_arity'] = get_metric_arity(query['metric_name'])\n",
    "                query_output['metric_determinism'] = get_metric_determinism(query['metric_name'],\n",
    "                                                                            query_output['metric_version'])\n",
    "                query_output['reference_set'] = query['reference_set']\n",
    "                query_hash = dict_hash({'metric_name': query_output['metric_name'],\n",
    "                                        'metric_version': query_output['metric_version'],\n",
    "                                        'reference_set': query_output['reference_set'],\n",
    "                                        'metric_attempt': query_output['metric_attempt'],\n",
    "                                        'metric_actors': query_output['metric_actors'],\n",
    "                                        'context': query_output['context'],\n",
    "                                        'metric_params': query_output['metric_params']})\n",
    "                for key in query['metric_actors'].keys(): # Lazy fix for \"_df\" suffix\n",
    "                    if query['metric_actors'][key][0] == MetricActor.DATASET_CHARCONTEXT or \\\n",
    "                        query['metric_actors'][key][0] == MetricActor.DATASET_CHAR:\n",
    "                        query['metric_actors'][key] = (query['metric_actors'][key][0],\n",
    "                                                       query['metric_actors'][key][1].replace(\"_df\", \"\"))\n",
    "                # Compute the actual metric\n",
    "                if query['metric_name'] in ['google bleu', 'meteor', 'rouge l', 'mpnet embedding similarity',\n",
    "                                'emotion classifier', 'distinct', 'roberta crossencoding similarity',\n",
    "                                'repetitiveness', 'term error rate', 'bertscore', 'bleurt', 'bartscore',\n",
    "                                'word mover distance', 't5 grammar correction edit distance',\n",
    "                                'extended edit distance', 'flesch-kincaid index']:\n",
    "                    args_map = {\n",
    "                        'predictor': 'predictions', 'reference': 'references', 'document': 'sentences',\n",
    "                        'document0': 'sentences_a', 'document1': 'sentences_b'\n",
    "                    }\n",
    "                    metric = get_cache_metric(query['metric_name'])\n",
    "                    args_dict = {}\n",
    "                    for actor_key, actor_pair in query['metric_actors'].items():\n",
    "                        args_dict[args_map[actor_key]] = sentence_callable(query['reference_set'],\n",
    "                                                                           actor_pair[1],\n",
    "                                                                           actor_to_column_map[actor_pair[0]])\n",
    "                elif query['metric_name'] == 'comet':\n",
    "                    args_map = {\n",
    "                        'predictor': 'predictions', 'reference': 'references', 'document': 'sources'\n",
    "                    }\n",
    "                    metric = get_cache_metric(query['metric_name'])\n",
    "                    args_dict = {}\n",
    "                    for actor_key, actor_pair in query['metric_actors'].items():    \n",
    "                        args_dict[args_map[actor_key]] = sentence_callable(query['reference_set'],\n",
    "                                                                           actor_pair[1],\n",
    "                                                                           actor_to_column_map[actor_pair[0]])\n",
    "                elif query['metric_name'] in ['perplexity']:\n",
    "                    actor_pair = list(query['metric_actors'].values())[0]\n",
    "                    metric = get_cache_metric(query['metric_name'])\n",
    "                    args_dict = perplexity_callable(query['reference_set'],\n",
    "                                                    actor_pair[1])\n",
    "                elif query['metric_name'] in ['frequency chatbot classifier']:\n",
    "                    actor_pair = list(query['metric_actors'].values())[0]\n",
    "                    metric = get_cache_metric(query['metric_name'],\n",
    "                                              mode=query['metric_params']['mode'])\n",
    "                    del query['metric_params']['mode']\n",
    "                    args_dict = {\n",
    "                        'sentences': sentence_callable(query['reference_set'],\n",
    "                                                       actor_pair[1],\n",
    "                                                       actor_to_column_map[actor_pair[0]])\n",
    "                    }\n",
    "                elif query['metric_name'] in ['distilbert-embedded chatbot classifier']:\n",
    "                    actor_pair = list(query['metric_actors'].values())[0]\n",
    "                    metric = get_cache_metric(query['metric_name'],\n",
    "                                              with_barney=query['metric_params']['with_barney'])\n",
    "                    del query['metric_params']['with_barney']\n",
    "                    args_dict = {\n",
    "                        'sentences': sentence_callable(query['reference_set'],\n",
    "                                                       actor_pair[1],\n",
    "                                                       actor_to_column_map[actor_pair[0]])\n",
    "                    }\n",
    "                elif query['metric_name'] in ['neural chatbot classifier']:\n",
    "                    actor_pair = list(query['metric_actors'].values())[0]\n",
    "                    classifier_char = query['metric_params']['classifier_char']\n",
    "                    args_dict = {\n",
    "                        'character': classifier_char,\n",
    "                        'load_path': os.path.join(base_folder, \"Data\", \"Characters\",\n",
    "                                      classifier_char, character_dict[classifier_char]['classifier_folder']),\n",
    "                    }\n",
    "                    metric = get_cache_metric(query['metric_name'],\n",
    "                                              classifier_char=classifier_char)\n",
    "                    del query['metric_params']['classifier_char']\n",
    "                    args_dict['sentences'] = sentence_callable(query['reference_set'],\n",
    "                                                               actor_pair[1],\n",
    "                                                               actor_to_column_map[actor_pair[0]])         \n",
    "                query_output['answer'] = metric.compute(**{**args_dict, **query['metric_params']})\n",
    "                results[query_hash] = query_output\n",
    "        except Exception as e:\n",
    "            print(\"Query failed due to \" + str(type(e)) + \" with message \" + str(e))\n",
    "        print()\n",
    "    print(\"Done.\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf531aa",
   "metadata": {},
   "source": [
    "# Example of Running an Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3be77e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric Name: See BBMetric.metrics_list\n",
    "# Metric Params: See optional and require params of each metric\n",
    "## NOTE: For neural chatbot classifier, add 'classifier_char' as a parameter\n",
    "# Metric Actors:\n",
    "## DATASET_CHARCONTEXT: (any character | \"Common\") + \"_df\"\n",
    "## DATASET_CHAR: (any character | \"Common\") + \"_df\"\n",
    "## DIALOGPT_GREEDY: any character | \"Base\"\n",
    "## DIALOGPT_NBEAMS: any character | \"Base\"\n",
    "## DIALOGPT_SAMPLE: any character | \"Base\"\n",
    "# Reference Set: (any character | \"Common\") + \"_df\"\n",
    "# Metric Attempt: Defaults to 0, add a number to save multiple runs of the same query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf0116aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    {\n",
    "        'metric_name': 'google bleu',\n",
    "        'metric_actors': {\n",
    "            'predictor': (MetricActor.DATASET_CHAR, 'Vader_df'),\n",
    "            'reference': (MetricActor.DATASET_CHARCONTEXT, 'Vader_df'),\n",
    "        },\n",
    "        'reference_set': 'Vader_df',\n",
    "        'metric_params': {},\n",
    "        'metric_attempt': 0\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5925112",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate_round(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a09bb1d",
   "metadata": {},
   "source": [
    "# Run Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f706444d",
   "metadata": {},
   "source": [
    "## Single Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c9fce35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor metric in [\\'distilbert-embedded chatbot classifier\\', \\'frequency chatbot classifier\\', \\'emotion classifier\\',\\n               \\'distinct\\', \\'repetitiveness\\', \\'t5 grammar correction edit distance\\', \\'flesch-kincaid index\\']:\\n    metric_pretty = BBMetric.load_metric(metric).pretty_name\\n    metric_params = dict()\\n    if metric == \"distilbert-embedded chatbot classifier\":\\n        metric_params = {\\'with_barney\\': True}\\n    elif metric == \"frequency chatbot classifier\":\\n        metric_params = {\\'mode\\': \\'c-tf-idf\\'}\\n    results = evaluate_round([\\n        {\\n            \\'metric_name\\': metric,\\n            \\'metric_actors\\': {\\n                \\'document\\': (MetricActor.DATASET_CHAR, char + \\'_df\\')\\n            },\\n            \\'reference_set\\': char + \\'_df\\',\\n            \\'metric_params\\': metric_params.copy(),\\n            \\'metric_attempt\\': 0\\n        } for char in characters + [\"Common\"]\\n    ] + [\\n        {\\n            \\'metric_name\\': metric,\\n            \\'metric_actors\\': {\\n                \\'document\\': (MetricActor.DIALOGPT_SAMPLE, char)\\n            },\\n            \\'reference_set\\': char + \\'_df\\',\\n            \\'metric_params\\': metric_params.copy(),\\n            \\'metric_attempt\\': 0\\n        } for char in characters\\n    ] + [\\n        {\\n            \\'metric_name\\': metric,\\n            \\'metric_actors\\': {\\n                \\'document\\': (MetricActor.DIALOGPT_SAMPLE, char)\\n            },\\n            \\'reference_set\\': \\'Common_df\\',\\n            \\'metric_params\\': metric_params.copy(),\\n            \\'metric_attempt\\': 0\\n        } for char in characters + [\"Base\"]\\n    ])\\n    metric_dict = load_metric_by_name(out_folder, metric_pretty)\\n    metric_dict = {**metric_dict, **results}\\n    save_metric_by_name(out_folder, metric_pretty, metric_dict)\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for metric in ['distilbert-embedded chatbot classifier', 'frequency chatbot classifier', 'emotion classifier',\n",
    "               'distinct', 'repetitiveness', 't5 grammar correction edit distance', 'flesch-kincaid index']:\n",
    "    metric_pretty = BBMetric.load_metric(metric).pretty_name\n",
    "    metric_params = dict()\n",
    "    if metric == \"distilbert-embedded chatbot classifier\":\n",
    "        metric_params = {'with_barney': True}\n",
    "    elif metric == \"frequency chatbot classifier\":\n",
    "        metric_params = {'mode': 'c-tf-idf'}\n",
    "    results = evaluate_round([\n",
    "        {\n",
    "            'metric_name': metric,\n",
    "            'metric_actors': {\n",
    "                'document': (MetricActor.DATASET_CHAR, char + '_df')\n",
    "            },\n",
    "            'reference_set': char + '_df',\n",
    "            'metric_params': metric_params.copy(),\n",
    "            'metric_attempt': 0\n",
    "        } for char in characters + [\"Common\"]\n",
    "    ] + [\n",
    "        {\n",
    "            'metric_name': metric,\n",
    "            'metric_actors': {\n",
    "                'document': (MetricActor.DIALOGPT_SAMPLE, char)\n",
    "            },\n",
    "            'reference_set': char + '_df',\n",
    "            'metric_params': metric_params.copy(),\n",
    "            'metric_attempt': 0\n",
    "        } for char in characters\n",
    "    ] + [\n",
    "        {\n",
    "            'metric_name': metric,\n",
    "            'metric_actors': {\n",
    "                'document': (MetricActor.DIALOGPT_SAMPLE, char)\n",
    "            },\n",
    "            'reference_set': 'Common_df',\n",
    "            'metric_params': metric_params.copy(),\n",
    "            'metric_attempt': 0\n",
    "        } for char in characters + [\"Base\"]\n",
    "    ])\n",
    "    metric_dict = load_metric_by_name(out_folder, metric_pretty)\n",
    "    metric_dict = {**metric_dict, **results}\n",
    "    save_metric_by_name(out_folder, metric_pretty, metric_dict)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0a7e1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmetric = 'neural chatbot classifier'\\nmetric_pretty = BBMetric.load_metric(metric).pretty_name\\nmetric_params = dict()\\nresults = evaluate_round(flatten([[\\n        {\\n            'metric_name': metric,\\n            'metric_actors': {\\n                'document': (MetricActor.DATASET_CHAR, char + '_df')\\n            },\\n            'reference_set': char + '_df',\\n            'metric_params': {'classifier_char': char},\\n            'metric_attempt': 0\\n        },\\n        {\\n            'metric_name': metric,\\n            'metric_actors': {\\n                'document': (MetricActor.DIALOGPT_SAMPLE, char)\\n            },\\n            'reference_set': char + '_df',\\n            'metric_params': {'classifier_char': char},\\n            'metric_attempt': 0\\n        },\\n        {\\n            'metric_name': metric,\\n            'metric_actors': {\\n                'document': (MetricActor.DIALOGPT_SAMPLE, char)\\n            },\\n            'reference_set': 'Common_df',\\n            'metric_params': {'classifier_char': char},\\n            'metric_attempt': 0\\n        },\\n        {\\n            'run': flush_cache_entries,\\n            'run_args': {\\n                'entries': [['trained_metric', 'neural chatbot classifier', char]]\\n            }\\n        }\\n] for char in characters\\n]))\\nmetric_dict = load_metric_by_name(out_folder, metric_pretty)\\nmetric_dict = {**metric_dict, **results}\\nsave_metric_by_name(out_folder, metric_pretty, metric_dict)\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "metric = 'neural chatbot classifier'\n",
    "metric_pretty = BBMetric.load_metric(metric).pretty_name\n",
    "metric_params = dict()\n",
    "results = evaluate_round(flatten([[\n",
    "        {\n",
    "            'metric_name': metric,\n",
    "            'metric_actors': {\n",
    "                'document': (MetricActor.DATASET_CHAR, char + '_df')\n",
    "            },\n",
    "            'reference_set': char + '_df',\n",
    "            'metric_params': {'classifier_char': char},\n",
    "            'metric_attempt': 0\n",
    "        },\n",
    "        {\n",
    "            'metric_name': metric,\n",
    "            'metric_actors': {\n",
    "                'document': (MetricActor.DIALOGPT_SAMPLE, char)\n",
    "            },\n",
    "            'reference_set': char + '_df',\n",
    "            'metric_params': {'classifier_char': char},\n",
    "            'metric_attempt': 0\n",
    "        },\n",
    "        {\n",
    "            'metric_name': metric,\n",
    "            'metric_actors': {\n",
    "                'document': (MetricActor.DIALOGPT_SAMPLE, char)\n",
    "            },\n",
    "            'reference_set': 'Common_df',\n",
    "            'metric_params': {'classifier_char': char},\n",
    "            'metric_attempt': 0\n",
    "        },\n",
    "        {\n",
    "            'run': flush_cache_entries,\n",
    "            'run_args': {\n",
    "                'entries': [['trained_metric', 'neural chatbot classifier', char]]\n",
    "            }\n",
    "        }\n",
    "] for char in characters\n",
    "]))\n",
    "metric_dict = load_metric_by_name(out_folder, metric_pretty)\n",
    "metric_dict = {**metric_dict, **results}\n",
    "save_metric_by_name(out_folder, metric_pretty, metric_dict)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22640fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmetric = \\'perplexity\\'\\nmetric_pretty = BBMetric.load_metric(metric).pretty_name\\nmetric_params = dict()\\nresults = evaluate_round([\\n    {\\n        \\'metric_name\\': metric,\\n        \\'metric_actors\\': {\\n            \\'predictor\\': (MetricActor.DIALOGPT_SAMPLE, charpair[0])\\n        },\\n        \\'reference_set\\': charpair[1] + \\'_df\\',\\n        \\'metric_params\\': {},\\n        \\'metric_attempt\\': 0\\n    } for charpair in [(\\'Joey\\', \\'Phoebe\\'), (\\'Joey\\', \\'Sheldon\\'), (\\'Bender\\', \\'Fry\\'), (\\'Bender\\', \\'Barney\\'),\\n                       (\\'Barney\\', \\'Harry\\')]\\n] + [\\n    {\\n        \\'metric_name\\': metric,\\n        \\'metric_actors\\': {\\n            \\'document\\': (MetricActor.DIALOGPT_SAMPLE, char)\\n        },\\n        \\'reference_set\\': char + \\'_df\\',\\n        \\'metric_params\\': metric_params.copy(),\\n        \\'metric_attempt\\': 0\\n    } for char in characters\\n] + [\\n    {\\n        \\'metric_name\\': metric,\\n        \\'metric_actors\\': {\\n            \\'document\\': (MetricActor.DIALOGPT_SAMPLE, \"Base\")\\n        },\\n        \\'reference_set\\': char + \\'_df\\',\\n        \\'metric_params\\': metric_params.copy(),\\n        \\'metric_attempt\\': 0\\n    } for char in characters\\n] + [\\n    {\\n        \\'metric_name\\': metric,\\n        \\'metric_actors\\': {\\n            \\'document\\': (MetricActor.DIALOGPT_SAMPLE, char)\\n        },\\n        \\'reference_set\\': \\'Common_df\\',\\n        \\'metric_params\\': metric_params.copy(),\\n        \\'metric_attempt\\': 0\\n    } for char in characters + [\"Base\"]\\n])\\nmetric_dict = load_metric_by_name(out_folder, metric_pretty)\\nmetric_dict = {**metric_dict, **results}\\nsave_metric_by_name(out_folder, metric_pretty, metric_dict)\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "metric = 'perplexity'\n",
    "metric_pretty = BBMetric.load_metric(metric).pretty_name\n",
    "metric_params = dict()\n",
    "results = evaluate_round([\n",
    "    {\n",
    "        'metric_name': metric,\n",
    "        'metric_actors': {\n",
    "            'predictor': (MetricActor.DIALOGPT_SAMPLE, charpair[0])\n",
    "        },\n",
    "        'reference_set': charpair[1] + '_df',\n",
    "        'metric_params': {},\n",
    "        'metric_attempt': 0\n",
    "    } for charpair in [('Joey', 'Phoebe'), ('Joey', 'Sheldon'), ('Bender', 'Fry'), ('Bender', 'Barney'),\n",
    "                       ('Barney', 'Harry')]\n",
    "] + [\n",
    "    {\n",
    "        'metric_name': metric,\n",
    "        'metric_actors': {\n",
    "            'document': (MetricActor.DIALOGPT_SAMPLE, char)\n",
    "        },\n",
    "        'reference_set': char + '_df',\n",
    "        'metric_params': metric_params.copy(),\n",
    "        'metric_attempt': 0\n",
    "    } for char in characters\n",
    "] + [\n",
    "    {\n",
    "        'metric_name': metric,\n",
    "        'metric_actors': {\n",
    "            'document': (MetricActor.DIALOGPT_SAMPLE, \"Base\")\n",
    "        },\n",
    "        'reference_set': char + '_df',\n",
    "        'metric_params': metric_params.copy(),\n",
    "        'metric_attempt': 0\n",
    "    } for char in characters\n",
    "] + [\n",
    "    {\n",
    "        'metric_name': metric,\n",
    "        'metric_actors': {\n",
    "            'document': (MetricActor.DIALOGPT_SAMPLE, char)\n",
    "        },\n",
    "        'reference_set': 'Common_df',\n",
    "        'metric_params': metric_params.copy(),\n",
    "        'metric_attempt': 0\n",
    "    } for char in characters + [\"Base\"]\n",
    "])\n",
    "metric_dict = load_metric_by_name(out_folder, metric_pretty)\n",
    "metric_dict = {**metric_dict, **results}\n",
    "save_metric_by_name(out_folder, metric_pretty, metric_dict)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987bfcb3",
   "metadata": {},
   "source": [
    "# COMET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16f17605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmetric = \"comet\"\\nmetric_pretty = BBMetric.load_metric(metric).pretty_name\\nmetric_params = dict()\\nresults = evaluate_round([\\n    {\\n        \\'metric_name\\': metric,\\n        \\'metric_actors\\': {\\n            \\'document\\': (MetricActor.DATASET_CHARCONTEXT, char + \\'_df\\'),\\n            \\'reference\\': (MetricActor.DATASET_CHAR, char + \"_df\"),\\n            \\'predictor\\': (MetricActor.DIALOGPT_SAMPLE, char)\\n        },\\n        \\'reference_set\\': char + \\'_df\\',\\n        \\'metric_params\\': {},\\n        \\'metric_attempt\\': 0\\n    } for char in characters\\n])\\nmetric_dict = load_metric_by_name(out_folder, metric_pretty)\\nmetric_dict = {**metric_dict, **results}\\nsave_metric_by_name(out_folder, metric_pretty, metric_dict)\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "metric = \"comet\"\n",
    "metric_pretty = BBMetric.load_metric(metric).pretty_name\n",
    "metric_params = dict()\n",
    "results = evaluate_round([\n",
    "    {\n",
    "        'metric_name': metric,\n",
    "        'metric_actors': {\n",
    "            'document': (MetricActor.DATASET_CHARCONTEXT, char + '_df'),\n",
    "            'reference': (MetricActor.DATASET_CHAR, char + \"_df\"),\n",
    "            'predictor': (MetricActor.DIALOGPT_SAMPLE, char)\n",
    "        },\n",
    "        'reference_set': char + '_df',\n",
    "        'metric_params': {},\n",
    "        'metric_attempt': 0\n",
    "    } for char in characters\n",
    "])\n",
    "metric_dict = load_metric_by_name(out_folder, metric_pretty)\n",
    "metric_dict = {**metric_dict, **results}\n",
    "save_metric_by_name(out_folder, metric_pretty, metric_dict)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fddbe56",
   "metadata": {},
   "source": [
    "# Pairwise Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4ba20ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor metric in [\\'bartscore\\', \\'bleurt\\', \\'bertscore\\', \\'roberta crossencoding similarity\\',\\n               \\'rouge l\\', \\'google bleu\\', \\'meteor\\', \\'bertscore\\', \\'bleurt\\', \\'term error rate\\']:\\n    metric_pretty = BBMetric.load_metric(metric).pretty_name\\n    results = evaluate_round([\\n        {\\n            \\'metric_name\\': metric,\\n            \\'metric_actors\\': {\\n                \\'reference\\': (MetricActor.DATASET_CHAR, char + \"_df\"),\\n                \\'predictor\\': (MetricActor.DIALOGPT_SAMPLE, char)\\n            },\\n            \\'reference_set\\': char + \\'_df\\',\\n            \\'metric_params\\': {},\\n            \\'metric_attempt\\': 0\\n        } for char in characters\\n    ] + [\\n        {\\n            \\'metric_name\\': metric,\\n            \\'metric_actors\\': {\\n                \\'reference\\': (MetricActor.DIALOGPT_SAMPLE, charpair[1]),\\n                \\'predictor\\': (MetricActor.DIALOGPT_SAMPLE, charpair[0])\\n            },\\n            \\'reference_set\\': \\'Common_df\\',\\n            \\'metric_params\\': {},\\n            \\'metric_attempt\\': 0\\n        } for charpair in [(\\'Joey\\', \\'Phoebe\\'), (\\'Joey\\', \\'Sheldon\\'), (\\'Bender\\', \\'Fry\\'), (\\'Bender\\', \\'Barney\\')]        \\n    ])\\n    metric_dict = load_metric_by_name(out_folder, metric_pretty)\\n    metric_dict = {**metric_dict, **results}\\n    save_metric_by_name(out_folder, metric_pretty, metric_dict)\\n\\nfor metric in [\\'extended edit distance\\', \\'word mover distance\\', \\'mpnet embedding similarity\\']:\\n    metric_pretty = BBMetric.load_metric(metric).pretty_name\\n    results = evaluate_round([\\n        {\\n            \\'metric_name\\': metric,\\n            \\'metric_actors\\': {\\n                \\'document0\\': (MetricActor.DATASET_CHAR, char + \"_df\"),\\n                \\'document1\\': (MetricActor.DIALOGPT_SAMPLE, char)\\n            },\\n            \\'reference_set\\': char + \\'_df\\',\\n            \\'metric_params\\': {},\\n            \\'metric_attempt\\': 0\\n        } for char in characters\\n    ] + [\\n        {\\n            \\'metric_name\\': metric,\\n            \\'metric_actors\\': {\\n                \\'document0\\': (MetricActor.DIALOGPT_SAMPLE, charpair[1]),\\n                \\'document1\\': (MetricActor.DIALOGPT_SAMPLE, charpair[0])\\n            },\\n            \\'reference_set\\': \\'Common_df\\',\\n            \\'metric_params\\': {},\\n            \\'metric_attempt\\': 0\\n        } for charpair in [(\\'Joey\\', \\'Phoebe\\'), (\\'Joey\\', \\'Sheldon\\'), (\\'Bender\\', \\'Fry\\'), (\\'Bender\\', \\'Barney\\'),\\n                           (\\'Barney\\', \\'Harry\\')]        \\n    ])\\n    metric_dict = load_metric_by_name(out_folder, metric_pretty)\\n    metric_dict = {**metric_dict, **results}\\n    save_metric_by_name(out_folder, metric_pretty, metric_dict)\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for metric in ['bartscore', 'bleurt', 'bertscore', 'roberta crossencoding similarity',\n",
    "               'rouge l', 'google bleu', 'meteor', 'bertscore', 'bleurt', 'term error rate']:\n",
    "    metric_pretty = BBMetric.load_metric(metric).pretty_name\n",
    "    results = evaluate_round([\n",
    "        {\n",
    "            'metric_name': metric,\n",
    "            'metric_actors': {\n",
    "                'reference': (MetricActor.DATASET_CHAR, char + \"_df\"),\n",
    "                'predictor': (MetricActor.DIALOGPT_SAMPLE, char)\n",
    "            },\n",
    "            'reference_set': char + '_df',\n",
    "            'metric_params': {},\n",
    "            'metric_attempt': 0\n",
    "        } for char in characters\n",
    "    ] + [\n",
    "        {\n",
    "            'metric_name': metric,\n",
    "            'metric_actors': {\n",
    "                'reference': (MetricActor.DIALOGPT_SAMPLE, charpair[1]),\n",
    "                'predictor': (MetricActor.DIALOGPT_SAMPLE, charpair[0])\n",
    "            },\n",
    "            'reference_set': 'Common_df',\n",
    "            'metric_params': {},\n",
    "            'metric_attempt': 0\n",
    "        } for charpair in [('Joey', 'Phoebe'), ('Joey', 'Sheldon'), ('Bender', 'Fry'), ('Bender', 'Barney')]        \n",
    "    ])\n",
    "    metric_dict = load_metric_by_name(out_folder, metric_pretty)\n",
    "    metric_dict = {**metric_dict, **results}\n",
    "    save_metric_by_name(out_folder, metric_pretty, metric_dict)\n",
    "\n",
    "for metric in ['extended edit distance', 'word mover distance', 'mpnet embedding similarity']:\n",
    "    metric_pretty = BBMetric.load_metric(metric).pretty_name\n",
    "    results = evaluate_round([\n",
    "        {\n",
    "            'metric_name': metric,\n",
    "            'metric_actors': {\n",
    "                'document0': (MetricActor.DATASET_CHAR, char + \"_df\"),\n",
    "                'document1': (MetricActor.DIALOGPT_SAMPLE, char)\n",
    "            },\n",
    "            'reference_set': char + '_df',\n",
    "            'metric_params': {},\n",
    "            'metric_attempt': 0\n",
    "        } for char in characters\n",
    "    ] + [\n",
    "        {\n",
    "            'metric_name': metric,\n",
    "            'metric_actors': {\n",
    "                'document0': (MetricActor.DIALOGPT_SAMPLE, charpair[1]),\n",
    "                'document1': (MetricActor.DIALOGPT_SAMPLE, charpair[0])\n",
    "            },\n",
    "            'reference_set': 'Common_df',\n",
    "            'metric_params': {},\n",
    "            'metric_attempt': 0\n",
    "        } for charpair in [('Joey', 'Phoebe'), ('Joey', 'Sheldon'), ('Bender', 'Fry'), ('Bender', 'Barney'),\n",
    "                           ('Barney', 'Harry')]        \n",
    "    ])\n",
    "    metric_dict = load_metric_by_name(out_folder, metric_pretty)\n",
    "    metric_dict = {**metric_dict, **results}\n",
    "    save_metric_by_name(out_folder, metric_pretty, metric_dict)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5926b622",
   "metadata": {},
   "source": [
    "## 10-Sentences Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b7991d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at E:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBotGit\\BarneyBot\\Data\\Characters\\Barney\\barney_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n",
      "Using custom data configuration default-9ad41ba7d956ebd3\n",
      "Found cached dataset csv (E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/cache/csv/default-9ad41ba7d956ebd3/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81666857486249e89bbd39d7721c9e3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cache at ['testset', 'Common_df']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 10%|                                                                          | 1/10 [00:03<00:27,  3.11s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 20%|                                                                  | 2/10 [00:04<00:18,  2.32s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 30%|                                                          | 3/10 [00:07<00:16,  2.39s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 40%|                                                 | 4/10 [00:08<00:11,  1.88s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 50%|                                         | 5/10 [00:09<00:07,  1.57s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 60%|                                 | 6/10 [00:12<00:08,  2.06s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 70%|                         | 7/10 [00:13<00:05,  1.71s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 80%|                | 8/10 [00:14<00:03,  1.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 90%|        | 9/10 [00:24<00:04,  4.17s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "100%|| 10/10 [00:26<00:00,  2.66s/it]\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at E:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBotGit\\BarneyBot\\Data\\Characters\\Vader\\vader_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n",
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 10%|                                                                          | 1/10 [00:05<00:49,  5.53s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 20%|                                                                  | 2/10 [00:07<00:26,  3.28s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 30%|                                                          | 3/10 [00:08<00:17,  2.50s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 40%|                                                 | 4/10 [00:12<00:17,  2.85s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 50%|                                         | 5/10 [00:14<00:13,  2.62s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 60%|                                 | 6/10 [00:16<00:09,  2.42s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 70%|                         | 7/10 [00:18<00:07,  2.37s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 80%|                | 8/10 [00:23<00:06,  3.16s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 90%|        | 9/10 [00:24<00:02,  2.52s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "100%|| 10/10 [00:26<00:00,  2.61s/it]\n"
     ]
    }
   ],
   "source": [
    "test_10 = dict()\n",
    "\n",
    "for char in ['Barney', 'Vader']:\n",
    "    test_10[char + '_model'] = get_cache_model(char)\n",
    "    if char == 'Barney':\n",
    "        test_10[char + '_context'] = get_cache_testset('Common', base_folder)['context/0'][0]\n",
    "        test_10[char + '_label'] = get_cache_testset('Common', base_folder)['response'][0]\n",
    "    elif char == 'Vader':\n",
    "        test_10[char + '_context'] = get_cache_testset('Common', base_folder)['context/0'][20]\n",
    "        test_10[char + '_label'] = get_cache_testset('Common', base_folder)['response'][20]\n",
    "    test_10[char + '_responses'] = list()\n",
    "    for i in tqdm(range(10)):\n",
    "        tokenized_question = cache.tokenizer.encode(test_10[char + '_context'] + cache.tokenizer.eos_token,\n",
    "                                                    return_tensors='tf')\n",
    "        max_length = 128 + tokenized_question.shape[1]\n",
    "        generated_answer = test_10[char + '_model'].generate(\n",
    "                            tokenized_question,\n",
    "                            pad_token_id=cache.tokenizer.eos_token_id,\n",
    "                            max_length=max_length,\n",
    "                            do_sample=True,\n",
    "                            top_k=BBData.top_k,\n",
    "                            top_p=BBData.top_p)[0].numpy().tolist()\n",
    "        generated_answer = generated_answer[len(tokenized_question[0]):]\n",
    "        test_10[char + '_responses'].append(cache.tokenizer.decode(generated_answer, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e32e8328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BARNEY\n",
      "\t-Context-\n",
      "\t\t* Barney, this is about the building.\n",
      "\t-Answers-\n",
      "\t\t* No way!\n",
      "\t\t* No, I said no.\n",
      "\t\t* I know it's a clich, but...\n",
      "\t\t* What building?\n",
      "\t\t* The building?\n",
      "\t\t* Okay, I guess I'll be taking it easy.\n",
      "\t\t* The building?\n",
      "\t\t* Ted. You okay?\n",
      "\t\t* Ted, your plan was flawless. Ted, I'm done with that. You've been a rock for six years. Six months, you're out of warranty. I call the fire department.\n",
      "\t\t* Ted, these people are ridiculous.\n",
      "\n",
      "VADER\n",
      "\t-Context-\n",
      "\t\t* I will not fight you.\n",
      "\t-Answers-\n",
      "\t\t* There is no doubt in my mind. I'm the Master. I will not fight you!\n",
      "\t\t* This is a trap!\n",
      "\t\t* You are too weak.\n",
      "\t\t* You are not a true Jedi until you have beaten The Senate.\n",
      "\t\t* Your men will not stand for this!\n",
      "\t\t* You must not think about me.\n",
      "\t\t* You have just accepted your fate.\n",
      "\t\t* Lord Vader, when you meet me again, I will not turn myself to your men.\n",
      "\t\t* My father.\n",
      "\t\t* Don't fight me.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for char in ['Barney', 'Vader']:\n",
    "    print(char.upper())\n",
    "    print(\"\\t-Context-\")\n",
    "    print(\"\\t\\t* \" + test_10[char + '_context'])\n",
    "    print(\"\\t-Answers-\")\n",
    "    for elem in test_10[char + '_responses']:\n",
    "        print('\\t\\t* ' + elem)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bbd0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing google bleu for character Barney\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|                                                                               | 1/21 [00:01<00:37,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing mpnet embedding similarity for character Barney\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|                                                                           | 2/21 [00:26<04:47, 15.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rouge l for character Barney\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|                                                                       | 3/21 [00:29<02:54,  9.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing meteor for character Barney\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tonel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tonel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\tonel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "\r",
      " 19%|                                                                   | 4/21 [00:30<01:48,  6.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing emotion classifier for character Barney\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Programs\\Anaconda\\envs\\barneybot\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      " 24%|                                                               | 5/21 [00:32<01:17,  4.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing roberta crossencoding similarity for character Barney\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|                                                       | 7/21 [00:43<01:04,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing distinct for character Barney\n",
      "Computing neural chatbot classifier for character Barney\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|                                                   | 8/21 [00:45<00:50,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing perplexity for character Barney\n",
      "Skipping Perplexity.\n",
      "Computing repetitiveness for character Barney\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|                                           | 10/21 [00:46<00:23,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing term error rate for character Barney\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|                                       | 11/21 [00:47<00:18,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing bertscore for character Barney\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      " 57%|                                   | 12/21 [00:53<00:27,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing comet for character Barney\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:comet.models.base:Path lightning_logs/cometinho_part-i/checkpoints/epoch=0-step=899999.ckpt does not exist!\n",
      "E:\\Programs\\Anaconda\\envs\\barneybot\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "E:\\Programs\\Anaconda\\envs\\barneybot\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "E:\\Programs\\Anaconda\\envs\\barneybot\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "E:\\Programs\\Anaconda\\envs\\barneybot\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "E:\\Programs\\Anaconda\\envs\\barneybot\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "E:\\Programs\\Anaconda\\envs\\barneybot\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "E:\\Programs\\Anaconda\\envs\\barneybot\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "E:\\Programs\\Anaconda\\envs\\barneybot\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "E:\\Programs\\Anaconda\\envs\\barneybot\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "E:\\Programs\\Anaconda\\envs\\barneybot\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|                               | 13/21 [01:04<00:40,  5.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing bleurt for character Barney\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:evaluate_modules.metrics.evaluate-metric--bleurt.98e148b2f8c4a88aba5037e4e0e90c9fd9ec35dc37a054ded8cfef0fa801ffab.bleurt:Using default BLEURT-Base checkpoint for sequence maximum length 128. You can use a bigger model for better results with e.g.: evaluate.load('bleurt', 'bleurt-large-512').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint C:\\Users\\tonel\\.cache\\huggingface\\metrics\\bleurt\\default\\downloads\\extracted\\b094b72f3dc7e1712a641ab624024c3b182ff714848ee334f1cc7a628d0b7798\\bleurt-base-128.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint C:\\Users\\tonel\\.cache\\huggingface\\metrics\\bleurt\\default\\downloads\\extracted\\b094b72f3dc7e1712a641ab624024c3b182ff714848ee334f1cc7a628d0b7798\\bleurt-base-128.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint bert_custom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint bert_custom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:bert_custom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:bert_custom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:vocab.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating WordPiece tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating WordPiece tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n",
      " 67%|                           | 14/21 [01:14<00:46,  6.60s/it]WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n",
      "WARNING:gensim.models.keyedvectors:At least one of the documents had no words that were in the vocabulary.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing word mover distance for character Barney\n",
      "Computing bartscore for character Barney\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|               | 17/21 [01:36<00:25,  6.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing extended edit distance for character Barney\n",
      "Computing t5 grammar correction edit distance for character Barney\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|           | 18/21 [01:58<00:31, 10.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing distilbert-embedded chatbot classifier for character Barney\n"
     ]
    }
   ],
   "source": [
    "metric_filename = \"10 Sentences Ranking.json\"\n",
    "\n",
    "metric_dict = {\n",
    "    'test_additional_data' : {\n",
    "        'generated_sentences': {\n",
    "            'Barney': test_10['Barney_responses'],\n",
    "            'Vader': test_10['Vader_responses']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "for char in ['Barney', 'Vader']:\n",
    "    for metric_name in tqdm(BBMetric.metrics_list):\n",
    "        print(\"Computing \" + metric_name + \" for character \" + char)\n",
    "        query_output = dict()\n",
    "        query_output['metric_name'] = metric_name\n",
    "        query_output['metric_version'] = 1\n",
    "        query_output['metric_attempt'] = 0\n",
    "        query_output['context'] = {\n",
    "            \"dialogpt_size\": \"small\",\n",
    "            \"dialogpt_context_sentences\": BBData.context_n,\n",
    "            \"dialogpt_nbeams_beams\": BBData.n_beams,\n",
    "            \"dialogpt_sample_top_p\": BBData.top_p,\n",
    "            \"dialogpt_sample_top_k\": BBData.top_k\n",
    "        }\n",
    "        query_output['metric_arity'] = get_metric_arity(metric_name)\n",
    "        query_output['metric_determinism'] = get_metric_determinism(metric_name, 1)\n",
    "        query_output['reference_set'] = [test_10[char + '_context']]\n",
    "        if metric_name in ['bartscore', 'rouge l', 'google bleu', 'meteor', 'bertscore', 'bleurt', 'term error rate',\n",
    "                           'bleurt', 'bertscore', 'roberta crossencoding similarity']:\n",
    "            metric = get_cache_metric(metric_name)\n",
    "            metric_params = dict()\n",
    "            query_output['metric_actors'] = {\n",
    "                'predictor': (MetricActor.DIALOGPT_SAMPLE, char),\n",
    "                'reference': [test_10[char + '_label']]\n",
    "            }\n",
    "            compute_args = [{\n",
    "                'predictions': sentence,\n",
    "                'references': test_10[char + '_label']\n",
    "            } for sentence in test_10[char + '_responses']]\n",
    "        elif metric_name in ['extended edit distance', 'word mover distance', 'mpnet embedding similarity']:\n",
    "            metric = get_cache_metric(metric_name)\n",
    "            metric_params = dict()\n",
    "            query_output['metric_actors'] = {\n",
    "                'document0': (MetricActor.DIALOGPT_SAMPLE, char),\n",
    "                'document1': [test_10[char + '_label']]\n",
    "            }\n",
    "            compute_args = [{\n",
    "                'sentences_a': sentence,\n",
    "                'sentences_b': test_10[char + '_label']\n",
    "            } for sentence in test_10[char + '_responses']]\n",
    "        elif metric_name in ['distilbert-embedded chatbot classifier', 'frequency chatbot classifier', 'emotion classifier',\n",
    "                             'distinct', 'repetitiveness', 't5 grammar correction edit distance', 'flesch-kincaid index']:\n",
    "            if metric_name == 'frequency chatbot classifier':\n",
    "                metric = get_cache_metric(metric_name, mode='c-tf-idf')\n",
    "                metric_params = {'mode': 'c-tf-idf'}\n",
    "            elif metric_name == 'distilbert-embedded chatbot classifier':\n",
    "                metric = get_cache_metric(metric_name, with_barney=True)\n",
    "                metric_params = {'with_barney': True}\n",
    "            metric = get_cache_metric(metric_name)\n",
    "            metric_params = dict()\n",
    "            query_output['metric_actors'] = {\n",
    "                'document': (MetricActor.DIALOGPT_SAMPLE, char)\n",
    "            }\n",
    "            compute_args = [{\n",
    "                'sentences': sentence\n",
    "            } for sentence in test_10[char + '_responses']]\n",
    "        elif metric_name == 'perplexity':\n",
    "            print(\"Skipping Perplexity.\")\n",
    "            continue\n",
    "        elif metric_name == 'neural chatbot classifier':\n",
    "            metric = get_cache_metric(metric_name, classifier_char=char)\n",
    "            metric_params = {'classifier_char': char}\n",
    "            query_output['metric_actors'] = {\n",
    "                'document': (MetricActor.DIALOGPT_SAMPLE, char)\n",
    "            }\n",
    "            responses_n = len(test_10[char + '_responses'])\n",
    "            compute_args = [{\n",
    "                'sentences': [test_10[char + '_responses'][i],\n",
    "                              test_10[char + '_responses'][(i+1) % responses_n],\n",
    "                              test_10[char + '_responses'][(i+2) % responses_n]],\n",
    "                'load_path': os.path.join(base_folder, \"Data\", \"Characters\", char, character_dict[char]['classifier_folder']),\n",
    "                'character': char\n",
    "            } for i in range(responses_n)]\n",
    "        elif metric_name == 'comet':\n",
    "            metric = get_cache_metric(metric_name)\n",
    "            metric_params = dict()\n",
    "            query_output['metric_actors'] = {\n",
    "                'document': [test_10[char + '_context']],\n",
    "                'predictor': (MetricActor.DIALOGPT_SAMPLE, char),\n",
    "                'reference': [test_10[char + '_label']]\n",
    "            }\n",
    "            compute_args = [{\n",
    "                'sources': test_10[char + '_context'],\n",
    "                'predictions': sentence,\n",
    "                'references': test_10[char + '_label']\n",
    "            } for sentence in test_10[char + '_responses']]\n",
    "        query_output['metric_params'] = metric_params\n",
    "        query_hash = dict_hash({'metric_name': query_output['metric_name'],\n",
    "                        'metric_version': query_output['metric_version'],\n",
    "                        'reference_set': query_output['reference_set'],\n",
    "                        'metric_attempt': query_output['metric_attempt'],\n",
    "                        'metric_actors': query_output['metric_actors'],\n",
    "                        'context': query_output['context'],\n",
    "                        'metric_params': query_output['metric_params']})\n",
    "        results = [metric.compute(**args) for args in compute_args]\n",
    "        query_output['answer'] = results\n",
    "        metric_dict = {**metric_dict, **query_output}\n",
    "\n",
    "save_metric_by_name(os.path.join(out_folder, 'Advanced Tests'), metric_filename, metric_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89c207c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

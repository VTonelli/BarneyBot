{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70fcf50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount google drive\n",
    "import os\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive',force_remount=True)\n",
    "    base_folder = '/content/drive/My Drive/unibo/NLP_project/BarneyBot'\n",
    "    os.system(\"pip install datasets\")\n",
    "    os.system(\"pip install transformers\")\n",
    "    os.system(\"pip install rouge_score\")\n",
    "    os.system(\"pip install -U sentence-transformers\")\n",
    "else:\n",
    "    base_folder = os.getcwd()\n",
    "    \n",
    "in_folder = os.path.join(base_folder, \"in\")\n",
    "if not os.path.exists(in_folder):\n",
    "    os.makedirs(in_folder)\n",
    "out_folder = os.path.join(base_folder, \"out\")\n",
    "if not os.path.exists(out_folder):\n",
    "    os.makedirs(out_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24485b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ed2f4026",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d57b85",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c94dba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "himym_path = os.path.join(base_folder, \"Datasets\", \"Characters\", \"Barney\")\n",
    "himym_df = pd.read_csv(os.path.join(himym_path, \"HIMYM_preprocessed.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50ae19e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>line</th>\n",
       "      <th>character</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>01x01</td>\n",
       "      <td>hey, so you know how I've always had a thing f...</td>\n",
       "      <td>Barney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>01x01</td>\n",
       "      <td>Okay, meet me at the bar in fifteen minutes, a...</td>\n",
       "      <td>Barney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>01x01</td>\n",
       "      <td>Where's your suit!? Just once when I say suit ...</td>\n",
       "      <td>Barney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>01x01</td>\n",
       "      <td>It was a blazer!</td>\n",
       "      <td>Barney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>01x01</td>\n",
       "      <td>I see what this is about. Have you forgotten w...</td>\n",
       "      <td>Barney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31210</th>\n",
       "      <td>08x24</td>\n",
       "      <td>I'm probably saying some political stuff right...</td>\n",
       "      <td>Barney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31214</th>\n",
       "      <td>08x24</td>\n",
       "      <td>Whoa. Is there going to be a fight?</td>\n",
       "      <td>Barney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31449</th>\n",
       "      <td>09x10</td>\n",
       "      <td>Karate Kid bad boy Billy Zabka, a shifty-eyed ...</td>\n",
       "      <td>Barney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31557</th>\n",
       "      <td>09x15</td>\n",
       "      <td>me or you?</td>\n",
       "      <td>Barney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31635</th>\n",
       "      <td>09x19</td>\n",
       "      <td>Lovers Foreve,  i  rand Ever and Ever: A Love ...</td>\n",
       "      <td>Barney</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5142 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      episode                                               line character\n",
       "14      01x01  hey, so you know how I've always had a thing f...    Barney\n",
       "16      01x01  Okay, meet me at the bar in fifteen minutes, a...    Barney\n",
       "18      01x01  Where's your suit!? Just once when I say suit ...    Barney\n",
       "20      01x01                                   It was a blazer!    Barney\n",
       "22      01x01  I see what this is about. Have you forgotten w...    Barney\n",
       "...       ...                                                ...       ...\n",
       "31210   08x24  I'm probably saying some political stuff right...    Barney\n",
       "31214   08x24                Whoa. Is there going to be a fight?    Barney\n",
       "31449   09x10  Karate Kid bad boy Billy Zabka, a shifty-eyed ...    Barney\n",
       "31557   09x15                                         me or you?    Barney\n",
       "31635   09x19  Lovers Foreve,  i  rand Ever and Ever: A Love ...    Barney\n",
       "\n",
       "[5142 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "himym_df[himym_df['character']=='Barney']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3528a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "himym_df['character'] = himym_df['character'].apply(lambda x: 1 if x=='Barney' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d1fd5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# himym_df['line'] = himym_df['line'].apply(lambda x: x[:128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5a770da",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>line</th>\n",
       "      <th>character</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>01x01</td>\n",
       "      <td>hey, so you know how I've always had a thing f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>01x01</td>\n",
       "      <td>Okay, meet me at the bar in fifteen minutes, a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>01x01</td>\n",
       "      <td>Where's your suit!? Just once when I say suit ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>01x01</td>\n",
       "      <td>It was a blazer!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>01x01</td>\n",
       "      <td>I see what this is about. Have you forgotten w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31210</th>\n",
       "      <td>08x24</td>\n",
       "      <td>I'm probably saying some political stuff right...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31214</th>\n",
       "      <td>08x24</td>\n",
       "      <td>Whoa. Is there going to be a fight?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31449</th>\n",
       "      <td>09x10</td>\n",
       "      <td>Karate Kid bad boy Billy Zabka, a shifty-eyed ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31557</th>\n",
       "      <td>09x15</td>\n",
       "      <td>me or you?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31635</th>\n",
       "      <td>09x19</td>\n",
       "      <td>Lovers Foreve,  i  rand Ever and Ever: A Love ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5142 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      episode                                               line  character\n",
       "14      01x01  hey, so you know how I've always had a thing f...          1\n",
       "16      01x01  Okay, meet me at the bar in fifteen minutes, a...          1\n",
       "18      01x01  Where's your suit!? Just once when I say suit ...          1\n",
       "20      01x01                                   It was a blazer!          1\n",
       "22      01x01  I see what this is about. Have you forgotten w...          1\n",
       "...       ...                                                ...        ...\n",
       "31210   08x24  I'm probably saying some political stuff right...          1\n",
       "31214   08x24                Whoa. Is there going to be a fight?          1\n",
       "31449   09x10  Karate Kid bad boy Billy Zabka, a shifty-eyed ...          1\n",
       "31557   09x15                                         me or you?          1\n",
       "31635   09x19  Lovers Foreve,  i  rand Ever and Ever: A Love ...          1\n",
       "\n",
       "[5142 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "himym_df[himym_df['character']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2450442",
   "metadata": {},
   "outputs": [],
   "source": [
    "himym_df = himym_df.drop(columns=['episode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eee39aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "himym_train, himym_test = train_test_split(himym_df, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8455cac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "himym_train = [(line['line'], line['character']) for _,line in himym_train.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e9d6b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "himym_test = [(line['line'], line['character']) for _,line in himym_test.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1c717896",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(himym_train, \n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True, \n",
    "                                          num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ac03e3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = torch.utils.data.DataLoader(himym_test, \n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=False, \n",
    "                                         num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "7c2be857",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = (0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ebeff1",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b779ba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7a48bccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: XLMRobertaModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "256b555d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encode('ciao').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "005cc1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "#         super(MyModel, self).__init__()\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(768, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = x['sentence_embedding']\n",
    "        print('\\n\\n x shape:\\t', y.shape, '\\n\\n')\n",
    "        y = self.fc(y)\n",
    "        y = self.sigmoid(y)\n",
    "        # x = nn.Softmax()\n",
    "        x['classifier'] = y\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ebda500b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: XLMRobertaModel >"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "36a72ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: XLMRobertaModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): MyModel(\n",
       "    (fc): Linear(in_features=768, out_features=1, bias=True)\n",
       "    (sigmoid): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.add_module('2', MyModel())\n",
    "model[0].train(mode=False)\n",
    "# for param in model[0].parameters():\n",
    "#     param.requires_grad = True\n",
    "model[1].train(mode=False)\n",
    "# for param in model[1].parameters():\n",
    "#     param.requires_grad = True\n",
    "model[2].train(mode=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6232faf",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7366057c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "bc0ed4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "inputs:\n",
      " [\"Yeah. I'll be bringing the crudits.\", 'Working? You are retired.', 'They tracked!', 'I think so.', 'And... send!', 'Unsubscribe.', \"Yeah, hell, yeah. I got 'em all: Teamwork, Courage, Awesomeness...\", 'Come on, man. People want to see Bruce Banner.They want Hulk. What?'] \n",
      "labels:\n",
      " tensor([0, 0, 0, 0, 0, 0, 1, 1])\n",
      "\n",
      "\n",
      " x shape:\t torch.Size([8, 768]) \n",
      "\n",
      "\n",
      "tensor([0.4871, 0.4938, 0.4782, 0.4909, 0.4890, 0.4955, 0.4862, 0.5026]) tensor([0., 0., 0., 0., 0., 0., 1., 1.])\n",
      "\n",
      "\n",
      "inputs:\n",
      " [\"They're barbecue.\", 'Probably!', 'He will watch it.', 'Bluntly.', 'Your move.', 'Mmm. Your dad?', 'And it does not stop there. We argue non-stop since.', \"OK, I don't really know how to tap-dance.\"] \n",
      "labels:\n",
      " tensor([0, 0, 0, 0, 0, 0, 1, 0])\n",
      "\n",
      "\n",
      " x shape:\t torch.Size([8, 768]) \n",
      "\n",
      "\n",
      "tensor([0.4851, 0.4940, 0.5036, 0.5002, 0.5050, 0.5053, 0.5061, 0.5160]) tensor([0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "\n",
      "\n",
      "inputs:\n",
      " ['Dear boy, what else?', 'We need it here. It is in hiding.', 'You hate that job.', \"Oh, my God, that sucks! Yeah. I'm gonna go do push-ups in the kitchen. Tantrum!\", 'Amanda refuses to return. She thinks you hate it. Marshall, you have to go apologize.', \"Oh, no way... you're the coolest.\", 'will I?', 'Will it work.'] \n",
      "labels:\n",
      " tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "\n",
      " x shape:\t torch.Size([8, 768]) \n",
      "\n",
      "\n",
      "tensor([0.4996, 0.4735, 0.4622, 0.5125, 0.4955, 0.4985, 0.4965, 0.4945]) tensor([0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "\n",
      "inputs:\n",
      " ['I spotted this girl there, and I will need help to land the plane.', \"I totally pull them off! It's a classic Western look.\", \"It's good. God thank you. What brings you here?\", \"Sure. What's up?\", \"You're not going to that show tonight!\", \"That doesn't sound familiar. Who wants Sloppy Joes?\", \"Hey. Come on. I mean, just because her life went one way and yours went another, it doesn't make your life any worse.\", \"Graduation goggles, like with high school. It's four years of bullies making fun of all the kids with braces, even after the braces come off and they can walk just fine. But then, on graduation day, you suddenly get all misty because you realize you're never going to see those jerks again. I just had graduation goggles with that guy Scooby I dated.\"] \n",
      "labels:\n",
      " tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "\n",
      " x shape:\t torch.Size([8, 768]) \n",
      "\n",
      "\n",
      "tensor([0.4974, 0.4942, 0.5062, 0.4810, 0.4918, 0.4776, 0.5037, 0.4797]) tensor([0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "\n",
      "inputs:\n",
      " [\"We're home on earth!\", 'What happened?', \"Look, Natalie, there's something I have to say and there's no good way to say it. I wanna break up. I don't think you're the one for me. I don't want to waste your time because I really like you. I wanna do right by you, and I think the best way to do that is just to be honest. I'm sorry.  Aah!\", 'Damn it.', 'Styrofoam trees.', 'Barney, I do not understand. You doing anything in these clips.', \"Move away. It's a Prada.\", 'Look, darling... dolphins.'] \n",
      "labels:\n",
      " tensor([0, 0, 0, 0, 0, 0, 1, 0])\n",
      "\n",
      "\n",
      " x shape:\t torch.Size([8, 768]) \n",
      "\n",
      "\n",
      "tensor([0.5139, 0.4737, 0.5175, 0.4774, 0.4743, 0.4697, 0.5021, 0.4850]) tensor([0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "\n",
      "\n",
      "inputs:\n",
      " [\"I don't want to talk about it.\", 'Honey, I do not have to worry about you and another woman.', 'I stubbed my toe, I missed my train... You put on the pants.', 'Iceketball? Just sounds weird.', \"Oops. Comin' right up!\", 'Great God, no! They belonged to Edgar Allan Poe!', 'You looked at my file.', \"I can't un-picture it.\"] \n",
      "labels:\n",
      " tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "\n",
      " x shape:\t torch.Size([8, 768]) \n",
      "\n",
      "\n",
      "tensor([0.4647, 0.4905, 0.4846, 0.4813, 0.4793, 0.4803, 0.4849, 0.4676]) tensor([0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "\n",
      "inputs:\n",
      " ['Grinchy, Grinch, Grinch, Grinch, Grinch, Grinch, Grinch.', \"You're Captain Hook.\", 'Hey, that is...', \"Really? That didn't work.\", \"Almighty TiVo, we thank you for all the gifts you have given us: the power to freeze live TV to go take a leak is nothing short of Godlike. Let's not forget fast-forwarding through commercials. It seems greedy to ask anything more from you, O magic box, but if you malfunction and miss the Super Bowl, we will destroy you in the\", \"Well, she'd have to with your spring-loaded toilet seat, wouldn't she?\", 'What?', 'A very big check!'] \n",
      "labels:\n",
      " tensor([0, 0, 0, 0, 0, 0, 0, 1])\n",
      "\n",
      "\n",
      " x shape:\t torch.Size([8, 768]) \n",
      "\n",
      "\n",
      "tensor([0.4689, 0.4845, 0.4779, 0.4820, 0.4844, 0.5055, 0.4879, 0.5044]) tensor([0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "\n",
      "\n",
      "inputs:\n",
      " ['Wendy the Waitress.', 'My God, have you ever had the baby! It is just behind me, huh?', \"It's a waste of time. Pumps!\", \"Ted, your problem is all you do is think, think, think. I'm teaching you how to do, do, do.\", \"Did I overhear that you're a surgeon?\", 'No! You can not have children! You want to raise a child in this crazy world.', 'Really? I remember that sale.', \"Oh, that's too bad, 'cause I was going to make a little presentation of my own.\"] \n",
      "labels:\n",
      " tensor([0, 1, 0, 1, 0, 1, 0, 0])\n",
      "\n",
      "\n",
      " x shape:\t torch.Size([8, 768]) \n",
      "\n",
      "\n",
      "tensor([0.4865, 0.4936, 0.4817, 0.4986, 0.4590, 0.5079, 0.5000, 0.4861]) tensor([0., 1., 0., 1., 0., 1., 0., 0.])\n",
      "\n",
      "\n",
      "inputs:\n",
      " [\"Nothing. That's it. Well, except for the phone booth. And the lamp by the desk... and the...\", \"I want her around all the time. I'd even consider not trying to sleep with her if that's what it takes. Guys, I've decided to seduce Michelle.\", 'Nice.  Nice. Hey, nice.', \"That's how I'm gonna use mine.\", 'Fine!', \"I'm still friends with Punchy.\", 'Hey. You feeling better?', 'Oh, my God.'] \n",
      "labels:\n",
      " tensor([0, 1, 1, 1, 0, 0, 0, 0])\n",
      "\n",
      "\n",
      " x shape:\t torch.Size([8, 768]) \n",
      "\n",
      "\n",
      "tensor([0.4757, 0.5190, 0.4987, 0.5016, 0.5079, 0.5018, 0.4937, 0.4677]) tensor([0., 1., 1., 1., 0., 0., 0., 0.])\n",
      "\n",
      "\n",
      "inputs:\n",
      " ['I get rid of this horrible Prngrphy on the field.', '... I quit, Arthur! I quit!', 'Look at us. It is horrible all the time like that?', \"Actually, we're cool. We just divided up CDs. It was all very civil. I'm proud of us. Wwe're, we're good.\", \"Nah, it's Kraft Croft night: mac and cheese and Tomb Raider. Biz-zow!\", 'Come on guys, you do it!', 'Marcus!', 'If serious, this is too funny.'] \n",
      "labels:\n",
      " tensor([0, 0, 0, 0, 0, 1, 0, 1])\n",
      "\n",
      "\n",
      " x shape:\t torch.Size([8, 768]) \n",
      "\n",
      "\n",
      "tensor([0.4695, 0.4867, 0.4823, 0.4992, 0.4702, 0.5062, 0.4931, 0.4783]) tensor([0., 0., 0., 0., 0., 1., 0., 1.])\n",
      "\n",
      "\n",
      "inputs:\n",
      " ['At least it was a great party.', 'Well, um, I better get back to these styrofoam trees.', 'No way. The last time I did this, the girl turned out to be engaged.', 'He calls you sweetie pie? He called me sweetie pie.', 'What?', \"You? Great! I'm coming.  Ranjit?\", \"Okay, that's it! You and me! I'm not afraid of you!\", 'Marshall, you might not want to hear this, but... is it at all possible this is the same'] \n",
      "labels:\n",
      " tensor([0, 0, 0, 0, 0, 0, 1, 0])\n",
      "\n",
      "\n",
      " x shape:\t torch.Size([8, 768]) \n",
      "\n",
      "\n",
      "tensor([0.4953, 0.4903, 0.5125, 0.4628, 0.4879, 0.5064, 0.4857, 0.4767]) tensor([0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "\n",
      "\n",
      "inputs:\n",
      " [\"It's The Diver.\", 'Righteous hair tab, brotha.', \"Well, we haven't actually decided anything yet so...\", 'How did you find me, anyway?', 'Yes! No! Lily, private convo time.', 'I knew it! You have not even given a single shot.', 'What the-no, no, no, no-come on!', \"Barney! You... You've got to stop living in these fairy tales that Mom told us! Bob Barker is not your father. Sam Gibbs might be, but Bob Barker is absolutely, unequivocally not your father.\"] \n",
      "labels:\n",
      " tensor([0, 0, 0, 0, 1, 0, 0, 0])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18120/825578890.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# forward + backward + optimize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# print('input shape:\\t', inputs.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'classifier'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m                 \u001b[0mout_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0moutput_value\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'token_embeddings'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[0mtrans_features\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'token_type_ids'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'token_type_ids'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0moutput_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mtrans_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m         \u001b[0moutput_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    846\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m         )\n\u001b[1;32m--> 848\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m    849\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    520\u001b[0m                 )\n\u001b[0;32m    521\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 522\u001b[1;33m                 layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m    523\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    447\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[0;32m    450\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m   2328\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2330\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 461\u001b[1;33m         \u001b[0mintermediate_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    462\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0): \n",
    "        \n",
    "        # print(data)\n",
    "        \n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        \n",
    "        inputs = list(inputs)\n",
    "        \n",
    "#         print('\\n\\ninputs:\\n', inputs,\n",
    "#               '\\nlabels:\\n', labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        # print('input shape:\\t', inputs.shape)\n",
    "        outputs = model.encode(inputs, output_value='classifier')\n",
    "        outputs = torch.concat(outputs)\n",
    "        labels = labels.type(torch.FloatTensor)\n",
    "#         outputs = model.encode('inputs')\n",
    "        print(outputs, labels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.requires_grad = True\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ea102b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

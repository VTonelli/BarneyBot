{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c3221e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Mount google drive (for Colab only)\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive',force_remount=True)\n",
    "    base_folder = '/content/drive/My Drive/unibo/NLP_project/BarneyBot'\n",
    "    # Install Huggingface libraries for running the notebook in Colab\n",
    "    os.system(\"pip install datasets\")\n",
    "    os.system(\"pip install transformers\")\n",
    "else:\n",
    "    base_folder = os.getcwd()\n",
    "\n",
    "# Import character dictionaries, useful to map a character to its data, and a fixed random seed\n",
    "from Data.data_dicts import character_dict, source_dict, random_state\n",
    "# Import BBMetrics library, usefull to performs metric scores\n",
    "from Lib.BBMetrics import BBMetric    \n",
    "\n",
    "# Import Huggingface transformers and load_dataset usefull for run the model and load datasets\n",
    "from transformers import TFAutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ecff908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the tokenizer for the pretrained model of DialoGPT small version\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small', cache_dir=os.path.join(os.getcwd(), \"cache\"))\n",
    "# Token used for padding by the tokenizer\n",
    "tokenizer.pad_token = '#'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cb6e20",
   "metadata": {},
   "source": [
    "# Human Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851144a0",
   "metadata": {},
   "source": [
    "In this notebook we ask to the user to perform a subjective evaluation according to some criteria:\n",
    "* _Coherency_: the chatbot does not contradict themselves over time\n",
    "* _Consistency_: the chatbot follows the flow of a conversation naturally\n",
    "* _Stylish_: the chatbot has a distinct personality, including related quirks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b344e128",
   "metadata": {},
   "source": [
    "The following function will load the dataset for evaluate the specified dataset (i.e. the common dataset used to evaluate each character bot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2268c68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-1255b828ba93d8fb\n",
      "Reusing dataset csv (C:\\Users\\User\\Documents\\GitHub\\cache\\csv\\default-1255b828ba93d8fb\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "327954da31a64d3eb9295f054602ec99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loads a common dataset used for evaluate every character bot  \n",
    "df_common = load_dataset('csv',\n",
    "                         data_files=os.path.join(base_folder, 'Data', 'common_dataset.csv'), \n",
    "                         cache_dir=os.path.join(base_folder, \"cache\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c92bd18",
   "metadata": {},
   "source": [
    "Below the user can find a function which performs 3 step in order to successfully evaluate the character specified in `character` parameter:\n",
    "1. **Chat evaluation**, for estimating the chatbot coherence (i.e. if the chatbot does not contradict themselves over time)\n",
    " * by giving a score from 0 to 5 (half score are not admitted)\n",
    " \n",
    "2. **Responses evaluation**, for estimating the chatbot consistency (i.e. how much true the chatbot' answers regarding to what the user previously said)\n",
    " * by giving a score from 0 to 5 (half score are not admitted)\n",
    " \n",
    "3. **Style evaluation**, for estimating the chatbot stylish (i.e. how much close are the answer of the chatbot according to what the user think the real character would say in response to him)\n",
    " * by giving a score from 0 to 5 (half score are not admitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "808dc2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_character(character='Default'):\n",
    "    # Takes the source location from the dictionary\n",
    "    source = character_dict[character]['source']\n",
    "    \n",
    "    # Checks if the character was trained \n",
    "    character_folder = os.path.join(base_folder, 'Data', 'Characters', character)\n",
    "    if not os.path.exists(character_folder):\n",
    "        raise Exception(\"The character \" + character + \" doesn't exist\")\n",
    "    \n",
    "    # Loads the pretrained model from the specified checkpoint folder `checkpoint_folder`\n",
    "    checkpoint_folder = os.path.join(character_folder, character_dict[character]['checkpoint_folder'])\n",
    "    model = TFAutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=checkpoint_folder)\n",
    "    \n",
    "    ### Compute human - coherence\n",
    "    print(\"Step 1) Chat with\", character, \"\\n\\tPlease evaluate your chat with this character:\", flush=True)\n",
    "    # Loads the metric\n",
    "    metric = BBMetric.load_metric(\"human - coherence\")\n",
    "    # Performs the metric evaluation\n",
    "    metric.train(model=model, tokenizer=tokenizer,\n",
    "                 filepath=os.path.join(os.getcwd(), \"Data\", \"Characters\", character, \"humancoherence.csv\"),\n",
    "                 length=5) # length is optional, defaults to 5\n",
    "    \n",
    "    ### Compute human - consistency\n",
    "    print(\"Step 2) Answers from\", character, \"\\n\\tPlease evaluate how true these responses are for the character:\", flush=True)\n",
    "    # Loads the metric\n",
    "    metric = BBMetric.load_metric(\"human - consistency\")\n",
    "    # Performs the metric evaluation\n",
    "    metric.train(model=model, tokenizer=tokenizer,\n",
    "                 filepath=os.path.join(os.getcwd(), \"Data\", \"Characters\", character, \"humanconsistency.csv\"))\n",
    "    ### Compute human - style\n",
    "    print(\"Step 3) Answers from\", character, \"\\n\\tPlease evaluate the style of the responses.\", flush=True)\n",
    "    print(\"\\tDo you think they are responses that\", character, \"would say?\", flush=True)\n",
    "    # Loads the metric\n",
    "    metric = BBMetric.load_metric(\"human - style\")\n",
    "    # Performs the metric evaluation\n",
    "    metric.train(model=model, tokenizer=tokenizer,\n",
    "                 filepath=os.path.join(os.getcwd(), \"Data\", \"Characters\", character, \"humanstyle.csv\"),\n",
    "                 questions=df_common['train'].filter(lambda x: x['source'] == source)['context'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e6a565",
   "metadata": {},
   "source": [
    "# Barney\n",
    "Evaluation of chatbot of _Barney Stinson_ from _How I Met Your Mother_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7dc1a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at C:\\Users\\User\\Documents\\GitHub\\Data\\Characters\\Barney\\barney_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1) Chat with Barney \n",
      "\tPlease evaluate your chat with this character:\n",
      ">> User:What is that suit?\n",
      "DialoGPT: Hook, line and sinker.\n",
      ">> User:Wow1\n",
      "DialoGPT: Wait. What... what are you talking about?\n",
      ">> User:I don't know, what are you talking about?\n",
      "DialoGPT: Look, Lily, I know you're excited about this. But it's not like I was saying hey, a million bucks if you make me wear a suit.\n",
      ">> User:I will give you a million bucks if you will wear a suit!\n",
      "DialoGPT: I want my money back!\n",
      ">> User:Which money?\n",
      "DialoGPT: What is it that you want?\n",
      "How do you rate this conversation (0 to 5)? 3\n",
      "Step 2) Answers from Barney \n",
      "\tPlease evaluate how true these responses are for the character:\n",
      "Question: Who are you?\n",
      "DialoGPT: Hey, guys. Tell us about yourselves.\n",
      "Question: What is your name?\n",
      "DialoGPT: Hann.\n",
      "Question: What is your job?\n",
      "DialoGPT: To tell people how great my job is.\n",
      "Question: Where do you live?\n",
      "DialoGPT: I don't remember saying the whole I don't like gambling thing. I just said it's bad for me. But if I'm going to take a gamble, it's probably getting me out of this thing.\n",
      "How do you rate these answers (0 to 5)? 1\n",
      "Step 3) Answers from Barney \n",
      "\tPlease evaluate the style of the responses.\n",
      "\tDo you think they are responses that Barney would say?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\User\\Documents\\GitHub\\cache\\csv\\default-1255b828ba93d8fb\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-dc68f8cad46f3147.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Barney, this is about the building.\n",
      "DialoGPT: I just have to say...\n",
      "Question: All right. I'll be right there. Stay where you are.\n",
      "DialoGPT: Okay, but only because I'm in a hurry, and I gotta take my dog to the vet. I'll be right back, and I gotta take care of the baby. Oh, and if you're still interested in that baby, get in here and get him fixed.\n",
      "Question: I think there's a pretty girl smiling at me there.\n",
      "DialoGPT: Look, I know I won't be sleeping through it, but it's still good to have you, either. Good luck.\n",
      "Question: I love you, man.\n",
      "DialoGPT: I'm so proud of you, dude.\n",
      "Question: Not even if she's hot?\n",
      "DialoGPT: Oh, well, I'd say yes. If she's hot. If she's hot...\n",
      "How do you rate these answers (0 to 5)? 2\n"
     ]
    }
   ],
   "source": [
    "eval_character(character='Barney')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e97db05",
   "metadata": {},
   "source": [
    "# Sheldon\n",
    "Evaluation of chatbot of _Sheldon Cooper_ from _The Big Bang Theory_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adae510d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_character(character='Sheldon')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d9a861",
   "metadata": {},
   "source": [
    "# Harry\n",
    "Evaluation of chatbot of _Harry Potter_ from _Harry Potter_ saga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747ad174",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_character(character='Harry')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ebfa26",
   "metadata": {},
   "source": [
    "# Fry\n",
    "Evaluation of chatbot of _Fry_ from _Futurama_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8584ca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_character(character='Fry')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df390a0c",
   "metadata": {},
   "source": [
    "# Bender\n",
    "Evaluation of chatbot of _Bender_ from _Futurama_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89de6774",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_character(character='Bender')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0c1a06",
   "metadata": {},
   "source": [
    "# Vader\n",
    "Evaluation of chatbot of _Darth Vader_ from _Star Wars_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec722a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_character(character='Vader')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292167a2",
   "metadata": {},
   "source": [
    "# Joey\n",
    "Evaluation of chatbot of _Joey_ from _Friends_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971cb8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_character(character='Joey')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a16b58",
   "metadata": {},
   "source": [
    "# Phoebe\n",
    "Evaluation of chatbot of _Phoebe_ from _Friends_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495be018",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_character(character='Phoebe')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

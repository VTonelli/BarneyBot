{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Lib.BBMetrics import BBMetric\n",
    "from transformers import TFAutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "from Data.data_dicts import character_dict, source_dict, random_state\n",
    "\n",
    "import os\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive',force_remount=True)\n",
    "    base_folder = '/content/drive/My Drive/unibo/NLP_project/BarneyBot'\n",
    "    os.system(\"pip install datasets\")\n",
    "    os.system(\"pip install transformers\")\n",
    "else:\n",
    "    base_folder = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small', cache_dir=os.path.join(os.getcwd(), \"cache\"))\n",
    "tokenizer.pad_token = '#'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-77d7688dea622ede\n",
      "Reusing dataset csv (D:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBot\\BarneyBot\\cache\\csv\\default-77d7688dea622ede\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ea740aef59a47ba8b2ab6b276428feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "df_common = load_dataset('csv',\n",
    "                         data_files=os.path.join(base_folder, 'Data', 'common_dataset.csv'), \n",
    "                         cache_dir=os.path.join(base_folder, \"cache\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_character(character='Default'):\n",
    "    source = character_dict[character]['source']\n",
    "    in_folder = os.path.join(base_folder, 'Data', 'Characters', character)\n",
    "    if not os.path.exists(in_folder):\n",
    "        os.makedirs(in_folder)\n",
    "    out_folder = os.path.join(base_folder, 'Data', 'Characters', character)\n",
    "    if not os.path.exists(out_folder):\n",
    "        os.makedirs(out_folder)\n",
    "    checkpoint_folder = os.path.join(out_folder, character_dict[character]['checkpoint_folder'])\n",
    "    model = TFAutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=checkpoint_folder)\n",
    "    \n",
    "    # compute human - coherence\n",
    "    print(\"Step 1) Chat with\", character, \"\\n\\tPlease evaluate your chat with this character:\", flush=True)\n",
    "    metric = BBMetric.load_metric(\"human - coherence\")\n",
    "    metric.train(model=model, tokenizer=tokenizer,\n",
    "                 filepath=os.path.join(os.getcwd(), \"Data\", \"Characters\", character, \"humancoherence.csv\"),\n",
    "                 length=5) # length is optional, defaults to 5\n",
    "    \n",
    "    # compute human - consistency\n",
    "    print(\"Step 2) Answers from\", character, \"\\n\\tPlease evaluate how true these responses are for the character:\", flush=True)\n",
    "    metric = BBMetric.load_metric(\"human - consistency\")\n",
    "    metric.train(model=model, tokenizer=tokenizer,\n",
    "                 filepath=os.path.join(os.getcwd(), \"Data\", \"Characters\", character, \"humanconsistency.csv\"))\n",
    "    # compute human - style\n",
    "    print(\"Step 3) Answers from\", character, \"\\n\\tPlease evaluate the style of the responses.\", flush=True)\n",
    "    print(\"\\tDo you think they are responses that\", character, \"would say?\", flush=True)\n",
    "    metric = BBMetric.load_metric(\"human - style\")\n",
    "    metric.train(model=model, tokenizer=tokenizer,\n",
    "                 filepath=os.path.join(os.getcwd(), \"Data\", \"Characters\", character, \"humanstyle.csv\"),\n",
    "                 questions=df_common['train'].filter(lambda x: x['source'] == source)['context'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Barney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at D:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBot\\BarneyBot\\Data\\Characters\\Barney\\barney_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3) Answers from Barney \n",
      "\tPlease evaluate the style of the responses.\n",
      "\tDo you think they are responses that Barney would say?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at D:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBot\\BarneyBot\\cache\\csv\\default-77d7688dea622ede\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-af6da9d0f5aaba29.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Barney, this is about the building.\n",
      "DialoGPT: That's all part of the plan.\n",
      "Question: All right. I'll be right there. Stay where you are.\n",
      "DialoGPT: Okay, okay. What do you want?\n",
      "Question: I think there's a pretty girl smiling at me there.\n",
      "DialoGPT: I see... a little girl is smiling there.\n",
      "Question: I love you, man.\n",
      "DialoGPT: Well, I love you, too.\n",
      "Question: Not even if she's hot?\n",
      "DialoGPT: And she's not even the hottest girl she's ever slept with.\n",
      "How do you rate these answers (0 to 5)? 4\n"
     ]
    }
   ],
   "source": [
    "eval_character(character='Barney')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sheldon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_character(character='Sheldon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_character(character='Harry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_character(character='Fry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_character(character='Vader')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_character(character='Joey')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phoebe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_character(character='Phoebe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_character(character='Bender')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "character = 'Vader' # 'Barney' | 'Sheldon' | 'Harry' | 'Fry' | 'Vader' | 'Joey' | 'Phoebe' | 'Default'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dict = {\n",
    "    'HIMYM':{\n",
    "        'dataset_folder': 'Episodes',\n",
    "    },\n",
    "    'Futurama':{\n",
    "        'dataset_folder': 'Episodes',\n",
    "    },\n",
    "    'Friends':{\n",
    "        'dataset_folder': None,\n",
    "    },\n",
    "    'HP':{\n",
    "        'dataset_folder': None,\n",
    "    },\n",
    "    'SW':{\n",
    "        'dataset_folder': 'Scripts',\n",
    "    },\n",
    "    'TBBT':{\n",
    "        'dataset_folder': 'Episodes',\n",
    "    },\n",
    "}\n",
    "\n",
    "character_dict = {\n",
    "    'Barney': {\n",
    "        'source':'HIMYM',\n",
    "        'delete_names':[\"Barney's Secretary\",\n",
    "                        'Marshall to Barney',\n",
    "                        \"Barney's mom\",\n",
    "                        'Ted, from seeing Barney',\n",
    "                        'Lily, holding Barney',\n",
    "                        'Marshall, on the phone with Barney',\n",
    "                        \"At Casa a pezzi. Barney is playing the piano.Ted's father\",\n",
    "                        'Marshall, to the girl Barney is talking to']\n",
    "    },\n",
    "    'Sheldon': {\n",
    "        'source':'TBBT',\n",
    "        'delete_names':[]\n",
    "    },\n",
    "    'Harry': {\n",
    "        'source':'HP',\n",
    "        'delete_names':[]\n",
    "    },\n",
    "    'Fry': {\n",
    "        'source':'Futurama',\n",
    "        'delete_names':['Mrs fry',\n",
    "                        'Mr fry',\n",
    "                        'Luck of the fryrish']\n",
    "    },\n",
    "    'Vader': {\n",
    "        'source':'SW',\n",
    "        'delete_names':[\"INT. DARTH VADER'S WINGMAN - COCKPIT\"]\n",
    "    },\n",
    "    'Joey': {\n",
    "        'source':'Friends',\n",
    "        'delete_names':[\"Joeys Sisters\",\n",
    "                        'Joey\\'s Date', \n",
    "                        \"Joey's Look-A-Like\", \n",
    "                        'Joeys Sister', \n",
    "                        \"Joey's Doctor\", \n",
    "                        \"Joey's Hand Twin\", \n",
    "                        'Joeys Date', \n",
    "                        'Joeys Grandmother']\n",
    "    },\n",
    "    'Phoebe': {\n",
    "        'source':'Friends',\n",
    "        'delete_names':['Amy turns around to Phoebe',\n",
    "                        'Phoebe Waitress']\n",
    "    },\n",
    "    'Default':None\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "if character != 'Default':\n",
    "    source = character_dict[character]['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount google drive (for Colab only)\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive',force_remount=True)\n",
    "    base_folder = '/content/drive/My Drive/unibo/NLP_project/BarneyBot'\n",
    "else:\n",
    "    base_folder = os.getcwd()\n",
    "    \n",
    "in_folder = os.path.join(base_folder, \"Data\", 'Characters', character)\n",
    "if not os.path.exists(in_folder):\n",
    "    os.makedirs(in_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the dataset documents and store their data into a DataFrame\n",
    "def load_dataset():\n",
    "    def _load_himym_friends_tbbt_dataset(sources_folder):\n",
    "        dataframe_rows = []\n",
    "        # Get number of documents and their names\n",
    "        documents_n = len(os.listdir(sources_folder))\n",
    "        documents_names = os.listdir(sources_folder)\n",
    "        # Loop over documents\n",
    "        for i in tqdm(range(documents_n)):\n",
    "            filename = documents_names[i]\n",
    "            sources_label = filename[:-4]\n",
    "            # Open document\n",
    "            with open(os.path.join(sources_folder, filename), encoding=\"utf8\") as file:\n",
    "                # Loop over lines (= words)\n",
    "                for line in file.readlines():\n",
    "                        dataframe_row = {\n",
    "                            \"source\": sources_label,\n",
    "                            \"line\": line,\n",
    "                        }\n",
    "                        dataframe_rows.append(dataframe_row)\n",
    "        # Build the dataframe from the words\n",
    "        df = pd.DataFrame(dataframe_rows)\n",
    "        return df\n",
    "    def _load_futurama_dataset(sources_folder):\n",
    "        futurama_txt = ''\n",
    "        # Loop over documents\n",
    "        for filename in tqdm(os.listdir(sources_folder)):\n",
    "            futurama_txt += open(os.path.join(sources_folder, filename)).read()\n",
    "        # Split lines\n",
    "        start_idx = 0\n",
    "        end_idx = 0\n",
    "        lines = []\n",
    "        while start_idx < len(futurama_txt):\n",
    "            start_idx = futurama_txt.find('<b>', end_idx)\n",
    "            if start_idx == -1: # if no '<b>' is found, just save the rest\n",
    "                lines.append(futurama_txt[end_idx:].replace('</b>',''))\n",
    "                break\n",
    "            elif start_idx != end_idx: # '<b>' is found\n",
    "                lines.append(futurama_txt[end_idx+4:start_idx])\n",
    "            end_idx = futurama_txt.find('</b>', start_idx)\n",
    "            if end_idx == -1: # if no '</b>' is found, just save the rest\n",
    "                lines.append(futurama_txt[start_idx:].replace('<b>',''))\n",
    "                break\n",
    "            lines.append(futurama_txt[start_idx+3:end_idx])\n",
    "        df = pd.DataFrame(lines, columns=['line'])\n",
    "        return df\n",
    "    def _load_hp_dataset(sources_folder):\n",
    "        sep = ';'\n",
    "        df = None\n",
    "        df_files = []\n",
    "        for filename in os.listdir(sources_folder):\n",
    "            df_files.append(pd.read_csv(os.path.join(sources_folder, filename), sep=sep).rename(columns = lambda x: x.lower()))\n",
    "        df = pd.concat(df_files)\n",
    "        df = df.rename(columns = {'character':'character', 'sentence':'line'})\n",
    "        return df\n",
    "    def _load_sw_dataset(source_folder):\n",
    "        dataframe_rows = []\n",
    "        # Get number of documents and their names\n",
    "        documents_n = len(os.listdir(source_folder))\n",
    "        documents_names = os.listdir(source_folder)\n",
    "        # Loop over documents\n",
    "        for i in tqdm(range(documents_n)):\n",
    "            filename = documents_names[i]\n",
    "            film_name = filename[:-4]\n",
    "            # Open document\n",
    "            with open(os.path.join(source_folder, filename)) as file:\n",
    "                film_rows = []\n",
    "                sentence = \"\"\n",
    "                empty_line_allow = False\n",
    "                between_numbers = False\n",
    "                found_character = False\n",
    "                for line in file.readlines():\n",
    "                    if re.search(r\"^[0-9]+.\", line) != None: # Line is number followed by dot (page number)\n",
    "                        pass\n",
    "                    elif re.search(r\"^[A-Z]{2,}\", line) != None: # Line begins with an-all caps (a character)\n",
    "                        sentence += line\n",
    "                        found_character = True\n",
    "                        empty_line_allow = True\n",
    "                    elif line.isspace():\n",
    "                        if empty_line_allow:\n",
    "                            pass\n",
    "                        else:\n",
    "                            if found_character:\n",
    "                                film_row = {\n",
    "                                    \"film\": film_name,\n",
    "                                    \"line\": sentence,\n",
    "                                }\n",
    "                                film_rows.append(film_row)\n",
    "                                sentence = \"\"\n",
    "                                found_character = False\n",
    "                    elif found_character:\n",
    "                        sentence += line\n",
    "                        empty_line_allow = False\n",
    "                dataframe_rows.extend(film_rows)\n",
    "        # Build the dataframe from the words\n",
    "        df = pd.DataFrame(dataframe_rows)\n",
    "        return df\n",
    "    ### Function starts here\n",
    "    if character == 'Default':\n",
    "        return None\n",
    "    sources_subfolder = source_dict[source]['dataset_folder']\n",
    "    if sources_subfolder:\n",
    "        sources_folder = os.path.join(base_folder, \"Data\", \"Sources\", source, sources_subfolder)\n",
    "    else:\n",
    "        sources_folder = os.path.join(base_folder, \"Data\", \"Sources\", source)\n",
    "    if source == 'HIMYM' or source == 'Friends' or source == 'TBBT':\n",
    "        df = _load_himym_friends_tbbt_dataset(sources_folder)\n",
    "    elif source == 'Futurama':\n",
    "        df = _load_futurama_dataset(sources_folder)\n",
    "    elif source == 'HP':\n",
    "        df = _load_hp_dataset(sources_folder)\n",
    "    elif source == 'SW':\n",
    "        df = _load_sw_dataset(sources_folder)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 46.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Dataset!\n",
      "\n",
      "                        film  \\\n",
      "0  Star Wars IV - A New Hope   \n",
      "1  Star Wars IV - A New Hope   \n",
      "2  Star Wars IV - A New Hope   \n",
      "3  Star Wars IV - A New Hope   \n",
      "4  Star Wars IV - A New Hope   \n",
      "\n",
      "                                                line  \n",
      "0  THREEPIO\\nDid you hear that? They've shut\\ndow...  \n",
      "1               THREEPIO (CONTâ€™D)\\nWe're doomed!\\n  \n",
      "2  THREEPIO (CONTâ€™D)\\nThere'll be no escape for...  \n",
      "3                THREEPIO (CONTâ€™D)\\nWhat's that?\\n  \n",
      "4  THREEPIO\\nI should have known better than to t...  \n",
      "film    2927\n",
      "line    2927\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Execute creation of dataset\n",
    "df = load_dataset()\n",
    "if not isinstance(df, type(None)):\n",
    "    print(\"Loaded Dataset!\")\n",
    "    print()\n",
    "    print(df.head())\n",
    "    print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "def process_dataset(df):\n",
    "    def _process_himym_dataset(df):\n",
    "        df = df[~df['line'].str.startswith(\"[\")]\n",
    "        df = df[~df['line'].str.startswith(\"(\")]\n",
    "        df['line'] = df['line'].str.strip()\n",
    "        df['line'] = df['line'].str.replace(r\"\\(.*\\)\",\"\")\n",
    "        df['line'] = df['line'].str.replace(r\"[\\/(){}\\[\\]\\|@_#]|\\\\t|\\\\n\",\" \")\n",
    "        df['line'] = df['line'].str.replace(r\"[^.\\',;:?!0-9a-zA-Z \\-]\",\"\")\n",
    "        df = df[~df['line'].isnull()]\n",
    "        df[['character', 'line']] = df['line'].str.split(\":\", 1, expand=True)\n",
    "        df = df.dropna()\n",
    "        df['line'] = df['line'].str.strip()\n",
    "        df['line'] = df['line'][df['line'].str.len() >= 2]\n",
    "        df = df[~df['line'].isnull()]\n",
    "        df = df.replace(r'^s*$', float('NaN'), regex = True)\n",
    "        df = df.dropna()\n",
    "        df = df.reset_index(drop=True)\n",
    "        return df\n",
    "    def _process_tbbt_dataset(df):\n",
    "        df = df[~df['line'].str.startswith(\"[\")]\n",
    "        df = df[~df['line'].str.startswith(\"(\")]\n",
    "        df = df[~df['line'].str.startswith(\"Scene: \")]\n",
    "        df['line'] = df['line'].str.strip()\n",
    "        df['line'] = df['line'].str.replace(r\"\\(.*\\)\",\"\")\n",
    "        df['line'] = df['line'].str.replace(r\"[\\/(){}\\[\\]\\|@_#]|\\\\t|\\\\n\",\" \")\n",
    "        df['line'] = df['line'].str.replace(r\"[^.\\',;:?!0-9a-zA-Z \\-]\",\"\")\n",
    "        df = df[~df['line'].isnull()]\n",
    "        df[['character', 'line']] = df['line'].str.split(\":\", 1, expand=True)\n",
    "        df = df.dropna()\n",
    "        df['line'] = df['line'].str.strip()\n",
    "        df['line'] = df['line'][df['line'].str.len() >= 2]\n",
    "        df = df[~df['line'].isnull()]\n",
    "        df = df.replace(r'^s*$', float('NaN'), regex = True)\n",
    "        df = df.dropna()\n",
    "        df = df.reset_index(drop=True)\n",
    "        return df\n",
    "    def _process_futurama_dataset(df):\n",
    "        df['line'] = df['line'].str.strip()\n",
    "        df['line'] = df['line'].str.replace(r\"\\[.*\\]\",\"\")\n",
    "        df['line'] = df['line'].str.replace(r\"\\(.*\\)\",\"\")\n",
    "        df['line'] = df['line'].str.replace(r\"\\<.*\\>\",\"\")\n",
    "        df['line'] = df['line'].str.replace(r\"\\s+\",\" \")\n",
    "        df['line'] = df['line'].str.replace(\"\\n\",\"\")\n",
    "        df = df[~df['line'].str.startswith(\"(\")]\n",
    "        df = df[~df['line'].str.startswith(\"[\")]\n",
    "        df['line'] = df['line'].str.replace(r\"[\\/(){}\\[\\]\\|@_#]|\\\\t|\\\\n\",\" \")\n",
    "        df['line'] = df['line'].str.replace(r\"[^.\\',;:?!0-9a-zA-Z \\-]\",\"\")\n",
    "        df['line'] = df['line'][df['line'].str.len() >= 2]\n",
    "        df = df.dropna()\n",
    "        df = df.reset_index(drop=True)\n",
    "        df_rows = []\n",
    "        for row in tqdm(range(len(df)-1)):\n",
    "            if df['line'][row].isupper():\n",
    "                df_row = {\n",
    "                    'line': df['line'][row+1].strip()[:512],\n",
    "                    'character': df['line'][row].strip().capitalize()\n",
    "                }\n",
    "                df_rows.append(df_row)\n",
    "        df = pd.DataFrame(df_rows)\n",
    "        df = df[df['character'].str.contains('Futurama')==False]\n",
    "        df = df.replace(r'^s*$', float('NaN'), regex = True)\n",
    "        df = df.dropna()\n",
    "        df = df.reset_index(drop=True)\n",
    "        return df\n",
    "    def _process_friends_dataset(df):\n",
    "        df = df[~df['line'].str.startswith(\"[\")]\n",
    "        df = df[~df['line'].str.startswith(\"(\")]\n",
    "        df['line'] = df['line'].str.strip()\n",
    "        df['line'] = df['line'].str.replace(r\"\\(.*\\)\",\"\")\n",
    "        df['line'] = df['line'].str.replace(r\"[\\/(){}\\[\\]\\|@_#]|\\\\t|\\\\n\",\" \")\n",
    "        df['line'] = df['line'].str.replace(r\"[^.\\',;:?!0-9a-zA-Z \\-]\",\"\")\n",
    "        df = df[~df['line'].isnull()]\n",
    "        df[['character', 'line']] = df['line'].str.split(\":\", 1, expand=True)\n",
    "        df = df.dropna()\n",
    "        df['line'] = df['line'].str.strip()\n",
    "        df['line'] = df['line'][df['line'].str.len() >= 2]\n",
    "        df = df[~df['line'].isnull()]\n",
    "        df = df[~(df['character'] == 'Written by')]\n",
    "        df = df.replace(r'^s*$', float('NaN'), regex = True)\n",
    "        df = df.dropna()\n",
    "        df = df.reset_index(drop=True)\n",
    "        return df\n",
    "    def _process_sw_dataset(df):\n",
    "        df = df[~df['line'].str.startswith(\"[\")]\n",
    "        df = df[~df['line'].str.startswith(\"(\")]\n",
    "        df['line'] = df['line'].str.strip()\n",
    "        df['line'] = df['line'].str.replace(r\"\\(.*\\)\",\"\")\n",
    "        df[['character', 'line']] = df['line'].str.split(\"\\n\", 1, expand=True)\n",
    "        df['line'] = df['line'].str.replace(r\"[\\/(){}\\[\\]\\|@_#]|\\\\t|\\\\n\",\" \")\n",
    "        df['line'] = df['line'].str.replace(r\"[^.\\',;:?!0-9a-zA-Z \\-]\",\"\")\n",
    "        df = df[~df['line'].isnull()]\n",
    "        df = df[df['character'].str.split().apply(lambda l: len(l)) <= 6]\n",
    "        df = df.replace(r'^s*$', float('NaN'), regex = True)\n",
    "        df = df.dropna()\n",
    "        df = df.reset_index(drop=True)\n",
    "        return df\n",
    "    def _process_hp_dataset(df):\n",
    "        df['line'] = df['line'].str.strip()\n",
    "        df['line'] = df['line'].str.replace(r\"\\(.*\\)\",\"\")\n",
    "        df['line'] = df['line'].str.replace(r\"[\\/(){}\\[\\]\\|@_#]|\\\\t|\\\\n\",\" \")\n",
    "        df['line'] = df['line'].str.replace(r\"[^.\\',;:?!0-9a-zA-Z \\-]\",\"\")\n",
    "        df = df[~df['line'].isnull()]\n",
    "        df = df.dropna()\n",
    "        df['line'] = df['line'].str.strip()\n",
    "        df['character'] = [line.lower() for line in df['character']]\n",
    "        df = df[~df['line'].isnull()]\n",
    "        df = df.replace(r'^s*$', float('NaN'), regex = True)\n",
    "        df = df.dropna()\n",
    "        df = df.reset_index(drop=True)\n",
    "        return df\n",
    "    # Function starts here\n",
    "    if character == 'Default':\n",
    "        return None\n",
    "    if source == 'HIMYM':\n",
    "        df = _process_himym_dataset(df)\n",
    "    elif source == 'Friends':\n",
    "        df = _process_friends_dataset(df)\n",
    "    elif source == 'Futurama':\n",
    "        df = _process_futurama_dataset(df)\n",
    "    elif source == 'TBBT':\n",
    "        df = _process_tbbt_dataset(df)\n",
    "    elif source == 'HP':\n",
    "        df = _process_hp_dataset(df)\n",
    "    elif source == 'SW':\n",
    "        df = _process_sw_dataset(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Dataset into line-character format!\n",
      "\n",
      "                        film  \\\n",
      "0  Star Wars IV - A New Hope   \n",
      "1  Star Wars IV - A New Hope   \n",
      "2  Star Wars IV - A New Hope   \n",
      "3  Star Wars IV - A New Hope   \n",
      "4  Star Wars IV - A New Hope   \n",
      "\n",
      "                                                line  character  \n",
      "0  Did you hear that? They've shutdown the main r...   THREEPIO  \n",
      "1                                      We're doomed!  THREEPIO   \n",
      "2   There'll be no escape for thePrincess this time.  THREEPIO   \n",
      "3                                       What's that?  THREEPIO   \n",
      "4  I should have known better than to trust the l...   THREEPIO  \n",
      "2750\n"
     ]
    }
   ],
   "source": [
    "df = process_dataset(df)\n",
    "if not isinstance(df, type(None)):\n",
    "    print(\"Processed Dataset into line-character format!\")\n",
    "    print()\n",
    "    print(df.head())\n",
    "    print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters contanining Vader : {'VADER\\t', \"INT. DARTH VADER'S COCKPIT\", 'VADER ', 'VADER', \"INT. VADER'S STAR DESTROYER - BRIDGE\", \"EXT. DARTH VADER'S TIE FIGHTER\", \"INT. DARTH VADER'S WINGMAN - COCKPIT\"} 161\n"
     ]
    }
   ],
   "source": [
    "if not isinstance(df, type(None)):\n",
    "    char_names = [c for c in df['character'] if character.lower() in c.lower()]\n",
    "    print(\"Characters contanining\", character, \":\", set(char_names), len(char_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isinstance(df, type(None)):\n",
    "    char_names = set(char_names) - set(character_dict[character]['delete_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isinstance(df, type(None)):\n",
    "    df['character'] = df['character'].apply(lambda x: character if x in char_names else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique character names in dataset after name processing: ['THREEPIO' 'THREEPIO ' 'LUKE' 'IMPERIAL OFFICER' 'Vader' 'REBEL OFFICER'\n",
      " 'TROOPER' 'TROOPER ' 'CHIEF PILOT' 'CAPTAIN' 'WOMAN' 'FIXER' 'CAMIE'\n",
      " 'LUKE ' 'BIGGS' 'DEAK' 'DAY' 'LEIA' 'COMMANDER' 'SECOND OFFICER'\n",
      " 'INT. SANDCRAWLER - HOLD AREA' 'INT. SANDCRAWLER - PRISON AREA'\n",
      " 'EXT. TATOOINE - DESERT - DAY' 'FIRST TROOPER' 'SECOND TROOPER'\n",
      " 'EXT. TATOOINE - DUNES' 'INT. SANDCRAWLER' 'BERU' 'OWEN' 'OWEN '\n",
      " 'AUNT BERU' 'EXT. TATOOINE - LARS HOMESTEAD'\n",
      " 'INT. LARS HOMESTEAD - PLAZA' 'BEN' 'BEN ' 'LEIA ' 'EXT. SPACE.'\n",
      " 'INT. DEATH STAR - CONFERENCE ROOM' 'TAGGE' 'MOTTI' 'TARKIN'\n",
      " 'EXT. TATOOINE - WASTELAND' 'EXT. SPACE'\n",
      " 'INT. DEATH STAR - DETENTION CORRIDOR' 'BARTENDER' 'CREATURE' 'HUMAN'\n",
      " 'HUMAN ' 'HAN' 'HAN ' 'GREEDO' 'SPEEDER LOT' 'JABBA'\n",
      " 'INT. MILLENNIUM FALCON' 'INT. MOS EISLEY SPACEPORT -'\n",
      " 'EXT. SPACE - PLANET TATOOINE' 'INT. MILLENNIUM FALCON -'\n",
      " 'INT. MILLENNIUM FALCON - COCKPIT' 'EXT. DEATH STAR'\n",
      " 'INT. DEATH STAR - CONTROL ROOM' 'TARKIN ' 'OFFICER CASS'\n",
      " 'INT. MILLENNIUM FALCON - DEATH STAR' 'VOICE OVER DEATH STAR INTERCOM'\n",
      " 'OFFICER' 'INT. DEATH STAR - HALLWAY' 'VOICE' \"HAN'S\" 'INT. DEATH STAR -'\n",
      " 'GANTRY OFFICER' 'GANTRY OFFICER ' 'AREA' 'INTERCOM VOICE'\n",
      " 'TROOPER VOICE' 'FIRST OFFICER' 'FIRST TROOPER ' 'SECOND TROOPER '\n",
      " \"BEN'S VOICE\" 'EXT. MILLENNIUM FALCON' 'SPACE'\n",
      " 'INT. TIE FIGHTER - COCKPIT' 'EXT. MILLENNIUM FALCON - IN SPACE'\n",
      " 'INT. MILLENNIUM FALCON - HOLD AREA' 'INT. MILLENNIUM FALCON - GUNPORTS'\n",
      " 'EXT. SPACE - MILLENNIUM FALCON/TIE FIGHTERS'\n",
      " 'INT. MILLENNIUM FALCON - CHEWBACCA' 'EXT. TIE FIGHTER - SPACE'\n",
      " 'EXT. SPACE - TIE FIGHTERS' 'INT. MILLENNIUM FALCON - MAIN PASSAGEWAY'\n",
      " 'EXT. TIE FIGHTER' 'EXT. SPACE - TIE FIGHTER'\n",
      " 'EXT. SPACE - MILLENNIUM FALCON' 'EXT. MASSASSI OUTPOST'\n",
      " 'EXT. MASSASSI OUTPOST - JUNGLE TEMPLE' 'WILLARD' 'WILLARD '\n",
      " 'DEATH STAR INTERCOM VOICE' 'EXT. YAVIN - JUNGLE'\n",
      " 'INT. MASSASSI - WAR ROOM' 'DODONNA' 'GOLD LEADER' 'DODONNA ' 'WEDGE'\n",
      " \"MAN'S VOICE\" 'RED LEADER' 'CHIEF' 'EXT. MASSASSI OUTPOST - JUNGLE'\n",
      " 'INT. MASSASSI OUTPOST - WAR ROOM' 'MASSASSI INTERCOM VOICE'\n",
      " 'EXT. SPACE - ANOTHER ANGLE' 'INT. RED LEADER STARSHIP - COCKPIT'\n",
      " 'INT. ANOTHER COCKPIT' 'RED TEN' \"INT. BIGGS' COCKPIT\"\n",
      " \"INT. PORKINS' COCKPIT\" \"INT. WEDGE'S FIGHTER - COCKPIT\"\n",
      " \"INT. LUKE'S X-WING FIGHTER - COCKPIT\" \"EXT. LUKE'S X-WING FIGHTER\"\n",
      " \"INT. RED LEADER'S FIGHTER - COCKPIT\" \"INT. RED LEADER'S COCKPIT\"\n",
      " \"INT. WEDGE'S COCKPIT\" \"INT. GOLD LEADER'S COCKPIT\" 'INT. DEATH STAR'\n",
      " 'EXT. SPACE AROUND THE DEATH STAR' 'EXT. SURFACE OF DEATH STAR'\n",
      " 'EXT. SURFACE OF THE DEATH STAR' 'ASTRO-OFFICER'\n",
      " \"EXT. LUKE'S X-WING TRAVELING\" \"INT. BIGGS' COCKPIT - TRAVELING\"\n",
      " 'CONTROL OFFICER' 'EXT. SURFACE AROUND THE DEATH STAR'\n",
      " \"INT. TIE FIGHTER'S COCKPIT\" \"LUKE'S VOICE\"\n",
      " \"INT. LUKE'S X-WING - COCKPIT\" 'EXT. SPACE AROUND THE'\n",
      " 'INT. MASSASSI OUTPOST - WAR' 'EXT. SPACE AROUND THE DEATH'\n",
      " \"INT. GOLD LEADER'S Y-WING - COCKPIT\" 'EXT. SURFACE OF THE DEATH'\n",
      " \"INT. GOLD LEADER'S Y-WING\" 'GOLD' \"INT. GOLD TWO'S Y-WING\" 'GOLD TWO'\n",
      " 'GOLD TWO ' 'EXT. GOLD' 'GOLD FIVE' 'EXT. DEATH STAR TRENCH'\n",
      " \"INT. GOLD FIVE'S Y-WING -\" \"INT. GOLD FIVE'S Y-WING - COCKPIT\"\n",
      " \"INT. RED TEN'S COCKPIT\" 'EXT. SPACE - DEATH STAR TRENCH'\n",
      " \"RED TEN'S VOICE\" 'RED LEADER ' \"INT. RED TEN'S COCKPIT.\"\n",
      " \"RED NINE'S VOICE\" 'EXT. SPACE AROUND' \"INT. LUKE'S COCKPIT\"\n",
      " 'EXT. DEATH STAR - GUN EMPLACEMENTS' 'EXT. DEATH STAR SURFACE'\n",
      " \"INT. LUKE'S X-WING FIGHTER\" \"INT. LUKE'S X-WING COCKPIT\"\n",
      " 'INT. DEATH STAR - CONTROL' 'DEATH' \"INT. LUKE'S X-WING -\" 'BASE VOICE'\n",
      " \"INT. DARTH VADER'S WINGMAN - COCKPIT\" 'WINGMAN'\n",
      " 'INT. MASSASSI OUTPOST - MAIN HANGAR' 'TECHNICIAN'\n",
      " 'EXT. PLAIN OF HOTH - DAY' 'RIEEKAN' 'DECK OFFICER' 'ASSISTANT OFFICER'\n",
      " 'LIEUTENANT' 'DERLIN' 'DERLIN ' 'EXT. HOTH - SNOWDRIFT - DAWN'\n",
      " 'INT. SNOWSPEEDER COCKPIT' 'EXT. HOTH - SNOWDRIFT'\n",
      " 'INT. SNOWSPEEDER - COCKPIT' 'ZEV' 'ANNOUNCER' 'RIEEKAN '\n",
      " 'SENIOR CONTROLLER' 'PIETT' 'OZZEL' 'REBEL CAPTAIN' 'REBEL FIGHTER'\n",
      " 'INT. HOTH - REBEL BASE' 'INT. REBEL BASE -' 'MEDICAL' 'MEDICAL DROID'\n",
      " 'CONTROLLER' 'VEERS' 'HOBBIE' 'INT. REBEL BASE - COMMAND CENTER'\n",
      " 'EXT. SPACE - IMPERIAL STAR DESTROYER' 'INT. IMPERIAL STAR DESTROYER -'\n",
      " 'INT. REBEL BASE - COMMAND' 'EXT. SPACE - HOTH - REBEL' 'DACK'\n",
      " 'TRENCH OFFICER' \"INT. LUKE'S SNOWSPEEDER, ROGUE\"\n",
      " \"INT.  LUKE'S SNOWSPEEDER,\" \"INT.  LUKE'S\" 'EXT. HOTH - ICE PLAIN'\n",
      " 'INT. IMPERIAL SNOW WALKER - COCKPIT' 'INT.' 'EXT. HOTH - BATTLEFIELD'\n",
      " 'JANSON' 'JANSON ' \"EXT. WEDGE'S SNOWSPEEDER, ROGUE THREE\"\n",
      " 'HEAD CONTROLLER' 'EXT. HOTH - SNOW TRENCH' 'EXT. HOTH - BATTLEFIELD -'\n",
      " 'PILOT' 'INT. HOTH - REBEL' 'EXT. SPACE - MILLENNIUM FALCON -'\n",
      " 'EXT. SPACE - STAR' 'EXT. MILLENNIUM FALCON -'\n",
      " 'INT. GIANT ASTEROID CRATER' \"EXT. SPACE -  LUKE'S X-WING\"\n",
      " 'STRANGE VOICE' 'CREATURE ' 'EMPEROR' \"INT. CREATURE'S HOUSE\" 'YODA'\n",
      " 'EXT. MILLENNIUM FALCON - GIANT ASTEROID' 'EXT. DAGOBAH - DAY' 'YODA '\n",
      " 'FIRST CONTROLLER' 'SECOND CONTROLLER' 'BOBA FETT' 'NEEDA' 'NEEDA '\n",
      " 'TRACKING OFFICER' 'COMMUNICATIONS OFFICER' 'LANDO' 'SECOND THREEPIO'\n",
      " 'EXT. SPACE - PLANET DAGOBAH' 'LANDO ' 'IMPERIAL SOLDIER'\n",
      " 'INT. CLOUD CITY -' 'EXT. CLOUD CITY -'\n",
      " 'EXT. CLOUD CITY - LANDING PLATFORM' 'EXT. BOTTOM OF CLOUD CITY'\n",
      " 'INT. MILLENNIUM' 'INT. MILLENNIUM FALCON - HOLD'\n",
      " 'EXT. SPACE - REBEL CRUISER' 'INT. STAR CRUISER - MEDICAL CENTER'\n",
      " 'SHUTTLE CAPTAIN' 'DEATH STAR CONTROLLER ' 'OPERATOR' 'JERJERROD'\n",
      " 'JERJERROD\\t' 'THREEPIO\\t' 'BIB' 'LUKE\\t' 'JABBA\\t' 'NINEDENINE'\n",
      " 'NINEDENINE\\t' 'OOLA' 'BOUSHH\\t' 'BOUSHH' 'BOUSHH ' 'BIB ' 'BIB\\t'\n",
      " 'JABBA ' 'HAN\\t' 'INT SAIL BARGE' 'EXT SKIFF'\n",
      " 'EXT UPPER DECK - SAIL BARGE' 'LEIA\\t' 'EMPEROR\\t' 'YODA\\t' 'BEN\\t'\n",
      " 'HEADQUARTERS FRIGATE.' 'LANDO\\t' 'MON MOTHMA' 'ACKBAR' 'GENERAL MADINE'\n",
      " 'VOICE\\t' 'PIETT\\t' 'PILOT VOICE\\t' 'PIETT ' 'PIETT  '\n",
      " 'INT STOLEN IMPERIAL SHUTTLE - COCKPIT' 'CONTROLLER   '\n",
      " 'EXT FOREST LANDING SITE - ENDOR' 'SCOUT #1' 'SCOUT #2' 'GUARD'\n",
      " 'HAN and LUKE Leia!' 'COMMANDER ' 'LURE' 'ACKBAR\\t'\n",
      " 'EXT BUNKER - ENTRANCE' 'EXT RIDGE' 'EXT BUNKER' 'SCOUT' 'EMPEROR '\n",
      " 'GRAY LEADER' 'GREEN LEADER' 'EXT SPACE - DEATH STAR SHIELD'\n",
      " 'INT REBEL STAR CRUISER - BRIDGE' 'REBEL PILOT' 'STORMTROOPER'\n",
      " 'BUNKER COMMANDER' 'RED TWO' 'RED THREE' 'NAVIGATOR'\n",
      " 'INT DEATH STAR - CONTROL ROOM' 'INT DEATH STAR - BLAST CHAMBER'\n",
      " 'EXT DEATH STAR' 'EXT SPACE - AIR BATTLE'\n",
      " 'INT MILLENNIUM FALCON - COCKPIT' 'WALKER PILOT #1' 'PILOT #2'\n",
      " 'STORMTROOPER\\t' 'HAN/PILOT\\t' 'CONTROL ROOM COMMANDER'\n",
      " 'SECOND COMMANDER' 'ANAKIN\\t' 'ANAKIN']\n"
     ]
    }
   ],
   "source": [
    "if not isinstance(df, type(None)):\n",
    "    print(\"Unique character names in dataset after name processing:\", df['character'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining Vader sentences: 160\n"
     ]
    }
   ],
   "source": [
    "if not isinstance(df, type(None)):\n",
    "    print(\"Remaining\", character, \"sentences:\", len(df[df['character'] == character]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dataset at D:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBot\\BarneyBot\\Data\\Sources\\SW\\SW.csv\n"
     ]
    }
   ],
   "source": [
    "if not isinstance(df, type(None)):\n",
    "    source_path = os.path.join(base_folder, \"Data\", \"Sources\", character_dict[character]['source'])\n",
    "    if not os.path.exists(source_path):\n",
    "        os.makedirs(source_path)\n",
    "    df.to_csv(os.path.join(source_path, str(character_dict[character]['source'])+\".csv\"), index=False)\n",
    "    print(\"Saved dataset at\", os.path.join(source_path, str(character_dict[character]['source'])+\".csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: May consider feeding one sentence and one Sheldon reply or multiple sentences encoded with one Sheldon reply\n",
    "def get_character(df, level=2):\n",
    "    if character == 'Default':\n",
    "        return None\n",
    "    dataframe_rows = []\n",
    "    idxs_character = df[df['character'] == character].index\n",
    "    dataframe_rows = []\n",
    "    for i in idxs_character:\n",
    "        l = []\n",
    "        l.append(df['line'][i])\n",
    "        for j in range(0,level):\n",
    "            line = max(i-j-1,0)\n",
    "            l.append(df['line'][line])\n",
    "        dataframe_rows.append(l)\n",
    "    df = pd.DataFrame(dataframe_rows, columns=['response', 'context', 'context/0'])\n",
    "    return df\n",
    "\n",
    "df = get_character(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            response  \\\n",
      "0      Where are those transmissions youintercepted?   \n",
      "1                What have you done with thoseplans?   \n",
      "2  If this is a consular ship... were is the Amba...   \n",
      "3  Commander, tear this ship apartuntil you've fo...   \n",
      "4  Don't play games with me, Your Highness. You w...   \n",
      "\n",
      "                                             context  \\\n",
      "0  The Death Star plans are not in the main compu...   \n",
      "1      Where are those transmissions youintercepted?   \n",
      "2  We intercepted no transmissions.Aaah... This i...   \n",
      "3  If this is a consular ship... were is the Amba...   \n",
      "4  Lord Vader, I should have known.Only you could...   \n",
      "\n",
      "                                           context/0  \n",
      "0                Wait a minute, where are you going?  \n",
      "1  The Death Star plans are not in the main compu...  \n",
      "2                What have you done with thoseplans?  \n",
      "3  We intercepted no transmissions.Aaah... This i...  \n",
      "4  I keep telling you, the Rebellion is a long wa...  \n"
     ]
    }
   ],
   "source": [
    "if not isinstance(df, type(None)):\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset length: 160\n"
     ]
    }
   ],
   "source": [
    "if not isinstance(df, type(None)):\n",
    "    print(\"Preprocessed dataset length:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dataset at D:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBot\\BarneyBot\\Data\\Characters\\Vader\\Vader.csv\n"
     ]
    }
   ],
   "source": [
    "if not isinstance(df, type(None)):\n",
    "    char_path = os.path.join(base_folder, \"Data\", \"Characters\", character)\n",
    "    if not os.path.exists(char_path):\n",
    "        os.makedirs(char_path)\n",
    "    df.to_csv(os.path.join(char_path, str(character)+\".csv\"), index=False)\n",
    "    print(\"Saved dataset at\", os.path.join(char_path, str(character)+\".csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

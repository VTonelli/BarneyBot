{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "126fc156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import for general utilities\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2a2a322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import character dictionaries, useful to map a character to its data, and a fixed random seed\n",
    "from Data.data_dicts import character_dict, source_dict, random_state\n",
    "\n",
    "character = 'Vader' # 'Barney' | 'Sheldon' | 'Harry' | 'Fry' | 'Vader' | 'Joey' | 'Phoebe' | 'Bender' | 'Default'\n",
    "# Sets the levels of context e.g. level=5 => have a sequance of context [context/0, ..., context/4]\n",
    "level = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7baf800b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the character selected is different from `Default` we extract the source where to find the data\n",
    "if character != 'Default':\n",
    "    source = character_dict[character]['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d455d370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount google drive (for Colab only)\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive',force_remount=True)\n",
    "    base_folder = '/content/drive/My Drive/unibo/NLP_project/BarneyBot'\n",
    "else:\n",
    "    base_folder = os.getcwd()\n",
    "    \n",
    "# define the path for `in_folder`, the one where will be stored data and model of the\n",
    "# selected character\n",
    "in_folder = os.path.join(base_folder, \"Data\", 'Characters', character)\n",
    "if not os.path.exists(in_folder):\n",
    "    os.makedirs(in_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ef6e5f",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a538393",
   "metadata": {},
   "source": [
    "In this notebook, functions and procedures are set up that make it possible to preprocess the various corpus. These will then be used later to fine tune all chatbots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a64feb1",
   "metadata": {},
   "source": [
    "First of all let's start from laoding the dataset. This process will be performed by `load_dataset` which performs the loading of the dataset as DataFrame from each of the tv show we selected for our task:\n",
    "* [How I Met Your Mother](https://transcripts.foreverdreaming.org/viewforum.php?f=177)\n",
    "* [Futurama](https://theinfosphere.org/Episode_Transcript_Listing)\n",
    "* [Harry Potter](https://www.kaggle.com/gulsahdemiryurek/harry-potter-dataset)\n",
    "* [Star Wars](https://bulletproofscreenwriting.tv/star-wars-movies-screenplay-download/)\n",
    "* [Friends](https://www.kaggle.com/datasets/blessondensil294/friends-tv-series-screenplay-script)\n",
    "* [The Big Bang Theory](https://bigbangtrans.wordpress.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64eb726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the dataset documents and store their data into a DataFrame\n",
    "def load_dataset():\n",
    "    ### Loading functions from other files\n",
    "    # Load the dataset from How I Met Your Mother or\n",
    "    #                       The Big Bang Theory   or\n",
    "    #                       Friends\n",
    "    def _load_himym_friends_tbbt_dataset(sources_folder):\n",
    "        dataframe_rows = []\n",
    "        # Get number of documents and their names\n",
    "        documents_n = len(os.listdir(sources_folder))\n",
    "        documents_names = os.listdir(sources_folder)\n",
    "        # Loop over documents\n",
    "        for i in tqdm(range(documents_n)):\n",
    "            # Extract filename which correspond to the link of the episode\n",
    "            filename = documents_names[i]\n",
    "            # the last 5 chars takes the form `sxe` with s the number of the current serie and\n",
    "            # and e as the number of the episode\n",
    "            sources_label = filename[:-4]\n",
    "            # Open document\n",
    "            with open(os.path.join(sources_folder, filename), encoding=\"utf8\") as file:\n",
    "                # Loop over lines (= words)\n",
    "                for line in file.readlines():\n",
    "                        dataframe_row = {\n",
    "                            \"source\": sources_label,\n",
    "                            \"line\": line,\n",
    "                        }\n",
    "                        dataframe_rows.append(dataframe_row)\n",
    "        # Build the dataframe from the words\n",
    "        df = pd.DataFrame(dataframe_rows)\n",
    "        return df\n",
    "    \n",
    "    # Load the dataset from Futurama\n",
    "    def _load_futurama_dataset(sources_folder):\n",
    "        futurama_txt = ''\n",
    "        # Loop over documents\n",
    "        for filename in tqdm(os.listdir(sources_folder)):\n",
    "            futurama_txt += open(os.path.join(sources_folder, filename)).read()\n",
    "        # Split lines\n",
    "        start_idx = 0\n",
    "        end_idx = 0\n",
    "        lines = []\n",
    "        while start_idx < len(futurama_txt):\n",
    "            # eventually bold tag are present, discard them\n",
    "            start_idx = futurama_txt.find('<b>', end_idx)\n",
    "            if start_idx == -1: # if no '<b>' is found, just save the rest\n",
    "                lines.append(futurama_txt[end_idx:].replace('</b>',''))\n",
    "                break\n",
    "            elif start_idx != end_idx: # '<b>' is found\n",
    "                lines.append(futurama_txt[end_idx+4:start_idx])\n",
    "            end_idx = futurama_txt.find('</b>', start_idx)\n",
    "            if end_idx == -1: # if no '</b>' is found, just save the rest\n",
    "                lines.append(futurama_txt[start_idx:].replace('<b>',''))\n",
    "                break\n",
    "            lines.append(futurama_txt[start_idx+3:end_idx])\n",
    "        df = pd.DataFrame(lines, columns=['line'])\n",
    "        return df\n",
    "    \n",
    "    # Load the dataset from Harry Potter\n",
    "    def _load_hp_dataset(sources_folder):\n",
    "        sep = ';'\n",
    "        df = None\n",
    "        df_files = []\n",
    "        # for each movie append the dataset which refers to it\n",
    "        for filename in os.listdir(sources_folder):\n",
    "            df_files.append(pd.read_csv(os.path.join(sources_folder, filename), sep=sep).rename(columns = lambda x: x.lower()))\n",
    "        df = pd.concat(df_files)\n",
    "        df = df.rename(columns = {'character':'character', 'sentence':'line'})\n",
    "        return df\n",
    "    \n",
    "    # Load the dataset from Star Wars\n",
    "    def _load_sw_dataset(source_folder):\n",
    "        dataframe_rows = []\n",
    "        # Get number of documents and their names\n",
    "        documents_n = len(os.listdir(source_folder))\n",
    "        documents_names = os.listdir(source_folder)\n",
    "        # Loop over documents\n",
    "        for i in tqdm(range(documents_n)):\n",
    "            filename = documents_names[i]\n",
    "            film_name = filename[:-4]\n",
    "            # Open document\n",
    "            with open(os.path.join(source_folder, filename)) as file:\n",
    "                film_rows = []\n",
    "                sentence = \"\"\n",
    "                empty_line_allow = False\n",
    "                between_numbers = False\n",
    "                found_character = False\n",
    "                for line in file.readlines():\n",
    "                    if re.search(r\"^[0-9]+.\", line) != None: # Line is number followed by dot (page number)\n",
    "                        pass\n",
    "                    elif re.search(r\"^[A-Z]{2,}\", line) != None: # Line begins with an-all caps (a character)\n",
    "                        sentence += line\n",
    "                        found_character = True\n",
    "                        empty_line_allow = True\n",
    "                    elif line.isspace():\n",
    "                        if empty_line_allow:\n",
    "                            pass\n",
    "                        else:\n",
    "                            if found_character:\n",
    "                                film_row = {\n",
    "                                    \"film\": film_name,\n",
    "                                    \"line\": sentence,\n",
    "                                }\n",
    "                                film_rows.append(film_row)\n",
    "                                sentence = \"\"\n",
    "                                found_character = False\n",
    "                    elif found_character:\n",
    "                        sentence += line\n",
    "                        empty_line_allow = False\n",
    "                dataframe_rows.extend(film_rows)\n",
    "        # Build the dataframe from the words\n",
    "        df = pd.DataFrame(dataframe_rows)\n",
    "        return df\n",
    "    \n",
    "    ### Function starts here\n",
    "    # if character selected is 'Default' so we don't need any dataset\n",
    "    if character == 'Default':\n",
    "        # no dataset is loaded\n",
    "        return None\n",
    "    # otherwise let's take from the source dictionary the folder which contains the datasets\n",
    "    # sources_subfolder is a parameter which contains the path where all data are stored, it can\n",
    "    #   be different from null if data are stored in a different subfolder\n",
    "    sources_subfolder = source_dict[source]['dataset_folder']\n",
    "    if sources_subfolder:\n",
    "        sources_folder = os.path.join(base_folder, \"Data\", \"Sources\", source, sources_subfolder)\n",
    "    else:\n",
    "        sources_folder = os.path.join(base_folder, \"Data\", \"Sources\", source)\n",
    "    # each tv shows loads data by a call to its respective function\n",
    "    if source == 'HIMYM' or source == 'Friends' or source == 'TBBT':\n",
    "        df = _load_himym_friends_tbbt_dataset(sources_folder)\n",
    "    elif source == 'Futurama':\n",
    "        df = _load_futurama_dataset(sources_folder)\n",
    "    elif source == 'HP':\n",
    "        df = _load_hp_dataset(sources_folder)\n",
    "    elif source == 'SW':\n",
    "        df = _load_sw_dataset(sources_folder)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3633e07b",
   "metadata": {},
   "source": [
    "Let's call the function to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f28c617",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 46.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Dataset!\n",
      "\n",
      "                        film  \\\n",
      "0  Star Wars IV - A New Hope   \n",
      "1  Star Wars IV - A New Hope   \n",
      "2  Star Wars IV - A New Hope   \n",
      "3  Star Wars IV - A New Hope   \n",
      "4  Star Wars IV - A New Hope   \n",
      "\n",
      "                                                line  \n",
      "0  THREEPIO\\nDid you hear that? They've shut\\ndow...  \n",
      "1               THREEPIO (CONTâ€™D)\\nWe're doomed!\\n  \n",
      "2  THREEPIO (CONTâ€™D)\\nThere'll be no escape for...  \n",
      "3                THREEPIO (CONTâ€™D)\\nWhat's that?\\n  \n",
      "4  THREEPIO\\nI should have known better than to t...  \n",
      "film    2927\n",
      "line    2927\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Execute creation of dataset\n",
    "df = load_dataset()\n",
    "if not isinstance(df, type(None)):\n",
    "    print(\"Loaded Dataset!\")\n",
    "    print()\n",
    "    print(df.head())\n",
    "    print(df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd2313c",
   "metadata": {},
   "source": [
    "Next we see the definition of the functions that preprocess the datasets. \n",
    "\n",
    "Generally, all the script files share the same structure for all the tv show we selected. Most relevant observation are the following:\n",
    "1. most scripts identify the incipit of an episode with square or round brackets $\\Rightarrow$ discard such lines,\n",
    "2. most scripts put inside round brackets, during the character line, some informations and details regarding some behaviors that character should have in that moment, $\\Rightarrow$ substitute all what there is between brackets with a blank char,\n",
    "3. most scripts identify a character's line with the character's name followed by a colon, $\\Rightarrow$ such lines should be divided into two part (i.e. one for character name and one for his line),\n",
    "4. some documents contains blank rows $\\Rightarrow$ they must be discarded \n",
    "5. some character lines contain blank text $\\Rightarrow$ they must be discarded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e64d819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ciao) mondo\n"
     ]
    }
   ],
   "source": [
    "txt = \"(ciao) mondo\"\n",
    "txt1 = txt.replace(r\"\\(.*\\)\",\"\")\n",
    "print(txt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54599f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(df):\n",
    "    def _process_himym_dataset(df):\n",
    "        # Removes lines which starts with brackets\n",
    "        df = df[~df['line'].str.startswith(\"[\")]\n",
    "        df = df[~df['line'].str.startswith(\"(\")]\n",
    "        # Removes white space\n",
    "        df['line'] = df['line'].str.strip()\n",
    "        # Removes everything is inside the round brackets\n",
    "        df['line'] = df['line'].str.replace(r\"\\(.*\\)\",\"\")\n",
    "        # Removes bracket char, newline, tabular char and special chars replacing them with a space\n",
    "        df['line'] = df['line'].str.replace(r\"[\\/(){}\\[\\]\\|@_#]|\\\\t|\\\\n\",\" \")\n",
    "        # Removes every char which is not present in the following \"white list\"\n",
    "        df['line'] = df['line'].str.replace(r\"[^.\\',;:?!0-9a-zA-Z \\-]\",\"\")\n",
    "        df = df[~df['line'].isnull()]\n",
    "        df[['character', 'line']] = df['line'].str.split(\":\", 1, expand=True)\n",
    "        # Removes empty lines\n",
    "        df = df.dropna()\n",
    "        # Removes white space\n",
    "        df['line'] = df['line'].str.strip()\n",
    "        df['line'] = df['line'][df['line'].str.len() >= 2]\n",
    "        # Removes empty lines\n",
    "        df = df[~df['line'].isnull()]\n",
    "        df = df.replace(r'^s*$', float('NaN'), regex = True)\n",
    "        df = df.dropna()\n",
    "        df = df.reset_index(drop=True)\n",
    "        return df\n",
    "    def _process_tbbt_dataset(df):\n",
    "        # Removes lines which starts with brackets\n",
    "        df = df[~df['line'].str.startswith(\"[\")]\n",
    "        df = df[~df['line'].str.startswith(\"(\")]\n",
    "        df = df[~df['line'].str.startswith(\"Scene: \")]\n",
    "        # Removes white space\n",
    "        df['line'] = df['line'].str.strip()\n",
    "        # Removes everything is inside the round brackets\n",
    "        df['line'] = df['line'].str.replace(r\"\\(.*\\)\",\"\")\n",
    "        # Removes bracket char, newline, tabular char and special chars replacing them with a space\n",
    "        df['line'] = df['line'].str.replace(r\"[\\/(){}\\[\\]\\|@_#]|\\\\t|\\\\n\",\" \")\n",
    "        # Removes every char which is not present in the following \"white list\"\n",
    "        df['line'] = df['line'].str.replace(r\"[^.\\',;:?!0-9a-zA-Z \\-]\",\"\")\n",
    "        # Removes empty lines\n",
    "        df = df[~df['line'].isnull()]\n",
    "        df[['character', 'line']] = df['line'].str.split(\":\", 1, expand=True)\n",
    "        df = df.dropna()\n",
    "        # Removes white space\n",
    "        df['line'] = df['line'].str.strip()\n",
    "        df['line'] = df['line'][df['line'].str.len() >= 2]\n",
    "        # Removes empty lines\n",
    "        df = df[~df['line'].isnull()]\n",
    "        df = df.replace(r'^s*$', float('NaN'), regex = True)\n",
    "        # Removes empty lines\n",
    "        df = df.dropna()\n",
    "        df = df.reset_index(drop=True)\n",
    "        return df\n",
    "    def _process_futurama_dataset(df):\n",
    "        # Remove white space\n",
    "        df['line'] = df['line'].str.strip()\n",
    "        # Removes everything is inside the round and square brackets\n",
    "        df['line'] = df['line'].str.replace(r\"\\[.*\\]\",\"\")\n",
    "        df['line'] = df['line'].str.replace(r\"\\(.*\\)\",\"\")\n",
    "        # Removes everything is inside the tags\n",
    "        df['line'] = df['line'].str.replace(r\"\\<.*\\>\",\"\")\n",
    "        df['line'] = df['line'].str.replace(r\"\\s+\",\" \")\n",
    "        df['line'] = df['line'].str.replace(\"\\n\",\"\")\n",
    "        # Removes lines which starts with brackets\n",
    "        df = df[~df['line'].str.startswith(\"(\")]\n",
    "        df = df[~df['line'].str.startswith(\"[\")]\n",
    "        # Removes bracket char, newline, tabular char and special chars replacing them with a space\n",
    "        df['line'] = df['line'].str.replace(r\"[\\/(){}\\[\\]\\|@_#]|\\\\t|\\\\n\",\" \")\n",
    "        # Removes every char which is not present in the following \"white list\"\n",
    "        df['line'] = df['line'].str.replace(r\"[^.\\',;:?!0-9a-zA-Z \\-]\",\"\")\n",
    "        df['line'] = df['line'][df['line'].str.len() >= 2]\n",
    "        # Removes empty lines\n",
    "        df = df.dropna()\n",
    "        df = df.reset_index(drop=True)\n",
    "        df_rows = []\n",
    "        for row in tqdm(range(len(df)-1)):\n",
    "            if df['line'][row].isupper():\n",
    "                df_row = {\n",
    "                    'line': df['line'][row+1].strip()[:512],\n",
    "                    'character': df['line'][row].strip().capitalize()\n",
    "                }\n",
    "                df_rows.append(df_row)\n",
    "        df = pd.DataFrame(df_rows)\n",
    "        # Discard titles\n",
    "        df = df[df['character'].str.contains('Futurama')==False]\n",
    "        df = df.replace(r'^s*$', float('NaN'), regex = True)\n",
    "        # Removes empty lines\n",
    "        df = df.dropna()\n",
    "        df = df.reset_index(drop=True)\n",
    "        return df\n",
    "    def _process_friends_dataset(df):\n",
    "        # Removes lines which starts with brackets\n",
    "        df = df[~df['line'].str.startswith(\"[\")]\n",
    "        df = df[~df['line'].str.startswith(\"(\")]\n",
    "        # Removes white space\n",
    "        df['line'] = df['line'].str.strip()\n",
    "        # Removes everything is inside the round brackets\n",
    "        df['line'] = df['line'].str.replace(r\"\\(.*\\)\",\"\")\n",
    "        # Removes bracket char, newline, tabular char and special chars replacing them with a space\n",
    "        df['line'] = df['line'].str.replace(r\"[\\/(){}\\[\\]\\|@_#]|\\\\t|\\\\n\",\" \")\n",
    "        # Removes every char which is not present in the following \"white list\"\n",
    "        df['line'] = df['line'].str.replace(r\"[^.\\',;:?!0-9a-zA-Z \\-]\",\"\")\n",
    "        df = df[~df['line'].isnull()]\n",
    "        df[['character', 'line']] = df['line'].str.split(\":\", 1, expand=True)\n",
    "        # Removes empty lines\n",
    "        df = df.dropna()\n",
    "        # Removes white space\n",
    "        df['line'] = df['line'].str.strip()\n",
    "        df['line'] = df['line'][df['line'].str.len() >= 2]\n",
    "        # Removes empty lines\n",
    "        df = df[~df['line'].isnull()]\n",
    "        df = df[~(df['character'] == 'Written by')]\n",
    "        df = df.replace(r'^s*$', float('NaN'), regex = True)\n",
    "        # Removes empty lines\n",
    "        df = df.dropna()\n",
    "        df = df.reset_index(drop=True)\n",
    "        return df\n",
    "    def _process_sw_dataset(df):\n",
    "        # Removes lines which starts with brackets\n",
    "        df = df[~df['line'].str.startswith(\"[\")]\n",
    "        df = df[~df['line'].str.startswith(\"(\")]\n",
    "        # Removes white space\n",
    "        df['line'] = df['line'].str.strip()\n",
    "        # Removes everything is inside the round brackets\n",
    "        df['line'] = df['line'].str.replace(r\"\\(.*\\)\",\"\")\n",
    "        df[['character', 'line']] = df['line'].str.split(\"\\n\", 1, expand=True)\n",
    "        # Removes bracket char, newline, tabular char and special chars replacing them with a space\n",
    "        df['line'] = df['line'].str.replace(r\"[\\/(){}\\[\\]\\|@_#]|\\\\t|\\\\n\",\" \")\n",
    "        # Removes every char which is not present in the following \"white list\"\n",
    "        df['line'] = df['line'].str.replace(r\"[^.\\',;:?!0-9a-zA-Z \\-]\",\"\")\n",
    "        # Removes empty lines\n",
    "        df = df[~df['line'].isnull()]\n",
    "        df = df[df['character'].str.split().apply(lambda l: len(l)) <= 6]\n",
    "        df = df.replace(r'^s*$', float('NaN'), regex = True)\n",
    "        # Removes empty lines\n",
    "        df = df.dropna()\n",
    "        df = df.reset_index(drop=True)\n",
    "        return df\n",
    "    def _process_hp_dataset(df):\n",
    "        # Removes white space\n",
    "        df['line'] = df['line'].str.strip()\n",
    "        # Removes everything is inside the round brackets\n",
    "        df['line'] = df['line'].str.replace(r\"\\(.*\\)\",\"\")\n",
    "        # Removes bracket char, newline, tabular char and special chars replacing them with a space\n",
    "        df['line'] = df['line'].str.replace(r\"[\\/(){}\\[\\]\\|@_#]|\\\\t|\\\\n\",\" \")\n",
    "        # Removes every char which is not present in the following \"white list\"\n",
    "        df['line'] = df['line'].str.replace(r\"[^.\\',;:?!0-9a-zA-Z \\-]\",\"\")\n",
    "        # Remove empty lines\n",
    "        df = df[~df['line'].isnull()]\n",
    "        df = df.dropna()\n",
    "        # Removes white space\n",
    "        df['line'] = df['line'].str.strip()\n",
    "        df['character'] = [line.lower() for line in df['character']]\n",
    "        # Removes empty lines\n",
    "        df = df[~df['line'].isnull()]\n",
    "        df = df.replace(r'^s*$', float('NaN'), regex = True)\n",
    "        # Removes empty lines\n",
    "        df = df.dropna()\n",
    "        df = df.reset_index(drop=True)\n",
    "        return df\n",
    "    # Function starts here\n",
    "    if character == 'Default':\n",
    "        return None\n",
    "    if source == 'HIMYM':\n",
    "        df = _process_himym_dataset(df)\n",
    "    elif source == 'Friends':\n",
    "        df = _process_friends_dataset(df)\n",
    "    elif source == 'Futurama':\n",
    "        df = _process_futurama_dataset(df)\n",
    "    elif source == 'TBBT':\n",
    "        df = _process_tbbt_dataset(df)\n",
    "    elif source == 'HP':\n",
    "        df = _process_hp_dataset(df)\n",
    "    elif source == 'SW':\n",
    "        df = _process_sw_dataset(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4342181",
   "metadata": {},
   "source": [
    "Finally we apply the processing function to the dataset `df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07045fca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Dataset into line-character format!\n",
      "\n",
      "                        film  \\\n",
      "0  Star Wars IV - A New Hope   \n",
      "1  Star Wars IV - A New Hope   \n",
      "2  Star Wars IV - A New Hope   \n",
      "3  Star Wars IV - A New Hope   \n",
      "4  Star Wars IV - A New Hope   \n",
      "\n",
      "                                                line  character  \n",
      "0  Did you hear that? They've shutdown the main r...   THREEPIO  \n",
      "1                                      We're doomed!  THREEPIO   \n",
      "2   There'll be no escape for thePrincess this time.  THREEPIO   \n",
      "3                                       What's that?  THREEPIO   \n",
      "4  I should have known better than to trust the l...   THREEPIO  \n",
      "2750\n"
     ]
    }
   ],
   "source": [
    "df = process_dataset(df)\n",
    "if not isinstance(df, type(None)):\n",
    "    print(\"Processed Dataset into line-character format!\\n\")\n",
    "    print(df.head())\n",
    "    print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eab935",
   "metadata": {},
   "source": [
    "Some errors can be detected after the whole process due to the bad quality of such scripts. In particular it can be noticed that if we provide a search for character name we can notice that there are some character which contains the name of the subject we selected (w.r.t `character`) but wchich instead they refear to other subjects of the show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f2a4b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters contanining Vader : {\"INT. VADER'S STAR DESTROYER - BRIDGE\", \"INT. DARTH VADER'S COCKPIT\", 'VADER ', 'VADER', \"EXT. DARTH VADER'S TIE FIGHTER\", 'VADER\\t', \"INT. DARTH VADER'S WINGMAN - COCKPIT\"} 161\n"
     ]
    }
   ],
   "source": [
    "# if the dataset is not None\n",
    "if not isinstance(df, type(None)):\n",
    "    # extract the list of names which contain the string in `character`\n",
    "    char_names = [c for c in df['character'] if character.lower() in c.lower()]\n",
    "    print(\"Characters contanining\", character, \":\", set(char_names), len(char_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcb9a8d",
   "metadata": {},
   "source": [
    "To further clean up the data, in order to remove the false aliases of the character, we discard the previously extracted list which contains all the names that also contain `character`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b34959e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the dataset is not None\n",
    "if not isinstance(df, type(None)):\n",
    "    # subtract to the set of `char_names` the names to delete (`delete_names`)\n",
    "    char_names = set(char_names) - set(character_dict[character]['delete_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e84257e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the dataset is not None\n",
    "if not isinstance(df, type(None)):\n",
    "    # Replace the in the dataset names, the only names contained in the resulting set after the\n",
    "    # subtruction of the name to delete with `character`\n",
    "    df['character'] = df['character'].apply(lambda x: character if x in char_names else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc78a184",
   "metadata": {},
   "source": [
    "Resulting names character are the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "940bbe24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique character names in dataset after name processing: ['THREEPIO' 'THREEPIO ' 'LUKE' 'IMPERIAL OFFICER' 'Vader' 'REBEL OFFICER'\n",
      " 'TROOPER' 'TROOPER ' 'CHIEF PILOT' 'CAPTAIN' 'WOMAN' 'FIXER' 'CAMIE'\n",
      " 'LUKE ' 'BIGGS' 'DEAK' 'DAY' 'LEIA' 'COMMANDER' 'SECOND OFFICER'\n",
      " 'INT. SANDCRAWLER - HOLD AREA' 'INT. SANDCRAWLER - PRISON AREA'\n",
      " 'EXT. TATOOINE - DESERT - DAY' 'FIRST TROOPER' 'SECOND TROOPER'\n",
      " 'EXT. TATOOINE - DUNES' 'INT. SANDCRAWLER' 'BERU' 'OWEN' 'OWEN '\n",
      " 'AUNT BERU' 'EXT. TATOOINE - LARS HOMESTEAD'\n",
      " 'INT. LARS HOMESTEAD - PLAZA' 'BEN' 'BEN ' 'LEIA ' 'EXT. SPACE.'\n",
      " 'INT. DEATH STAR - CONFERENCE ROOM' 'TAGGE' 'MOTTI' 'TARKIN'\n",
      " 'EXT. TATOOINE - WASTELAND' 'EXT. SPACE'\n",
      " 'INT. DEATH STAR - DETENTION CORRIDOR' 'BARTENDER' 'CREATURE' 'HUMAN'\n",
      " 'HUMAN ' 'HAN' 'HAN ' 'GREEDO' 'SPEEDER LOT' 'JABBA'\n",
      " 'INT. MILLENNIUM FALCON' 'INT. MOS EISLEY SPACEPORT -'\n",
      " 'EXT. SPACE - PLANET TATOOINE' 'INT. MILLENNIUM FALCON -'\n",
      " 'INT. MILLENNIUM FALCON - COCKPIT' 'EXT. DEATH STAR'\n",
      " 'INT. DEATH STAR - CONTROL ROOM' 'TARKIN ' 'OFFICER CASS'\n",
      " 'INT. MILLENNIUM FALCON - DEATH STAR' 'VOICE OVER DEATH STAR INTERCOM'\n",
      " 'OFFICER' 'INT. DEATH STAR - HALLWAY' 'VOICE' \"HAN'S\" 'INT. DEATH STAR -'\n",
      " 'GANTRY OFFICER' 'GANTRY OFFICER ' 'AREA' 'INTERCOM VOICE'\n",
      " 'TROOPER VOICE' 'FIRST OFFICER' 'FIRST TROOPER ' 'SECOND TROOPER '\n",
      " \"BEN'S VOICE\" 'EXT. MILLENNIUM FALCON' 'SPACE'\n",
      " 'INT. TIE FIGHTER - COCKPIT' 'EXT. MILLENNIUM FALCON - IN SPACE'\n",
      " 'INT. MILLENNIUM FALCON - HOLD AREA' 'INT. MILLENNIUM FALCON - GUNPORTS'\n",
      " 'EXT. SPACE - MILLENNIUM FALCON/TIE FIGHTERS'\n",
      " 'INT. MILLENNIUM FALCON - CHEWBACCA' 'EXT. TIE FIGHTER - SPACE'\n",
      " 'EXT. SPACE - TIE FIGHTERS' 'INT. MILLENNIUM FALCON - MAIN PASSAGEWAY'\n",
      " 'EXT. TIE FIGHTER' 'EXT. SPACE - TIE FIGHTER'\n",
      " 'EXT. SPACE - MILLENNIUM FALCON' 'EXT. MASSASSI OUTPOST'\n",
      " 'EXT. MASSASSI OUTPOST - JUNGLE TEMPLE' 'WILLARD' 'WILLARD '\n",
      " 'DEATH STAR INTERCOM VOICE' 'EXT. YAVIN - JUNGLE'\n",
      " 'INT. MASSASSI - WAR ROOM' 'DODONNA' 'GOLD LEADER' 'DODONNA ' 'WEDGE'\n",
      " \"MAN'S VOICE\" 'RED LEADER' 'CHIEF' 'EXT. MASSASSI OUTPOST - JUNGLE'\n",
      " 'INT. MASSASSI OUTPOST - WAR ROOM' 'MASSASSI INTERCOM VOICE'\n",
      " 'EXT. SPACE - ANOTHER ANGLE' 'INT. RED LEADER STARSHIP - COCKPIT'\n",
      " 'INT. ANOTHER COCKPIT' 'RED TEN' \"INT. BIGGS' COCKPIT\"\n",
      " \"INT. PORKINS' COCKPIT\" \"INT. WEDGE'S FIGHTER - COCKPIT\"\n",
      " \"INT. LUKE'S X-WING FIGHTER - COCKPIT\" \"EXT. LUKE'S X-WING FIGHTER\"\n",
      " \"INT. RED LEADER'S FIGHTER - COCKPIT\" \"INT. RED LEADER'S COCKPIT\"\n",
      " \"INT. WEDGE'S COCKPIT\" \"INT. GOLD LEADER'S COCKPIT\" 'INT. DEATH STAR'\n",
      " 'EXT. SPACE AROUND THE DEATH STAR' 'EXT. SURFACE OF DEATH STAR'\n",
      " 'EXT. SURFACE OF THE DEATH STAR' 'ASTRO-OFFICER'\n",
      " \"EXT. LUKE'S X-WING TRAVELING\" \"INT. BIGGS' COCKPIT - TRAVELING\"\n",
      " 'CONTROL OFFICER' 'EXT. SURFACE AROUND THE DEATH STAR'\n",
      " \"INT. TIE FIGHTER'S COCKPIT\" \"LUKE'S VOICE\"\n",
      " \"INT. LUKE'S X-WING - COCKPIT\" 'EXT. SPACE AROUND THE'\n",
      " 'INT. MASSASSI OUTPOST - WAR' 'EXT. SPACE AROUND THE DEATH'\n",
      " \"INT. GOLD LEADER'S Y-WING - COCKPIT\" 'EXT. SURFACE OF THE DEATH'\n",
      " \"INT. GOLD LEADER'S Y-WING\" 'GOLD' \"INT. GOLD TWO'S Y-WING\" 'GOLD TWO'\n",
      " 'GOLD TWO ' 'EXT. GOLD' 'GOLD FIVE' 'EXT. DEATH STAR TRENCH'\n",
      " \"INT. GOLD FIVE'S Y-WING -\" \"INT. GOLD FIVE'S Y-WING - COCKPIT\"\n",
      " \"INT. RED TEN'S COCKPIT\" 'EXT. SPACE - DEATH STAR TRENCH'\n",
      " \"RED TEN'S VOICE\" 'RED LEADER ' \"INT. RED TEN'S COCKPIT.\"\n",
      " \"RED NINE'S VOICE\" 'EXT. SPACE AROUND' \"INT. LUKE'S COCKPIT\"\n",
      " 'EXT. DEATH STAR - GUN EMPLACEMENTS' 'EXT. DEATH STAR SURFACE'\n",
      " \"INT. LUKE'S X-WING FIGHTER\" \"INT. LUKE'S X-WING COCKPIT\"\n",
      " 'INT. DEATH STAR - CONTROL' 'DEATH' \"INT. LUKE'S X-WING -\" 'BASE VOICE'\n",
      " \"INT. DARTH VADER'S WINGMAN - COCKPIT\" 'WINGMAN'\n",
      " 'INT. MASSASSI OUTPOST - MAIN HANGAR' 'TECHNICIAN'\n",
      " 'EXT. PLAIN OF HOTH - DAY' 'RIEEKAN' 'DECK OFFICER' 'ASSISTANT OFFICER'\n",
      " 'LIEUTENANT' 'DERLIN' 'DERLIN ' 'EXT. HOTH - SNOWDRIFT - DAWN'\n",
      " 'INT. SNOWSPEEDER COCKPIT' 'EXT. HOTH - SNOWDRIFT'\n",
      " 'INT. SNOWSPEEDER - COCKPIT' 'ZEV' 'ANNOUNCER' 'RIEEKAN '\n",
      " 'SENIOR CONTROLLER' 'PIETT' 'OZZEL' 'REBEL CAPTAIN' 'REBEL FIGHTER'\n",
      " 'INT. HOTH - REBEL BASE' 'INT. REBEL BASE -' 'MEDICAL' 'MEDICAL DROID'\n",
      " 'CONTROLLER' 'VEERS' 'HOBBIE' 'INT. REBEL BASE - COMMAND CENTER'\n",
      " 'EXT. SPACE - IMPERIAL STAR DESTROYER' 'INT. IMPERIAL STAR DESTROYER -'\n",
      " 'INT. REBEL BASE - COMMAND' 'EXT. SPACE - HOTH - REBEL' 'DACK'\n",
      " 'TRENCH OFFICER' \"INT. LUKE'S SNOWSPEEDER, ROGUE\"\n",
      " \"INT.  LUKE'S SNOWSPEEDER,\" \"INT.  LUKE'S\" 'EXT. HOTH - ICE PLAIN'\n",
      " 'INT. IMPERIAL SNOW WALKER - COCKPIT' 'INT.' 'EXT. HOTH - BATTLEFIELD'\n",
      " 'JANSON' 'JANSON ' \"EXT. WEDGE'S SNOWSPEEDER, ROGUE THREE\"\n",
      " 'HEAD CONTROLLER' 'EXT. HOTH - SNOW TRENCH' 'EXT. HOTH - BATTLEFIELD -'\n",
      " 'PILOT' 'INT. HOTH - REBEL' 'EXT. SPACE - MILLENNIUM FALCON -'\n",
      " 'EXT. SPACE - STAR' 'EXT. MILLENNIUM FALCON -'\n",
      " 'INT. GIANT ASTEROID CRATER' \"EXT. SPACE -  LUKE'S X-WING\"\n",
      " 'STRANGE VOICE' 'CREATURE ' 'EMPEROR' \"INT. CREATURE'S HOUSE\" 'YODA'\n",
      " 'EXT. MILLENNIUM FALCON - GIANT ASTEROID' 'EXT. DAGOBAH - DAY' 'YODA '\n",
      " 'FIRST CONTROLLER' 'SECOND CONTROLLER' 'BOBA FETT' 'NEEDA' 'NEEDA '\n",
      " 'TRACKING OFFICER' 'COMMUNICATIONS OFFICER' 'LANDO' 'SECOND THREEPIO'\n",
      " 'EXT. SPACE - PLANET DAGOBAH' 'LANDO ' 'IMPERIAL SOLDIER'\n",
      " 'INT. CLOUD CITY -' 'EXT. CLOUD CITY -'\n",
      " 'EXT. CLOUD CITY - LANDING PLATFORM' 'EXT. BOTTOM OF CLOUD CITY'\n",
      " 'INT. MILLENNIUM' 'INT. MILLENNIUM FALCON - HOLD'\n",
      " 'EXT. SPACE - REBEL CRUISER' 'INT. STAR CRUISER - MEDICAL CENTER'\n",
      " 'SHUTTLE CAPTAIN' 'DEATH STAR CONTROLLER ' 'OPERATOR' 'JERJERROD'\n",
      " 'JERJERROD\\t' 'THREEPIO\\t' 'BIB' 'LUKE\\t' 'JABBA\\t' 'NINEDENINE'\n",
      " 'NINEDENINE\\t' 'OOLA' 'BOUSHH\\t' 'BOUSHH' 'BOUSHH ' 'BIB ' 'BIB\\t'\n",
      " 'JABBA ' 'HAN\\t' 'INT SAIL BARGE' 'EXT SKIFF'\n",
      " 'EXT UPPER DECK - SAIL BARGE' 'LEIA\\t' 'EMPEROR\\t' 'YODA\\t' 'BEN\\t'\n",
      " 'HEADQUARTERS FRIGATE.' 'LANDO\\t' 'MON MOTHMA' 'ACKBAR' 'GENERAL MADINE'\n",
      " 'VOICE\\t' 'PIETT\\t' 'PILOT VOICE\\t' 'PIETT ' 'PIETT  '\n",
      " 'INT STOLEN IMPERIAL SHUTTLE - COCKPIT' 'CONTROLLER   '\n",
      " 'EXT FOREST LANDING SITE - ENDOR' 'SCOUT #1' 'SCOUT #2' 'GUARD'\n",
      " 'HAN and LUKE Leia!' 'COMMANDER ' 'LURE' 'ACKBAR\\t'\n",
      " 'EXT BUNKER - ENTRANCE' 'EXT RIDGE' 'EXT BUNKER' 'SCOUT' 'EMPEROR '\n",
      " 'GRAY LEADER' 'GREEN LEADER' 'EXT SPACE - DEATH STAR SHIELD'\n",
      " 'INT REBEL STAR CRUISER - BRIDGE' 'REBEL PILOT' 'STORMTROOPER'\n",
      " 'BUNKER COMMANDER' 'RED TWO' 'RED THREE' 'NAVIGATOR'\n",
      " 'INT DEATH STAR - CONTROL ROOM' 'INT DEATH STAR - BLAST CHAMBER'\n",
      " 'EXT DEATH STAR' 'EXT SPACE - AIR BATTLE'\n",
      " 'INT MILLENNIUM FALCON - COCKPIT' 'WALKER PILOT #1' 'PILOT #2'\n",
      " 'STORMTROOPER\\t' 'HAN/PILOT\\t' 'CONTROL ROOM COMMANDER'\n",
      " 'SECOND COMMANDER' 'ANAKIN\\t' 'ANAKIN']\n"
     ]
    }
   ],
   "source": [
    "# if the dataset is not None\n",
    "if not isinstance(df, type(None)):\n",
    "    print(\"Unique character names in dataset after name processing:\", df['character'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9417e3e6",
   "metadata": {},
   "source": [
    "Therefore the amount of final sentences are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9563e134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining Vader sentences: 160\n"
     ]
    }
   ],
   "source": [
    "# If the dataset is not None\n",
    "if not isinstance(df, type(None)):\n",
    "    print(\"Remaining\", character, \"sentences:\", len(df[df['character'] == character]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8e7821",
   "metadata": {},
   "source": [
    "Let's save the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3fa354cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dataset at D:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBot\\BarneyBot\\Data\\Sources\\SW\\SW.csv\n"
     ]
    }
   ],
   "source": [
    "# if the dataset is not None\n",
    "if not isinstance(df, type(None)):\n",
    "    source_path = os.path.join(base_folder, \"Data\", \"Sources\", character_dict[character]['source'])\n",
    "    if not os.path.exists(source_path):\n",
    "        os.makedirs(source_path)\n",
    "    df.to_csv(os.path.join(source_path, str(character_dict[character]['source'])+\".csv\"), index=False)\n",
    "    print(\"Saved dataset at\", os.path.join(source_path, str(character_dict[character]['source'])+\".csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b99be10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: May consider feeding one sentence and one Sheldon reply or multiple sentences encoded with one Sheldon reply\n",
    "def get_character(df, level=2):\n",
    "    if character == 'Default':\n",
    "        return None\n",
    "    dataframe_rows = []\n",
    "    idxs_character = df[df['character'] == character].index\n",
    "    dataframe_rows = []\n",
    "    # Formats the column name\n",
    "    columns = ['response'] + ['context/'+i for i in range(level)]\n",
    "    for i in idxs_character:\n",
    "        l = []\n",
    "        l.append(df['line'][i])\n",
    "        for j in range(0,level):\n",
    "            line = max(i-j-1,0)\n",
    "            l.append(df['line'][line])\n",
    "        dataframe_rows.append(l)\n",
    "    df = pd.DataFrame(dataframe_rows, columns=columns)\n",
    "    return df\n",
    "\n",
    "# Call the function\n",
    "df = get_character(df, level=level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bab2a55",
   "metadata": {},
   "source": [
    "Below you can notice the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7f47676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            response  \\\n",
      "0      Where are those transmissions youintercepted?   \n",
      "1                What have you done with thoseplans?   \n",
      "2  If this is a consular ship... were is the Amba...   \n",
      "3  Commander, tear this ship apartuntil you've fo...   \n",
      "4  Don't play games with me, Your Highness. You w...   \n",
      "\n",
      "                                             context  \\\n",
      "0  The Death Star plans are not in the main compu...   \n",
      "1      Where are those transmissions youintercepted?   \n",
      "2  We intercepted no transmissions.Aaah... This i...   \n",
      "3  If this is a consular ship... were is the Amba...   \n",
      "4  Lord Vader, I should have known.Only you could...   \n",
      "\n",
      "                                           context/0  \n",
      "0                Wait a minute, where are you going?  \n",
      "1  The Death Star plans are not in the main compu...  \n",
      "2                What have you done with thoseplans?  \n",
      "3  We intercepted no transmissions.Aaah... This i...  \n",
      "4  I keep telling you, the Rebellion is a long wa...  \n"
     ]
    }
   ],
   "source": [
    "# If the dataset is not None\n",
    "if not isinstance(df, type(None)):\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "85e08fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset length: 160\n"
     ]
    }
   ],
   "source": [
    "# If the dataset is not None\n",
    "if not isinstance(df, type(None)):\n",
    "    print(\"Preprocessed dataset length:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ef7a7ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dataset at D:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBot\\BarneyBot\\Data\\Characters\\Vader\\Vader.csv\n"
     ]
    }
   ],
   "source": [
    "# If the dataset is not None\n",
    "if not isinstance(df, type(None)):\n",
    "    char_path = os.path.join(base_folder, \"Data\", \"Characters\", character)\n",
    "    if not os.path.exists(char_path):\n",
    "        os.makedirs(char_path)\n",
    "    df.to_csv(os.path.join(char_path, str(character)+\".csv\"), index=False)\n",
    "    print(\"Saved dataset at\", os.path.join(char_path, str(character)+\".csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

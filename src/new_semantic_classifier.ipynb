{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers, callbacks, regularizers\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import random\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from lib.BBData import character_dict, random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "lr = 1e-6\n",
    "patience = 6\n",
    "regularizer_weight_r = 1e-4\n",
    "regularizer_weight_s = 1e-3\n",
    "dropout_rate = 0.2\n",
    "train_size = 0.85\n",
    "test_size = 0.10\n",
    "# Instance state, for caching, in case of repeated usage of this metric\n",
    "sentence_transformer = None\n",
    "character = None\n",
    "embedding_model = None\n",
    "# Embedding params\n",
    "embedding_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_state():\n",
    "    sentence_transformer = None\n",
    "    character = None\n",
    "    embedding_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_model(input_size):\n",
    "    # Input is a concatenated triplet of sentences\n",
    "    inputs = keras.Input(shape=input_size)\n",
    "    # Model is a concatenation of dense layers alternated by batch normalizations\n",
    "    x = layers.Dense(\n",
    "        1024,\n",
    "        activation='relu',\n",
    "    )(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Dense(\n",
    "        1024,\n",
    "        activation='relu',\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(\n",
    "        512,\n",
    "        activation='relu',\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    # The last layers have L2 regularization, to avoid too high values\n",
    "    x = layers.Dense(\n",
    "        256,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=regularizers.l2(regularizer_weight_r),\n",
    "        bias_regularizer=regularizers.l2(regularizer_weight_r))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    # The output is the embedding for the input\n",
    "    out = layers.Dense(\n",
    "        embedding_size,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=regularizers.l2(regularizer_weight_r),\n",
    "        bias_regularizer=regularizers.l2(regularizer_weight_r))(x)\n",
    "    # Create and compile keras model\n",
    "    embedding_model = keras.Model(inputs, out)\n",
    "    return embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create siamese network for training\n",
    "def create_siamese_net(input_size, embedding_model):\n",
    "    input_anchor = layers.Input(shape=(input_size, ))\n",
    "    input_positive = layers.Input(shape=(input_size, ))\n",
    "    input_negative = layers.Input(shape=(input_size, ))\n",
    "\n",
    "    embedding_anchor = embedding_model(input_anchor)\n",
    "    embedding_positive = embedding_model(input_positive)\n",
    "    embedding_negative = embedding_model(input_negative)\n",
    "\n",
    "    output = layers.concatenate(\n",
    "        [embedding_anchor, embedding_positive, embedding_negative], axis=1)\n",
    "\n",
    "    siamese_net = keras.Model(\n",
    "        [input_anchor, input_positive, input_negative], output)\n",
    "\n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a dataset composed of triples from a dataset of single sentences. Used in training only.\n",
    "def get_triplet_df(series_df, n_shuffles, random_state):\n",
    "    # Separate lines by character from all the others\n",
    "    series_df_1 = series_df[series_df['character'] == 1].copy()\n",
    "    # Define triplet dataset as having a character label and the line, already encoded\n",
    "    df_rows = {'character': [], 'encoded_lines': []}\n",
    "    # Shuffle by a parametrized amount\n",
    "    for i in range(n_shuffles):\n",
    "        # print(\"Running shuffle \" + str(i) + \"/\" + str(n_shuffles))\n",
    "        # Shuffle the dataset and balance number of 0s (we suppose its cardinality is higher than that of 1s)\n",
    "        series_df_1 = series_df_1.sample(frac=1,\n",
    "                                            random_state=random_state +\n",
    "                                            i).reset_index(drop=True)\n",
    "        # Iterate over lines\n",
    "        for i in range(2, len(series_df_1)):\n",
    "            # Get a triple of consecutive lines for the character, and concatenate them in one sample\n",
    "            lines = list(series_df_1['encoded_line'][i - 2:i + 1])\n",
    "            lines = np.concatenate(lines)\n",
    "            df_rows['character'].append(1)\n",
    "            df_rows['encoded_lines'].append(lines)\n",
    "    # Create a new dataframe from the rows we have built\n",
    "    df = pd.DataFrame(data=df_rows)\n",
    "    # Sample the dataset one last time to shuffle it\n",
    "    return df.sample(frac=1,\n",
    "                        random_state=random_state).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = list(character_dict.keys())\n",
    "if 'Default' in characters:\n",
    "    characters.remove('Default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_transformer = SentenceTransformer(\n",
    "        \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")\n",
    "for character in characters:\n",
    "    source = character_dict[character]['source']\n",
    "    series_df_path = os.path.join('..', 'Data', 'Sources', source, source+'.csv')\n",
    "    series_df = pd.read_csv(series_df_path)\n",
    "    \n",
    "    # Apply class labelling to the dataset sentences\n",
    "    series_df['character'] = series_df['character'].apply(\n",
    "        lambda x: 1 if x == character else 0)\n",
    "    # Throw away unnecessary dataset rows\n",
    "    series_df = series_df[['character', 'line']]\n",
    "    # Encode lines and add them to the dataset as a new row\n",
    "    series_df['encoded_line'] = [\n",
    "        sentence_transformer.encode(line)\n",
    "        for line in tqdm(series_df['line'])\n",
    "    ]\n",
    "    # Save the dataset rows as a csv file\n",
    "    p = os.path.join('..', 'Data', 'Characters', character)\n",
    "    save_path = os.path.join(p, character.lower()+'_classifier.csv')\n",
    "    series_df[['line', 'character']].to_csv(save_path, index=False)\n",
    "    # The encoded lines are saved separately via numpy due to their type (array)\n",
    "    np.save(os.path.join(os.path.join(p, character.lower() + '_encoded_lines.npy')),\n",
    "            series_df['encoded_line'].to_numpy())\n",
    "    print(\"Saved encoded lines at \" + p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(\n",
    "    source_encoded_path,\n",
    "    random_state=random_state,\n",
    "    n_shuffles=10,\n",
    "    shutdown_at_end=False\n",
    "    ):\n",
    "\n",
    "    # Flush the instance state cache\n",
    "    reset_state()\n",
    "\n",
    "    # shuffled_df = pd.DataFrame.from_dict({'line':[], 'character':[]})\n",
    "    df_list = []\n",
    "    print('Loading encoded lines...')\n",
    "    for c in tqdm(range(len(characters))):\n",
    "        # Load the preprocessed dataset\n",
    "        series_df = pd.read_csv(os.path.join(\n",
    "            source_encoded_path, characters[c],\n",
    "            characters[c].lower() + '_classifier.csv'),\n",
    "                                dtype={\n",
    "                                    'line': str,\n",
    "                                    'character': int\n",
    "                                })\n",
    "\n",
    "        # Load encoded lines dataset, via numpy, and add it as a new row in the dataset\n",
    "        series_df['encoded_line'] = np.load(os.path.join(\n",
    "            source_encoded_path, characters[c],\n",
    "            characters[c].lower() + '_encoded_lines.npy'),\n",
    "                                            allow_pickle=True)\n",
    "        #print(\"Loaded encoded lines from \" + source_encoded_path + '/' + characters[c])\n",
    "        tmp_df = get_triplet_df(series_df, n_shuffles=n_shuffles, random_state=random_state)\n",
    "        tmp_df['character'] = [c for _ in range(len(tmp_df))]\n",
    "\n",
    "        assert len(tmp_df[tmp_df['character']==c]) > 0\n",
    "\n",
    "        # shuffled_df = pd.concat([shuffled_df, tmp_df])\n",
    "        df_list.append(tmp_df)\n",
    "\n",
    "    tot_len = min([len(df) for df in df_list])\n",
    "    # Store into variables the train, val, test, total lengths of the new (triplets) dataset\n",
    "    train_len = int(tot_len * train_size)\n",
    "    test_len = int(tot_len * test_size)\n",
    "    val_len = tot_len - train_len - test_len\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    X_val = []\n",
    "    y_val = []\n",
    "    print('Creating merged data...')\n",
    "    for shuffled_df in tqdm(df_list):\n",
    "        # Load triples into numpy arrays, separating data and labels\n",
    "        # print('Loading training data...')\n",
    "        shuffled_df = shuffled_df.sample(frac=1)\n",
    "        shuffled_df = shuffled_df.iloc[:tot_len]\n",
    "        X_train += [[float(e) for e in s]\n",
    "                for s in shuffled_df['encoded_lines'].iloc[:train_len]]\n",
    "        y_train += [c for c in shuffled_df['character'].iloc[:train_len]]\n",
    "        # print('Loading test data...')\n",
    "        X_test += [[float(e) for e in s]\n",
    "                for s in shuffled_df['encoded_lines'].iloc[train_len:train_len +\n",
    "                                                        test_len]]\n",
    "        y_test += [c for c in shuffled_df['character'].iloc[train_len:train_len +\n",
    "                                                        test_len]]\n",
    "        # print('Loading validation data...')\n",
    "        X_val += [[float(e) for e in s]\n",
    "                for s in shuffled_df['encoded_lines'].iloc[train_len +\n",
    "                                                        test_len:train_len +\n",
    "                                                        test_len + val_len]]\n",
    "        y_val += [c for c in shuffled_df['character'].iloc[train_len +\n",
    "                                                        test_len:train_len +\n",
    "                                                        test_len + val_len]]\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "    X_val = np.array(X_val)\n",
    "    y_val = np.array(y_val)\n",
    "\n",
    "    print(X_train.shape)\n",
    "\n",
    "    random_idxs_train = list(range(train_len))\n",
    "    random.shuffle(random_idxs_train)\n",
    "    random_idxs_test = list(range(test_len))\n",
    "    random.shuffle(random_idxs_test)\n",
    "    random_idxs_val = list(range(val_len))\n",
    "    random.shuffle(random_idxs_val)\n",
    "\n",
    "    X_train = X_train[random_idxs_train]\n",
    "    y_train = y_train[random_idxs_train]\n",
    "    X_test = X_test[random_idxs_test]\n",
    "    y_test = y_test[random_idxs_test]\n",
    "    X_val = X_val[random_idxs_val]\n",
    "    y_val = y_val[random_idxs_val]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading encoded lines...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [71], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train, y_train, X_test, y_test, X_val, y_val \u001b[39m=\u001b[39m get_data(source_encoded_path\u001b[39m=\u001b[39;49mos\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(\u001b[39m'\u001b[39;49m\u001b[39m..\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mData\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mCharacters\u001b[39;49m\u001b[39m'\u001b[39;49m))\n",
      "Cell \u001b[1;32mIn [70], line 33\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(source_encoded_path, random_state, n_shuffles, shutdown_at_end)\u001b[0m\n\u001b[0;32m     30\u001b[0m tmp_df \u001b[39m=\u001b[39m get_triplet_df(series_df, n_shuffles\u001b[39m=\u001b[39mn_shuffles, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m     31\u001b[0m tmp_df[\u001b[39m'\u001b[39m\u001b[39mcharacter\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m [c \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(tmp_df))]\n\u001b[1;32m---> 33\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(tmp_df[tmp_df[\u001b[39m'\u001b[39m\u001b[39mcharacter\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m!=\u001b[39m\u001b[39m0\u001b[39m]) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     35\u001b[0m \u001b[39m# shuffled_df = pd.concat([shuffled_df, tmp_df])\u001b[39;00m\n\u001b[0;32m     36\u001b[0m df_list\u001b[39m.\u001b[39mappend(tmp_df)\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test, X_val, y_val = get_data(source_encoded_path=os.path.join('..', 'Data', 'Characters'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[y_test==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def get_triplet_dataset(X, y):\n",
    "    assert len(X)==len(y)\n",
    "\n",
    "    anchors = X\n",
    "    positives = []\n",
    "    negatives = []\n",
    "    for i in range(len(X)):\n",
    "        y_ref = y[i]\n",
    "\n",
    "        pos_idxs = np.squeeze(np.where(y == y_ref))\n",
    "        neg_idxs = np.squeeze(np.where(y != y_ref))\n",
    "\n",
    "        positives.append(X[random.choice(pos_idxs)])\n",
    "        negatives.append(X[random.choice(neg_idxs)])\n",
    "\n",
    "    anchor_dataset = tf.data.Dataset.from_tensor_slices(anchors)\n",
    "    positive_dataset = tf.data.Dataset.from_tensor_slices(positives)\n",
    "    negative_dataset = tf.data.Dataset.from_tensor_slices(negatives)\n",
    "\n",
    "    dataset = tf.data.Dataset.zip((anchor_dataset, positive_dataset, negative_dataset))\n",
    "    dataset = dataset.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Cannot choose from an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [55], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_dataset \u001b[39m=\u001b[39m get_triplet_dataset(X_train, y_train)\n\u001b[0;32m      2\u001b[0m test_dataset \u001b[39m=\u001b[39m get_triplet_dataset(X_test, y_test)\n\u001b[0;32m      3\u001b[0m val_dataset \u001b[39m=\u001b[39m get_triplet_dataset(X_val, y_val)\n",
      "Cell \u001b[1;32mIn [54], line 15\u001b[0m, in \u001b[0;36mget_triplet_dataset\u001b[1;34m(X, y)\u001b[0m\n\u001b[0;32m     12\u001b[0m     neg_idxs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msqueeze(np\u001b[39m.\u001b[39mwhere(y \u001b[39m!=\u001b[39m y_ref))\n\u001b[0;32m     14\u001b[0m     positives\u001b[39m.\u001b[39mappend(X[random\u001b[39m.\u001b[39mchoice(pos_idxs)])\n\u001b[1;32m---> 15\u001b[0m     negatives\u001b[39m.\u001b[39mappend(X[random\u001b[39m.\u001b[39;49mchoice(neg_idxs)])\n\u001b[0;32m     17\u001b[0m anchor_dataset \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mfrom_tensor_slices(anchors)\n\u001b[0;32m     18\u001b[0m positive_dataset \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mfrom_tensor_slices(positives)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\random.py:290\u001b[0m, in \u001b[0;36mRandom.choice\u001b[1;34m(self, seq)\u001b[0m\n\u001b[0;32m    288\u001b[0m     i \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_randbelow(\u001b[39mlen\u001b[39m(seq))\n\u001b[0;32m    289\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m--> 290\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mCannot choose from an empty sequence\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    291\u001b[0m \u001b[39mreturn\u001b[39;00m seq[i]\n",
      "\u001b[1;31mIndexError\u001b[0m: Cannot choose from an empty sequence"
     ]
    }
   ],
   "source": [
    "train_dataset = get_triplet_dataset(X_train, y_train)\n",
    "test_dataset = get_triplet_dataset(X_test, y_test)\n",
    "val_dataset = get_triplet_dataset(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseModel(keras.Model):\n",
    "    \"\"\"The Siamese Network model with a custom training and testing loops.\n",
    "\n",
    "    Computes the triplet loss using the three embeddings produced by the\n",
    "    Siamese Network.\n",
    "\n",
    "    The triplet loss is defined as:\n",
    "       L(A, P, N) = max(‖f(A) - f(P)‖² - ‖f(A) - f(N)‖² + margin, 0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, siamese_network, margin=0.5):\n",
    "        super(SiameseModel, self).__init__()\n",
    "        self.siamese_network = siamese_network\n",
    "        self.margin = margin\n",
    "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.siamese_network(inputs)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # GradientTape is a context manager that records every operation that\n",
    "        # you do inside. We are using it here to compute the loss so we can get\n",
    "        # the gradients and apply them using the optimizer specified in\n",
    "        # `compile()`.\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self._compute_loss(data)\n",
    "\n",
    "        # Storing the gradients of the loss function with respect to the\n",
    "        # weights/parameters.\n",
    "        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n",
    "\n",
    "        # Applying the gradients on the model using the specified optimizer\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.siamese_network.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Let's update and return the training loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        loss = self._compute_loss(data)\n",
    "\n",
    "        # Let's update and return the loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def _compute_loss(self, data):\n",
    "        # The output of the network is a tuple containing the distances\n",
    "        # between the anchor and the positive example, and the anchor and\n",
    "        # the negative example.\n",
    "        ap_distance, an_distance = self.siamese_network(data)\n",
    "\n",
    "        # Computing the Triplet Loss by subtracting both distances and\n",
    "        # making sure we don't get a negative value.\n",
    "        loss = ap_distance - an_distance\n",
    "        loss = tf.maximum(loss + self.margin, 0.0)\n",
    "        return loss\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We need to list our metrics here so the `reset_states()` can be\n",
    "        # called automatically.\n",
    "        return [self.loss_tracker]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "451812313a2cc9ef7b1a116a2be532c610a0f65ac693e04b1a4edd064a67cb06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

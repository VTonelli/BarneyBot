{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers, callbacks, regularizers\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from lib.BBData import character_dict, random_state, model_name\n",
    "from transformers import BertModel, BertConfig, TFAutoModelForCausalLM, TFBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "lr = 1e-4\n",
    "patience = 6\n",
    "regularizer_weight_r = 1e-4\n",
    "regularizer_weight_s = 1e-3\n",
    "dropout_rate = 0.2\n",
    "train_size = 0.85\n",
    "test_size = 0.10\n",
    "# Instance state, for caching, in case of repeated usage of this metric\n",
    "sentence_transformer = None\n",
    "character = None\n",
    "embedding_model = None\n",
    "# Embedding params\n",
    "embedding_size = 32\n",
    "margin = 0.5*embedding_size\n",
    "\n",
    "create_classifier_dataset = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_state():\n",
    "    sentence_transformer = None\n",
    "    character = None\n",
    "    embedding_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.activations import linear\n",
    "\n",
    "def create_embedding_model(input_size):\n",
    "    # Input is a concatenated triplet of sentences\n",
    "    inputs = keras.Input(shape=input_size)\n",
    "    # Model is a concatenation of dense layers alternated by batch normalizations\n",
    "    x = layers.Dense(\n",
    "        1024,\n",
    "        activation='relu',\n",
    "    )(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Dense(\n",
    "        1024,\n",
    "        activation='relu',\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(\n",
    "        512,\n",
    "        activation='relu',\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(\n",
    "        256,\n",
    "        activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    # The last layers have L2 regularization, to avoid too high values\n",
    "    x = layers.Dense(\n",
    "        128,\n",
    "        activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    # The last layers have L2 regularization, to avoid too high values\n",
    "    x = layers.Dense(\n",
    "        64,\n",
    "        activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    # The output is the embedding for the input\n",
    "    out = layers.Dense(\n",
    "        embedding_size,\n",
    "        activation=linear)(x)\n",
    "    # Create and compile keras model\n",
    "    embedding_model = keras.Model(inputs, out)\n",
    "    return embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create siamese network for training\n",
    "def create_siamese_net(input_size, embedding_model):\n",
    "    input_anchor = layers.Input(shape=(input_size, ))\n",
    "    input_positive = layers.Input(shape=(input_size, ))\n",
    "    input_negative = layers.Input(shape=(input_size, ))\n",
    "\n",
    "    embedding_anchor = embedding_model(input_anchor)\n",
    "    embedding_positive = embedding_model(input_positive)\n",
    "    embedding_negative = embedding_model(input_negative)\n",
    "\n",
    "    output = layers.concatenate(\n",
    "        [embedding_anchor, embedding_positive, embedding_negative], axis=1)\n",
    "\n",
    "    siamese_net = keras.Model(\n",
    "        [input_anchor, input_positive, input_negative], output)\n",
    "\n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a dataset composed of triples from a dataset of single sentences. Used in training only.\n",
    "def get_triplet_df(series_df, n_shuffles, random_state):\n",
    "    # Separate lines by character from all the others\n",
    "    series_df_1 = series_df[series_df['character'] == 1].copy()\n",
    "    # Define triplet dataset as having a character label and the line, already encoded\n",
    "    df_rows = {'character': [], 'encoded_line': []}\n",
    "    # Shuffle by a parametrized amount\n",
    "    for i in range(n_shuffles):\n",
    "        # print(\"Running shuffle \" + str(i) + \"/\" + str(n_shuffles))\n",
    "        # Shuffle the dataset and balance number of 0s (we suppose its cardinality is higher than that of 1s)\n",
    "        series_df_1 = series_df_1.sample(frac=1,\n",
    "                                            random_state=random_state +\n",
    "                                            i).reset_index(drop=True)\n",
    "        # Iterate over lines\n",
    "        n = 4\n",
    "        for i in range(n, len(series_df_1)-n):\n",
    "            # Get a triple of consecutive lines for the character, and concatenate them in one sample\n",
    "            lines = list(series_df_1['encoded_line'][i - n:i + n])\n",
    "            lines = np.concatenate(lines)\n",
    "            df_rows['character'].append(1)\n",
    "            df_rows['encoded_line'].append(lines)\n",
    "    # Create a new dataframe from the rows we have built\n",
    "    df = pd.DataFrame(data=df_rows)\n",
    "    # Sample the dataset one last time to shuffle it\n",
    "    return df.sample(frac=1,\n",
    "                        random_state=random_state).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = list(character_dict.keys())\n",
    "if 'Default' in characters:\n",
    "    characters.remove('Default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_transformer = SentenceTransformer(\n",
    "        \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")\n",
    "if create_classifier_dataset:\n",
    "    for character in characters:\n",
    "        source = character_dict[character]['source']\n",
    "        series_df_path = os.path.join('..', 'Data', 'Sources', source, source+'.csv')\n",
    "        series_df = pd.read_csv(series_df_path)\n",
    "        \n",
    "        # Apply class labelling to the dataset sentences\n",
    "        series_df['character'] = series_df['character'].apply(\n",
    "            lambda x: 1 if x == character else 0)\n",
    "        # Throw away unnecessary dataset rows\n",
    "        series_df = series_df[['character', 'line']]\n",
    "        # Encode lines and add them to the dataset as a new row\n",
    "        series_df['encoded_line'] = [\n",
    "            sentence_transformer.encode(line)\n",
    "            for line in tqdm(series_df['line'])\n",
    "        ]\n",
    "        # Save the dataset rows as a csv file\n",
    "        p = os.path.join('..', 'Data', 'Characters', character)\n",
    "        save_path = os.path.join(p, character.lower()+'_classifier.csv')\n",
    "        series_df[['line', 'character']].to_csv(save_path, index=False)\n",
    "        # The encoded lines are saved separately via numpy due to their type (array)\n",
    "        np.save(os.path.join(os.path.join(p, character.lower() + '_encoded_lines.npy')),\n",
    "                series_df['encoded_line'].to_numpy())\n",
    "        print(\"Saved encoded lines at \" + p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(\n",
    "    source_encoded_path,\n",
    "    random_state=random_state,\n",
    "    n_shuffles=10,\n",
    "    use_triplets=False\n",
    "    ):\n",
    "\n",
    "    # Flush the instance state cache\n",
    "    reset_state()\n",
    "\n",
    "    # shuffled_df = pd.DataFrame.from_dict({'line':[], 'character':[]})\n",
    "    df_list = []\n",
    "    print('Loading encoded lines...')\n",
    "    for c in tqdm(range(len(characters))):\n",
    "        # Load the preprocessed dataset\n",
    "        series_df = pd.read_csv(os.path.join(\n",
    "            source_encoded_path, characters[c],\n",
    "            characters[c].lower() + '_classifier.csv'),\n",
    "                                dtype={\n",
    "                                    'line': str,\n",
    "                                    'character': int\n",
    "                                })\n",
    "\n",
    "        # Load encoded lines dataset, via numpy, and add it as a new row in the dataset\n",
    "        series_df['encoded_line'] = np.load(os.path.join(\n",
    "            source_encoded_path, characters[c],\n",
    "            characters[c].lower() + '_encoded_lines.npy'),\n",
    "                                            allow_pickle=True)\n",
    "        #print(\"Loaded encoded lines from \" + source_encoded_path + '/' + characters[c])\n",
    "        if use_triplets:\n",
    "            tmp_df = get_triplet_df(series_df, n_shuffles=n_shuffles, random_state=random_state)\n",
    "        else:\n",
    "            tmp_df = series_df[series_df['character']==1].reset_index()[['encoded_line', 'character']]\n",
    "        tmp_df['character'] = [c for _ in range(len(tmp_df))]\n",
    "\n",
    "        # shuffled_df = pd.concat([shuffled_df, tmp_df])\n",
    "        df_list.append(tmp_df)\n",
    "\n",
    "    #print(pd.concat(df_list).sample(frac=1).head(10))\n",
    "\n",
    "    tot_len = min([len(df) for df in df_list])\n",
    "    # Store into variables the train, val, test, total lengths of the new (triplets) dataset\n",
    "    train_len = int(tot_len * train_size)\n",
    "    test_len = int(tot_len * test_size)\n",
    "    val_len = tot_len - train_len - test_len\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    X_val = []\n",
    "    y_val = []\n",
    "    print('Creating merged data...')\n",
    "    for shuffled_df in tqdm(df_list):\n",
    "        # Load triples into numpy arrays, separating data and labels\n",
    "        # print('Loading training data...')\n",
    "        shuffled_df = shuffled_df.sample(frac=1)\n",
    "        shuffled_df = shuffled_df.iloc[:tot_len]\n",
    "        X_train += [[float(e) for e in s]\n",
    "                for s in shuffled_df['encoded_line'].iloc[:train_len]]\n",
    "        y_train += shuffled_df['character'].iloc[:train_len].tolist()\n",
    "        # print('Loading test data...')\n",
    "        X_test += [[float(e) for e in s]\n",
    "                for s in shuffled_df['encoded_line'].iloc[train_len:train_len +\n",
    "                                                        test_len]]\n",
    "        y_test += shuffled_df['character'].iloc[train_len:train_len + test_len].tolist()\n",
    "        # print('Loading validation data...')\n",
    "        X_val += [[float(e) for e in s]\n",
    "                for s in shuffled_df['encoded_line'].iloc[train_len +\n",
    "                                                        test_len:train_len +\n",
    "                                                        test_len + val_len]]\n",
    "        y_val += shuffled_df['character'].iloc[train_len+test_len:train_len+test_len+val_len].tolist()\n",
    "    \n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "    X_val = np.array(X_val)\n",
    "    y_val = np.array(y_val)\n",
    "\n",
    "    assert len(y_test[y_test!=0]) > 0, 'assertion before randomization'\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test, X_val, y_val = get_data(\n",
    "    source_encoded_path=os.path.join('..', 'Data', 'Characters'),\n",
    "    use_triplets=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def get_triplet_dataset(X, y):\n",
    "    assert len(X)==len(y)\n",
    "\n",
    "    anchors = X\n",
    "    positives = []\n",
    "    negatives = []\n",
    "    print('Creating triplets...')\n",
    "    for i in tqdm(range(len(X))):\n",
    "        y_ref = y[i]\n",
    "\n",
    "        pos_idxs = np.squeeze(np.where(y == y_ref))\n",
    "        neg_idxs = np.squeeze(np.where(y != y_ref))\n",
    "\n",
    "        positives.append(X[random.choice(pos_idxs)])\n",
    "        negatives.append(X[random.choice(neg_idxs)])\n",
    "    \n",
    "    anchors = np.array(anchors)[:,np.newaxis,:]\n",
    "    positives = np.array(positives)[:,np.newaxis,:]\n",
    "    negatives = np.array(negatives)[:,np.newaxis,:]\n",
    "\n",
    "    anchor_dataset = tf.data.Dataset.from_tensor_slices(anchors)\n",
    "    positive_dataset = tf.data.Dataset.from_tensor_slices(positives)\n",
    "    negative_dataset = tf.data.Dataset.from_tensor_slices(negatives)\n",
    "\n",
    "    print('Zipping to unique dataset...')\n",
    "    dataset = tf.data.Dataset.zip((anchor_dataset, positive_dataset, negative_dataset))\n",
    "    print('Shuffling...')\n",
    "    dataset = dataset.shuffle(buffer_size=1024)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_triplet_dataset(X_train, y_train)\n",
    "test_dataset = get_triplet_dataset(X_test, y_test)\n",
    "val_dataset = get_triplet_dataset(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistanceLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    This layer is responsible for computing the distance between the anchor\n",
    "    embedding and the positive embedding, and the anchor embedding and the\n",
    "    negative embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, anchor, positive, negative):\n",
    "        ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)\n",
    "        an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)\n",
    "        return (ap_distance, an_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseModel(keras.Model):\n",
    "    \"\"\"The Siamese Network model with a custom training and testing loops.\n",
    "\n",
    "    Computes the triplet loss using the three embeddings produced by the\n",
    "    Siamese Network.\n",
    "\n",
    "    The triplet loss is defined as:\n",
    "       L(A, P, N) = max(‖f(A) - f(P)‖² - ‖f(A) - f(N)‖² + margin, 0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, siamese_network, margin=0.5):\n",
    "        super(SiameseModel, self).__init__()\n",
    "        self.siamese_network = siamese_network\n",
    "        self.margin = margin\n",
    "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.siamese_network(inputs)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # GradientTape is a context manager that records every operation that\n",
    "        # you do inside. We are using it here to compute the loss so we can get\n",
    "        # the gradients and apply them using the optimizer specified in\n",
    "        # `compile()`.\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self._compute_loss(data)\n",
    "\n",
    "        # Storing the gradients of the loss function with respect to the\n",
    "        # weights/parameters.\n",
    "        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n",
    "\n",
    "        # Applying the gradients on the model using the specified optimizer\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.siamese_network.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Let's update and return the training loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        loss = self._compute_loss(data)\n",
    "\n",
    "        # Let's update and return the loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def _compute_loss(self, data):\n",
    "        # The output of the network is a tuple containing the distances\n",
    "        # between the anchor and the positive example, and the anchor and\n",
    "        # the negative example.\n",
    "        ap_distance, an_distance = self.siamese_network(data)\n",
    "\n",
    "        # Computing the Triplet Loss by subtracting both distances and\n",
    "        # making sure we don't get a negative value.\n",
    "        loss = ap_distance - an_distance\n",
    "        loss = tf.maximum(loss + self.margin, 0.0)\n",
    "        return loss\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We need to list our metrics here so the `reset_states()` can be\n",
    "        # called automatically.\n",
    "        return [self.loss_tracker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_benderbot = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = create_embedding_model(input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding = TFAutoModelForCausalLM.from_pretrained(model_name, cache_dir=os.path.join('..', \"cache\")) # not working\n",
    "# embedding = TFBertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "anchor_input = layers.Input(name=\"anchor\", shape=input_size)\n",
    "positive_input = layers.Input(name=\"positive\", shape=input_size)\n",
    "negative_input = layers.Input(name=\"negative\", shape=input_size)\n",
    "\n",
    "# embedding = create_embedding_model(input_size)\n",
    "\n",
    "distances = DistanceLayer()(\n",
    "    embedding(anchor_input),\n",
    "    embedding(positive_input),\n",
    "    embedding(negative_input),\n",
    ")\n",
    "\n",
    "siamese_network = keras.Model(\n",
    "    inputs=[anchor_input, positive_input, negative_input], outputs=distances,\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop_callback = callbacks.EarlyStopping(\n",
    "    monitor=\"loss\",\n",
    "    min_delta=0,\n",
    "    patience=5,\n",
    "    verbose=0,\n",
    "    mode=\"min\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model = SiameseModel(siamese_network, margin=margin)\n",
    "siamese_model.compile(optimizer=keras.optimizers.Adam(lr), weighted_metrics=[])\n",
    "siamese_model.fit(\n",
    "    train_dataset, \n",
    "    epochs=5000, \n",
    "    validation_data=val_dataset, \n",
    "    batch_size=128,\n",
    "    callbacks=[earlystop_callback]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings = embedding(X_test).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "\n",
    "kmeans = KMeans(n_clusters=len(characters), random_state=random_state).fit(test_embeddings)\n",
    "dbscan = DBSCAN().fit(test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_kmeans = kmeans.labels_\n",
    "y_pred_dbscan = dbscan.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test, \n",
    "    y_pred_kmeans, \n",
    "    normalize='true',\n",
    "    display_labels=characters)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = embedding(X_train).numpy()\n",
    "kmeans = KMeans(n_clusters=len(characters), random_state=random_state).fit(train_embeddings)\n",
    "y_pred = kmeans.labels_\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_train, \n",
    "    y_pred, \n",
    "    normalize='true',\n",
    "    display_labels=characters)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=0.2).fit(train_embeddings)\n",
    "labels = dbscan.labels_\n",
    "# labels[labels!=0]\n",
    "print(max(labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "451812313a2cc9ef7b1a116a2be532c610a0f65ac693e04b1a4edd064a67cb06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import DataCollatorForLanguageModeling, AutoTokenizer, TFAutoModelForCausalLM, AdamWeightDecay\n",
    "from nltk.corpus import stopwords\n",
    "from os.path import join, exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import character_dict, model_name, preprocess_function, load_df, get_predictions_cached, get_dataframe_for_metrics\n",
    "\n",
    "from metrics import freq_pairwise_sim, filter_by_weights, get_word_frequency, get_tfidfs, FrequencyChatbotClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = list(character_dict.keys())\n",
    "characters.remove('Default')\n",
    "\n",
    "mass_value = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount google drive, if in Colaboratory environment\n",
    "import os\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive',force_remount=True)\n",
    "    base_folder = '/content/drive/My Drive/unibo/NLP_project/BarneyBot'\n",
    "    os.system(\"pip install datasets\")\n",
    "    os.system(\"pip install transformers\")\n",
    "    os.system(\"pip install rouge_score\")\n",
    "    os.system(\"pip install -U sentence-transformers\")\n",
    "else:\n",
    "    # base_folder = os.getcwd()\n",
    "    base_folder = '..'\n",
    "\n",
    "out_folder = os.path.join(base_folder, 'Data', 'Characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_preprocessing(sentence):\n",
    "    sentence = re.sub(r'[^A-Za-z\\s]', ' ', sentence)\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_docs = dict()\n",
    "for character in characters:\n",
    "    df = pd.read_csv(os.path.join(out_folder, character, f'{character}.csv'))\n",
    "    character_docs[character] = df['response'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for character in characters:\n",
    "    for i in tqdm(range(len(character_docs[character]))):\n",
    "        character_docs[character][i] = sentence_preprocessing(character_docs[character][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.33\n",
    "character_docs_train = {}\n",
    "character_docs_test = {}\n",
    "for c in characters:\n",
    "    shuffle(character_docs[c])\n",
    "    end_idx = int(len(character_docs[c]) * test_size)\n",
    "    character_docs_train[c] = character_docs[c][end_idx:]\n",
    "    character_docs_test[c] = character_docs[c][:end_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfreqs = dict()\n",
    "for character in tqdm(characters):\n",
    "    wordfreqs[character] = get_word_frequency(' '.join(character_docs[character]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfreqs_reduced = dict()\n",
    "for character in characters:\n",
    "    wordfreqs_reduced[character] = filter_by_weights(wordfreqs[character], mass=mass_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(input='content', stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfs = get_tfidfs([' '.join(character_docs[character]) for character in characters], characters, tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfs_reduced = dict()\n",
    "for character in characters:\n",
    "    tfidfs_reduced[character] = filter_by_weights(tfidfs[character], mass=mass_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordCloud Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "def plot_word_cloud(freqdict, cmap='viridis', title=None, plot=False):\n",
    "    wordcloud = WordCloud(background_color = 'black', width = 800, height = 400,\n",
    "                      colormap = cmap, max_words = 180, contour_width = 3,\n",
    "                      max_font_size = 80, contour_color = 'steelblue',\n",
    "                      random_state = 0)\n",
    "\n",
    "    wordcloud.generate_from_frequencies(freqdict)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_word_cloud(tfidfs_reduced['Barney'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Pairwise Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_pairwise_sim(tfidfs_reduced['Fry'], tfidfs_reduced['Barney'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test classifiers on test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf_classifier = FrequencyChatbotClassifier(characters, mode='word frequency')\n",
    "wf_classifier.train(list(character_docs_train.values()))\n",
    "predictions = []\n",
    "for c in tqdm(characters):\n",
    "    prediction = wf_classifier.predict(character_docs_test[c], mass=mass_value)\n",
    "    predictions.append(\n",
    "        int(max(prediction, key=prediction.get) == c)\n",
    "    )\n",
    "\n",
    "print('Frequency classifier test accuracy: {:.2f}'.format(sum(predictions)/len(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_classifier = FrequencyChatbotClassifier(characters, mode='tf-idf')\n",
    "tfidf_classifier.train(list(character_docs_train.values()))\n",
    "predictions = []\n",
    "for c in characters:\n",
    "    prediction = tfidf_classifier.predict(character_docs_test[c], mass=mass_value)\n",
    "    predictions.append(\n",
    "        int(max(prediction, key=prediction.get) == c)\n",
    "    )\n",
    "\n",
    "print('TF-IDF classifier test accuracy: {:.2f}'.format(sum(predictions)/len(predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test classifiers on chatbot sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_classifier = FrequencyChatbotClassifier(characters, mode='tf-idf')\n",
    "tfidf_classifier.train(list(character_docs.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=join(\"cache\"))\n",
    "tokenizer.pad_token = '#'\n",
    "data_collator = DataCollatorForLanguageModeling(mlm=False, tokenizer=tokenizer, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create dataset\n",
    "n_tests = 10\n",
    "doc_test = []\n",
    "batch_size = 8\n",
    "override_predictions = True\n",
    "predictions = {c:[] for c in characters}\n",
    "raw_predictions = {c:[] for c in characters}\n",
    "print('Creating dataset...')\n",
    "if n_tests > 1 and not override_predictions:\n",
    "    raise Exception('must override previous predictions if you need more tests')\n",
    "\n",
    "for i in range(n_tests):\n",
    "    print(f'Run {i}/{n_tests}')\n",
    "    for character in tqdm(characters):\n",
    "        character_checkpoint = join(out_folder, character, character_dict[character]['checkpoint_folder'])\n",
    "        model_chatbot = TFAutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=character_checkpoint) if override_predictions else None\n",
    "        if model_chatbot:\n",
    "            model_chatbot.compile(optimizer=AdamWeightDecay(learning_rate=2e-5))\n",
    "\n",
    "        character_hg = load_df(character)\n",
    "        # This transform in a sequence of tokens ours dataset\n",
    "        tokenized_character_hg = character_hg.map(preprocess_function, batched=False)\n",
    "\n",
    "        # Define tensorflow datasets\n",
    "        encoded_test_set = tokenized_character_hg[\"test\"].to_tf_dataset(\n",
    "            columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "            shuffle=False,\n",
    "            batch_size=batch_size,\n",
    "            collate_fn=data_collator,\n",
    "        )\n",
    "\n",
    "        # Takes the testset as sample question \n",
    "        sample_questions = character_hg['test']['context/0']\n",
    "\n",
    "        # Sampling generation method\n",
    "        predictions_sampling = get_predictions_cached(\n",
    "            sample_questions,\n",
    "            model_chatbot,\n",
    "            character_dict[character]['prediction_filename'] + '_sampling.json',\n",
    "            \"Sampling\",\n",
    "            character,\n",
    "            tokenizer,\n",
    "            override_predictions=override_predictions\n",
    "        )\n",
    "                                                    \n",
    "        sentences = get_dataframe_for_metrics(character_hg['test'], None, None, predictions_sampling, tokenizer)['prd_sampling'].tolist()\n",
    "        doc_test.append([sentence_preprocessing(s) for s in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### prediction on last test\n",
    "print('Classification...')\n",
    "for c in tqdm(range(len(characters))):\n",
    "    prediction = tfidf_classifier.predict(doc_test[c], mass=mass_value)\n",
    "    raw_predictions[characters[c]].append(prediction)\n",
    "    predictions[characters[c]].append(\n",
    "        int(max(prediction, key=prediction.get) == characters[c])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TF-IDF classifier test accuracy: {:.2f}'.format(sum([char_pred[-1] for char_pred in predictions.values()])/len(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save predictions\n",
    "append_predictions = True\n",
    "override_predictions = False\n",
    "predictions_file = join('..', 'Data', 'tfidf_predictions.json')\n",
    "\n",
    "if append_predictions and exists(predictions_file):\n",
    "    with open(predictions_file, 'r', encoding='utf-8') as file:\n",
    "        predictions_dict = json.load(file)\n",
    "elif override_predictions or not exists(predictions_file):\n",
    "    predictions_dict = {'one_hot':{c:[] for c in characters}, 'raw_predictions': {c:[] for c in characters}}\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "    \n",
    "for c in characters:\n",
    "    predictions_dict['one_hot'][c] += predictions[c]\n",
    "    predictions_dict['raw_predictions'][c] += raw_predictions[c]\n",
    "\n",
    "with open(predictions_file, 'w', encoding='utf-8') as file:\n",
    "    json.dump(predictions_dict, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "451812313a2cc9ef7b1a116a2be532c610a0f65ac693e04b1a4edd064a67cb06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

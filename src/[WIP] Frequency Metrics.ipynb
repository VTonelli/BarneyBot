{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import DataCollatorForLanguageModeling, AutoTokenizer, TFAutoModelForCausalLM, AdamWeightDecay\n",
    "from os.path import join, exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.BBData import character_dict, model_name\n",
    "from lib.BBDataLoad import dialogpt_preprocess_function, load_char_df, get_chatbot_predictions, merge_df_for_metrics\n",
    "\n",
    "from lib.wip.frequency import sentence_preprocess, freq_pairwise_sim, filter_by_weights, get_word_frequency, get_tfidfs, FrequencyChatbotClassifier\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = list(character_dict.keys())\n",
    "characters.remove('Default')\n",
    "\n",
    "mass_value = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount google drive, if in Colaboratory environment\n",
    "import os\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive',force_remount=True)\n",
    "    base_folder = '/content/drive/My Drive/unibo/NLP_project/BarneyBot'\n",
    "    os.system(\"pip install datasets\")\n",
    "    os.system(\"pip install transformers\")\n",
    "    os.system(\"pip install rouge_score\")\n",
    "    os.system(\"pip install -U sentence-transformers\")\n",
    "else:\n",
    "    # base_folder = os.getcwd()\n",
    "    base_folder = '..'\n",
    "\n",
    "out_folder = os.path.join(base_folder, 'Data', 'Characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_docs = dict()\n",
    "for character in characters:\n",
    "    df = pd.read_csv(os.path.join(out_folder, character, f'{character}.csv'))\n",
    "    character_docs[character] = df['response'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for character in characters:\n",
    "    for i in tqdm(range(len(character_docs[character]))):\n",
    "        sentence, relevant = sentence_preprocess(character_docs[character][i])\n",
    "        if relevant:\n",
    "            character_docs[character][i] = sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.33\n",
    "character_docs_train = {}\n",
    "character_docs_test = {}\n",
    "for c in characters:\n",
    "    shuffle(character_docs[c])\n",
    "    end_idx = int(len(character_docs[c]) * test_size)\n",
    "    character_docs_train[c] = character_docs[c][end_idx:]\n",
    "    character_docs_test[c] = character_docs[c][:end_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfreqs_train = dict()\n",
    "for character in tqdm(characters):\n",
    "    wordfreqs_train[character] = get_word_frequency(' '.join(character_docs_train[character]), f_sorted=True)\n",
    "\n",
    "wordfreqs_test = dict()\n",
    "for character in tqdm(characters):\n",
    "    wordfreqs_test[character] = get_word_frequency(' '.join(character_docs_test[character]), f_sorted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfreqs_reduced_train = dict()\n",
    "for character in characters:\n",
    "    wordfreqs_reduced_train[character] = filter_by_weights(wordfreqs_train[character], mass=0.3)\n",
    "\n",
    "wordfreqs_reduced_test = dict()\n",
    "for character in characters:\n",
    "    wordfreqs_reduced_test[character] = filter_by_weights(wordfreqs_test[character], mass=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading TF-IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(input='content', stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfs_train = get_tfidfs([' '.join(character_docs_train[character]) for character in characters], characters, tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfs_reduced_train = dict()\n",
    "for character in characters:\n",
    "    tfidfs_reduced_train[character] = filter_by_weights(tfidfs_train[character], mass=mass_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordCloud Plot (Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.BBVisualizations import BBVisualization\n",
    "BBVisualization.load_visualization(\"wordcloud\").plot(freqdict=tfidfs_reduced_train['Barney'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Pairwise Similarity (Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_pairwise_sim(wordfreqs_reduced_train['Fry'], wordfreqs_reduced_test['Fry'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Classifier Performances Against Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf_classifier = FrequencyChatbotClassifier(characters, mode='word frequency')\n",
    "wf_classifier.train(list(character_docs_train.values()))\n",
    "\n",
    "y_true = range(0, len(characters))\n",
    "y_pred = [np.argmax(list(wf_classifier.predict(character_docs_test[character], mass=0.3).values()))\n",
    "          for character in characters]\n",
    "\n",
    "print('Word Frequency classifier test accuracy: {:.2f}'.format(\n",
    "             sum([y_pred[i] == y_true[i] for i in range(len(y_true))]) / len(y_true)))\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_classifier = FrequencyChatbotClassifier(characters, mode='tf-idf')\n",
    "tfidf_classifier.train(list(character_docs_train.values()))\n",
    "\n",
    "y_true = range(0, len(characters))\n",
    "y_pred = [np.argmax(list(tfidf_classifier.predict(character_docs_test[character], mass=0.3).values()))\n",
    "          for character in characters]\n",
    "\n",
    "print('TF-IDF classifier test accuracy: {:.2f}'.format(\n",
    "             sum([y_pred[i] == y_true[i] for i in range(len(y_true))]) / len(y_true)))\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Classifiers On Chatbots (Not Cached!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_classifier = FrequencyChatbotClassifier(characters, mode='tf-idf')\n",
    "tfidf_classifier.train(list(character_docs.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=join(\"..\", \"cache\"))\n",
    "tokenizer.pad_token = '#'\n",
    "data_collator = DataCollatorForLanguageModeling(mlm=False, tokenizer=tokenizer, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create dataset\n",
    "n_tests = 10\n",
    "doc_test = {c:[] for c in characters}\n",
    "batch_size = 32\n",
    "override_predictions = True\n",
    "predictions = {c:[] for c in characters}\n",
    "raw_predictions = {c:[] for c in characters}\n",
    "print('Creating dataset...')\n",
    "if n_tests > 1 and not override_predictions:\n",
    "    raise Exception('must override previous predictions if you need more tests')\n",
    "\n",
    "for character in characters:\n",
    "    print('Character: ', character)\n",
    "    for i in range(n_tests):\n",
    "        print(f'Test {i+1}/{n_tests}')\n",
    "        character_checkpoint = join(out_folder, character, character_dict[character]['checkpoint_folder'])\n",
    "        model_chatbot = TFAutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=character_checkpoint) if override_predictions else None\n",
    "        if model_chatbot:\n",
    "            model_chatbot.compile(optimizer=AdamWeightDecay(learning_rate=2e-5))\n",
    "\n",
    "        character_hg = load_char_df(character, base_folder)\n",
    "        # This transform in a sequence of tokens ours dataset\n",
    "        tokenized_character_hg = character_hg.map(lambda row: dialogpt_preprocess_function(row, tokenizer), batched=False)\n",
    "\n",
    "        # Define tensorflow datasets\n",
    "        encoded_test_set = tokenized_character_hg[\"test\"].to_tf_dataset(\n",
    "            columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "            shuffle=False,\n",
    "            batch_size=batch_size,\n",
    "            collate_fn=data_collator,\n",
    "        )\n",
    "\n",
    "        # Takes the testset as sample question \n",
    "        sample_questions = character_hg['test']['context/0']\n",
    "\n",
    "        # Sampling generation method\n",
    "        predictions_sampling = get_chatbot_predictions(\n",
    "            sample_questions,\n",
    "            model_chatbot,\n",
    "            character_dict[character]['prediction_filename'] + '_sampling.json',\n",
    "            \"Sampling\",\n",
    "            character,\n",
    "            tokenizer,\n",
    "            base_folder,\n",
    "            override_predictions=override_predictions\n",
    "        )\n",
    "                                                    \n",
    "        sentences = merge_df_for_metrics(character_hg['test'], None, None, predictions_sampling, tokenizer)['prd_sampling'].tolist()\n",
    "        doc_test[character].append([sentence_preprocess(s) for s in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Classification...')\n",
    "for c in range(len(characters)):\n",
    "    print('Character: ', characters[c])\n",
    "    for doc in doc_test[characters[c]]:\n",
    "        prediction = tfidf_classifier.predict(doc, mass=mass_value)\n",
    "        raw_predictions[characters[c]].append(prediction)\n",
    "        predictions[characters[c]].append(\n",
    "            int(max(prediction, key=prediction.get) == characters[c])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TF-IDF classifier test accuracy: {:.2f}'.format(sum([char_pred[-1] for char_pred in predictions.values()])/len(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save predictions\n",
    "append_predictions = True\n",
    "override_predictions = False\n",
    "predictions_file = join('..', 'Data', 'tfidf_predictions.json')\n",
    "\n",
    "if append_predictions and exists(predictions_file):\n",
    "    with open(predictions_file, 'r', encoding='utf-8') as file:\n",
    "        predictions_dict = json.load(file)\n",
    "elif override_predictions or not exists(predictions_file):\n",
    "    predictions_dict = {'one_hot':{c:[] for c in characters}, 'raw_predictions': {c:[] for c in characters}}\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "    \n",
    "for c in characters:\n",
    "    predictions_dict['one_hot'][c] += predictions[c]\n",
    "    predictions_dict['raw_predictions'][c] += raw_predictions[c]\n",
    "\n",
    "with open(predictions_file, 'w', encoding='utf-8') as file:\n",
    "    json.dump(predictions_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "451812313a2cc9ef7b1a116a2be532c610a0f65ac693e04b1a4edd064a67cb06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

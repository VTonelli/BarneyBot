{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\Documents\\unibo\\natural_language_processing\\project\\BarneyBot\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers, callbacks, regularizers\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from lib.BBData import character_dict, random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, SentencesDataset, losses\n",
    "from sentence_transformers.readers import InputExample\n",
    "from sentence_transformers import models\n",
    "from transformers import BertModel\n",
    "from torch.nn import LeakyReLU, Identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from sentence_transformers.losses import TripletDistanceMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "lr = 1e-3\n",
    "epochs = 20\n",
    "patience = 6\n",
    "regularizer_weight_r = 1e-4\n",
    "regularizer_weight_s = 1e-3\n",
    "dropout_rate = 0.2\n",
    "train_size = 0.85\n",
    "test_size = 0.10\n",
    "# Instance state, for caching, in case of repeated usage of this metric\n",
    "sentence_transformer = None\n",
    "character = None\n",
    "embedding_model = None\n",
    "# Embedding params\n",
    "embedding_size = 32\n",
    "margin = embedding_size * 10\n",
    "n_merged_sentences_x_sample = 5\n",
    "n_triplets_x_sample = 1\n",
    "training_steps = 50\n",
    "\n",
    "create_classifier_dataset = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a dataset composed of triples from a dataset of single sentences. Used in training only.\n",
    "def get_triplet_df(series_df, n_shuffles, random_state, n=3):\n",
    "    # Separate lines by character from all the others\n",
    "    series_df_1 = series_df[series_df['character'] == 1].copy()\n",
    "    # Define triplet dataset as having a character label and the line, already encoded\n",
    "    df_rows = {'character': [], 'line': []}\n",
    "    # Shuffle by a parametrized amount\n",
    "    for i in range(n_shuffles):\n",
    "        # print(\"Running shuffle \" + str(i) + \"/\" + str(n_shuffles))\n",
    "        # Shuffle the dataset and balance number of 0s (we suppose its cardinality is higher than that of 1s)\n",
    "        series_df_1 = series_df_1.sample(frac=1,\n",
    "                                            random_state=random_state +\n",
    "                                            i).reset_index(drop=True)\n",
    "        # Iterate over lines\n",
    "        for i in range(n, len(series_df_1)-n+1):\n",
    "            # Get a triple of consecutive lines for the character, and concatenate them in one sample\n",
    "            lines = ' '.join(series_df_1['line'][i - n:i + n])\n",
    "            df_rows['character'].append(1)\n",
    "            df_rows['line'].append(lines)\n",
    "    # Create a new dataframe from the rows we have built\n",
    "    df = pd.DataFrame(data=df_rows)\n",
    "    # Sample the dataset one last time to shuffle it\n",
    "    return df.sample(frac=1,\n",
    "                        random_state=random_state).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = list(character_dict.keys())\n",
    "if 'Default' in characters:\n",
    "    characters.remove('Default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_state():\n",
    "    sentence_transformer = None\n",
    "    character = None\n",
    "    embedding_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(\n",
    "    source_encoded_path,\n",
    "    random_state=random_state,\n",
    "    n_shuffles=10,\n",
    "    use_triplets=False,\n",
    "    n=3\n",
    "    ):\n",
    "\n",
    "    # Flush the instance state cache\n",
    "    reset_state()\n",
    "\n",
    "    # shuffled_df = pd.DataFrame.from_dict({'line':[], 'character':[]})\n",
    "    df_list = []\n",
    "    print('Loading encoded lines...')\n",
    "    for c in tqdm(range(len(characters))):\n",
    "        # Load the preprocessed dataset\n",
    "        series_df = pd.read_csv(os.path.join(\n",
    "            source_encoded_path, characters[c],\n",
    "            characters[c].lower() + '_classifier.csv'),\n",
    "                                dtype={\n",
    "                                    'line': str,\n",
    "                                    'character': int\n",
    "                                })\n",
    "\n",
    "        #print(\"Loaded encoded lines from \" + source_encoded_path + '/' + characters[c])\n",
    "        if use_triplets:\n",
    "            tmp_df = get_triplet_df(series_df, n_shuffles=n_shuffles, random_state=random_state, n=n)\n",
    "        else:\n",
    "            tmp_df = series_df[series_df['character']==1].reset_index()[['line', 'character']]\n",
    "        tmp_df['character'] = [c for _ in range(len(tmp_df))]\n",
    "\n",
    "        # shuffled_df = pd.concat([shuffled_df, tmp_df])\n",
    "        df_list.append(tmp_df)\n",
    "\n",
    "    #print(pd.concat(df_list).sample(frac=1).head(10))\n",
    "\n",
    "    tot_len = min([len(df) for df in df_list])\n",
    "    # Store into variables the train, val, test, total lengths of the new (triplets) dataset\n",
    "    train_len = int(tot_len * train_size)\n",
    "    test_len = int(tot_len * test_size)\n",
    "    val_len = tot_len - train_len - test_len\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    X_val = []\n",
    "    y_val = []\n",
    "    print('Creating merged data...')\n",
    "    for shuffled_df in tqdm(df_list):\n",
    "        # Load triples into numpy arrays, separating data and labels\n",
    "        # print('Loading training data...')\n",
    "        shuffled_df = shuffled_df.sample(frac=1)\n",
    "        shuffled_df = shuffled_df.iloc[:tot_len]\n",
    "        X_train += shuffled_df['line'].iloc[:train_len].tolist()\n",
    "        y_train += shuffled_df['character'].iloc[:train_len].tolist()\n",
    "        # print('Loading test data...')\n",
    "        X_test += shuffled_df['line'].iloc[train_len:train_len +\n",
    "                                                        test_len].tolist()\n",
    "        y_test += shuffled_df['character'].iloc[train_len:train_len+test_len].tolist()\n",
    "        # print('Loading validation data...')\n",
    "        X_val += shuffled_df['line'].iloc[train_len+test_len:].to_list()\n",
    "        y_val += shuffled_df['character'].iloc[train_len+test_len:].tolist()\n",
    "    \n",
    "    assert len([y for y in y_train if y!=0]) > 0\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test, X_val, y_val = get_data(\n",
    "    source_encoded_path=os.path.join('..', 'Data', 'Characters'),\n",
    "    use_triplets=True, n=n_merged_sentences_x_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "# model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "# model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.training = False\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = models.Dense(in_features=model.get_sentence_embedding_dimension(), out_features=embedding_size, activation_function=Identity())\n",
    "model.add_module('dense', dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_embeddings = model.encode(X_test)\n",
    "# kmeans_test = KMeans(n_clusters=len(characters), random_state=random_state).fit(test_embeddings)\n",
    "# y_pred_kmeans_test = kmeans_test.labels_\n",
    "# ConfusionMatrixDisplay.from_predictions(\n",
    "#     y_test, \n",
    "#     y_pred_kmeans_test, \n",
    "#     normalize='pred',\n",
    "#     display_labels=characters)\n",
    "# plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triplet_dataset(X, y, n_triplets_x_sample, model=None, margin=None, verbose=False):\n",
    "    assert len(X)==len(y)\n",
    "\n",
    "    #n_triplets_x_sample = max(1, int(len(X) * n_triplets_x_sample))\n",
    "\n",
    "    print('Creating triplets...')\n",
    "    examples = []\n",
    "    hard_negatives_count = 0\n",
    "    easy_positives_count = 0\n",
    "    for i in tqdm(range(len(X))):\n",
    "        y_ref = y[i]\n",
    "\n",
    "        # pos_idxs = np.squeeze(np.where(y == y_ref))\n",
    "        pos_idxs = [y_i for y_i in y if y_i==y_ref]\n",
    "        random.shuffle(pos_idxs)\n",
    "        # neg_idxs = np.squeeze(np.where(y != y_ref))\n",
    "        neg_idxs = [y_i for y_i in y if y_i!=y_ref]\n",
    "        random.shuffle(neg_idxs)\n",
    "        assert len(pos_idxs)>n_triplets_x_sample\n",
    "        assert len(neg_idxs)>n_triplets_x_sample\n",
    "\n",
    "        #positive = X[random.choice(pos_idxs)]\n",
    "        #negative = X[random.choice(neg_idxs)]\n",
    "\n",
    "        ### last thing to test: semi-hard negative mining\n",
    "\n",
    "        for pos in pos_idxs[:n_triplets_x_sample]:\n",
    "            for neg in neg_idxs[:n_triplets_x_sample]:\n",
    "                positive = X[pos]\n",
    "                negative = X[neg]\n",
    "\n",
    "                if model is not None:\n",
    "                    anchor_emb = np.array(model.encode(X[i]))\n",
    "                    positive_emb = np.array(model.encode(positive))\n",
    "                    negative_emb = np.array(model.encode(negative))\n",
    "\n",
    "                    dist_ap = np.linalg.norm(anchor_emb - positive_emb)\n",
    "                    dist_an = np.linalg.norm(anchor_emb - negative_emb)\n",
    "\n",
    "                    if dist_ap < dist_an: \n",
    "                        if dist_an < dist_ap + margin:\n",
    "                            examples.append(InputExample(texts=[X[i], positive, negative]))\n",
    "                        else:\n",
    "                            easy_positives_count += 1\n",
    "                    else:\n",
    "                        hard_negatives_count += 1\n",
    "                else:\n",
    "                    examples.append(InputExample(texts=[X[i], positive, negative]))\n",
    "        \n",
    "    if model is not None and verbose:\n",
    "        print('Dataset length:      ', len(examples))\n",
    "        print('Hard negatives count:', hard_negatives_count)\n",
    "        print('Easy positives count:', easy_positives_count)\n",
    "\n",
    "    random.shuffle(examples)\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_examples = [InputExample(texts=[X_train[i]], label=y_train[i]) for i in range(len(X_train))]\n",
    "# test_examples = [InputExample(texts=[X_test[i]], label=y_test[i]) for i in range(len(X_test))]\n",
    "# val_examples = [InputExample(texts=[X_val[i]], label=y_val[i]) for i in range(len(X_val))]\n",
    "\n",
    "# train_examples = get_triplet_dataset(X_train, y_train, n_triplets_x_sample)\n",
    "test_examples = get_triplet_dataset(X_test, y_test, n_triplets_x_sample)\n",
    "val_examples = get_triplet_dataset(X_val, y_val, n_triplets_x_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = SentencesDataset(train_examples, model)\n",
    "# train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "# train_loss = losses.TripletLoss(\n",
    "#     model=model, \n",
    "#     triplet_margin=margin,\n",
    "#     distance_metric=TripletDistanceMetric.EUCLIDEAN\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit([(train_dataloader, train_loss)], epochs=epochs, optimizer_params={'lr': lr}, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "train_loss = losses.TripletLoss(\n",
    "    model=model, \n",
    "    triplet_margin=margin,\n",
    "    distance_metric=TripletDistanceMetric.EUCLIDEAN\n",
    "    )\n",
    "for n_merged_sentences_x_sample in range(training_steps):\n",
    "    print('#'*100)\n",
    "    print(f'step {n_merged_sentences_x_sample+1}/{training_steps}')\n",
    "\n",
    "    train_examples = get_triplet_dataset(X_train, y_train, n_triplets_x_sample, model, margin, verbose=True)\n",
    "    train_dataset = SentencesDataset(train_examples, model)\n",
    "    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "    \n",
    "    model.fit([(train_dataloader, train_loss)], epochs=epochs, optimizer_params={'lr': lr}, show_progress_bar=True)\n",
    "\n",
    "model.save(os.path.join('..', 'Data', 'Metrics', 'distil_bert_embedder'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings = model.encode(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=len(characters), random_state=random_state).fit(test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_to_char = {}\n",
    "for c in range(len(characters)):\n",
    "    char_ref = X_train[y_train.index(c)]\n",
    "    char_emb = model.encode([char_ref])\n",
    "    cluster_to_char[kmeans.predict(char_emb)[0]] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_kmeans = kmeans.labels_\n",
    "y_pred_kmeans = [cluster_to_char[y] for y in y_pred_kmeans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test, \n",
    "    y_pred_kmeans, \n",
    "    normalize='true',\n",
    "    display_labels=characters)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from transformers import DataCollatorForLanguageModeling, AutoTokenizer, TFAutoModelForCausalLM, AdamWeightDecay\n",
    "from lib.BBData import character_dict, model_name\n",
    "from lib.BBDataLoad import dialogpt_preprocess_function, load_char_df, get_chatbot_predictions, merge_df_for_metrics\n",
    "from lib.wip.frequency import sentence_preprocess\n",
    "\n",
    "base_folder = '..'\n",
    "out_folder = os.path.join(base_folder, 'Data', 'Characters')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=join(\"..\", \"cache\"))\n",
    "tokenizer.pad_token = '#'\n",
    "data_collator = DataCollatorForLanguageModeling(mlm=False, tokenizer=tokenizer, return_tensors='tf')\n",
    "\n",
    "\n",
    "### create dataset\n",
    "n_tests = 1\n",
    "doc_test = {c:[] for c in characters}\n",
    "batch_size = 128\n",
    "override_predictions = False\n",
    "predictions = {c:[] for c in characters}\n",
    "raw_predictions = {c:[] for c in characters}\n",
    "print('Creating dataset...')\n",
    "if n_tests > 1 and not override_predictions:\n",
    "    raise Exception('must override previous predictions if you need more tests')\n",
    "\n",
    "for character in characters:\n",
    "    print('Character: ', character)\n",
    "    for i in range(n_tests):\n",
    "        print(f'Test {i+1}/{n_tests}')\n",
    "        character_checkpoint = join(out_folder, character, character_dict[character]['checkpoint_folder'])\n",
    "        model_chatbot = TFAutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=character_checkpoint) if override_predictions else None\n",
    "        if model_chatbot:\n",
    "            model_chatbot.compile(optimizer=AdamWeightDecay(learning_rate=2e-5))\n",
    "\n",
    "        character_hg = load_char_df(character, base_folder)\n",
    "        # This transform in a sequence of tokens ours dataset\n",
    "        tokenized_character_hg = character_hg.map(lambda row: dialogpt_preprocess_function(row, tokenizer), batched=False)\n",
    "\n",
    "        # Define tensorflow datasets\n",
    "        encoded_test_set = tokenized_character_hg[\"test\"].to_tf_dataset(\n",
    "            columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "            shuffle=False,\n",
    "            batch_size=batch_size,\n",
    "            collate_fn=data_collator,\n",
    "        )\n",
    "\n",
    "        # Takes the testset as sample question \n",
    "        sample_questions = character_hg['test']['context/0']\n",
    "\n",
    "        # Sampling generation method\n",
    "        predictions_sampling = get_chatbot_predictions(\n",
    "            sample_questions,\n",
    "            model_chatbot,\n",
    "            character_dict[character]['prediction_filename'] + '_sampling.json',\n",
    "            \"Sampling\",\n",
    "            character,\n",
    "            tokenizer,\n",
    "            base_folder,\n",
    "            override_predictions=override_predictions\n",
    "        )\n",
    "                                                    \n",
    "        sentences = merge_df_for_metrics(character_hg['test'], None, None, predictions_sampling, tokenizer)['prd_sampling'].tolist()\n",
    "        doc_test[character].append([sentence_preprocess(s)[0] for s in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "df_list = []\n",
    "X_test_chatbot = []\n",
    "y_test_chatbot = []\n",
    "for c in tqdm(range(len(characters))):\n",
    "    # Load the preprocessed dataset\n",
    "    lines = doc_test[characters[c]][0]\n",
    "    series_df = {\n",
    "        'character': [1 for _ in range(len(lines))], \n",
    "        'line': lines}\n",
    "    series_df = pd.DataFrame.from_dict(series_df)\n",
    "\n",
    "    #print(\"Loaded encoded lines from \" + source_encoded_path + '/' + characters[c])\n",
    "    tmp_df = get_triplet_df(series_df, n_shuffles=10, random_state=random_state, n=n_merged_sentences_x_sample)\n",
    "    y_test_chatbot += [c for _ in range(len(tmp_df))]\n",
    "    X_test_chatbot += tmp_df['line'].tolist()\n",
    "\n",
    "X_test_chatbot, y_test_chatbot = shuffle(X_test_chatbot, y_test_chatbot, random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_chat_embeddings = model.encode(X_test_chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_kmeans_chat = kmeans.predict(test_chat_embeddings)\n",
    "y_pred_kmeans_chat = [cluster_to_char[y] for y in y_pred_kmeans_chat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test_chatbot, \n",
    "    y_pred_kmeans_chat, \n",
    "    normalize='true',\n",
    "    display_labels=characters)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('shutdown -h')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "451812313a2cc9ef7b1a116a2be532c610a0f65ac693e04b1a4edd064a67cb06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

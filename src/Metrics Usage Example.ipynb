{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will present examples on how to use library BBMetric for evaluate our chatbots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pip install -r \"E:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBotGit\\BarneyBot\\requirements.txt\"\n"
     ]
    }
   ],
   "source": [
    "### Run environment setup\n",
    "import os\n",
    "import lib.BBSetup as BBSetup\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    BBSetup.colab_setup(mount_folder=r\"/content/drive/My Drive/unibo/NLP_project/BarneyBot\")\n",
    "except:\n",
    "    BBSetup.anaconda_setup(base_folder=r\"E:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBotGit\\BarneyBot\",\n",
    "                           env_name=\"barneybot\")\n",
    "\n",
    "### Define folders\n",
    "base_folder = BBSetup.BASE_FOLDER\n",
    "out_folder = BBSetup.set_folder(os.path.join(base_folder, 'Metrics', 'New'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tonel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import the metrics library\n",
    "from lib.BBMetrics import BBMetric \n",
    "\n",
    "# Create some basic sentences to feed to the metrics\n",
    "sentences_basic = [\"Hi!\", \"How are you?\", \"I hate you.\"]\n",
    "sentences_basic_2 = [\"Hello!\", \"How are you doing?\", \"I think this is good.\"]\n",
    "sentences_vader = [\"Come to the dark side!\", \"I will kill you!\", \"Luke, I am your father.\"]\n",
    "sentences_barney = [\"Did you get the suit?\", \"Legendary!\", \"I like girls.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty print module\n",
    "import pprint\n",
    "from lib.BBMetricResults import *\n",
    "\n",
    "printer = pprint.PrettyPrinter(depth=4, width=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BBMetric.metrics_list` show up the list of all the available metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['google bleu',\n",
       " 'mpnet embedding similarity',\n",
       " 'rouge l',\n",
       " 'meteor',\n",
       " 'emotion classifier',\n",
       " 'roberta crossencoding similarity',\n",
       " 'distinct',\n",
       " 'neural chatbot classifier',\n",
       " 'perplexity',\n",
       " 'repetitiveness',\n",
       " 'term error rate',\n",
       " 'bertscore',\n",
       " 'comet',\n",
       " 'bleurt',\n",
       " 'word mover distance',\n",
       " 'bartscore',\n",
       " 'extended edit distance',\n",
       " 't5 grammar correction edit distance']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the available metrics list\n",
    "BBMetric.metrics_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing a metric shows its info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'google bleu', 'args': {'train': {'required': set(), 'optional': set()}, 'compute': {'required': {'references', 'predictions'}, 'optional': set()}}, 'returns': ['score', 'std'], 'description': None, 'paper': None, 'save_actors': ['predictor', 'reference']}\n"
     ]
    }
   ],
   "source": [
    "# Display info for a loaded metric\n",
    "metric = BBMetric.load_metric(\"google bleu\")\n",
    "print(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BBMetric.load_metric(metric_name)` load the specified metric with name `metric_name` by loading the respective model or algorithm which computes it. It will return the `metric` asked ready to be compute by invoking `metric.compute`. Some metrics (such as the human ones and the semantic classifier) require training, in which case a method `metric.train` is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.3148148148148148, 'std': 0.15930231976004866}\n"
     ]
    }
   ],
   "source": [
    "# Google BLEU (Variation of BLEU more useful for sentences) on a pair of sets of sentences\n",
    "metric = BBMetric.load_metric(\"google bleu\")\n",
    "\n",
    "print(metric.compute(predictions=sentences_basic, references=sentences_basic_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/14/2022 15:47:42 - INFO - happytransformer.happy_transformer -   Using model: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.3076923076923077, 'std': 0.3076923076923077}\n"
     ]
    }
   ],
   "source": [
    "# Grammar Correction Distance on of sentences (based on a neural corrector plus a normalized edit distance)\n",
    "metric = BBMetric.load_metric(\"t5 grammar correction edit distance\")\n",
    "\n",
    "print(metric.compute(sentences=[\"this wrong is\", \"This is correct.\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.5964818000793457, 'std': 0.4743334650993347}\n"
     ]
    }
   ],
   "source": [
    "# Symmetric Semantic Similarity on a pair of sets of sentences\n",
    "metric = BBMetric.load_metric(\"mpnet embedding similarity\")\n",
    "\n",
    "print(metric.compute(sentences_a=sentences_basic, sentences_b=sentences_basic_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.36904761904761907, 'std': 0.3599099156626422}\n"
     ]
    }
   ],
   "source": [
    "# Rouge-L on a pair of sets of sentences\n",
    "metric = BBMetric.load_metric(\"rouge l\")\n",
    "\n",
    "print(metric.compute(predictions=sentences_basic, references=sentences_basic_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tonel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tonel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\tonel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9739583333333334, 'std': 0.025779934730759544}\n"
     ]
    }
   ],
   "source": [
    "# METEOR on a pair of sets of sentences\n",
    "metric = BBMetric.load_metric(\"meteor\")\n",
    "\n",
    "print(metric.compute(predictions=sentences_basic, references=sentences_basic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Programs\\Anaconda\\envs\\barneybot\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:89: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': [0.040451311971992254, 0.3372651059180498, 0.0295343438629061, 0.33119263251622516, 0.24915530377378067, 0.012401272969630858], 'std': [0.02466328219141657, 0.3520862192023358, 0.03524458751490872, 0.3607271122098797, 0.31209391813628645, 0.008284036892139822], 'label': ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']}\n"
     ]
    }
   ],
   "source": [
    "# Emotion labeling on a set of sentences\n",
    "metric = BBMetric.load_metric(\"emotion classifier\")\n",
    "\n",
    "'''\n",
    "for char in ['Barney', 'Bender']:\n",
    "    char_hg = load_char_df(char, base_folder)\n",
    "    result = metric.compute(sentences=char_hg['test']['response'])\n",
    "    print(char + \"Emotions\")\n",
    "    printer.pprint({result['label'][i]: result['score'][i] for i in range(len(result['score']))})\n",
    "'''\n",
    "\n",
    "print(metric.compute(sentences=sentences_basic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.6090299487113953, 'std': 0.4026722013950348}\n"
     ]
    }
   ],
   "source": [
    "# Semantic Answer Similarity on a pair of sets of sentences\n",
    "metric = BBMetric.load_metric(\"roberta crossencoding similarity\")\n",
    "\n",
    "print(metric.compute(predictions=sentences_basic, references=sentences_basic_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.1272727272727273, 'std': 0.09030099651970509}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distinct on a set of sentences\n",
    "metric = BBMetric.load_metric(\"distinct\")\n",
    "\n",
    "# ngram_size is optional, defaults to 3\n",
    "metric.compute(sentences=sentences_basic, ngram_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.04097739979624748, 'std': 0.04034863039851189}\n",
      "{'score': 0.641024649143219, 'std': 0.2366839498281479}\n",
      "{'score': 0.10722770541906357, 'std': 0.08116302639245987}\n"
     ]
    }
   ],
   "source": [
    "# Semantic Classifier on a set of sentences\n",
    "metric = BBMetric.load_metric(\"neural chatbot classifier\")\n",
    "\n",
    "from lib.BBData import character_dict\n",
    "\n",
    "# n_shuffles is optional, defaults to 10\n",
    "# Either specify a source_encoded_path to load encoded lines, or specify a source_path and a source_save_path to create them\n",
    "# shutdown_at_end is optional, defaults to False\n",
    "metric.train(character='Vader', random_state=random_state,\n",
    "             source_encoded_path=None,\n",
    "             source_path=os.path.join(base_folder, \"data\", \"Sources\", character_dict[\"Vader\"]['source'], character_dict[\"Vader\"]['source'] + \".csv\"),\n",
    "             source_save_path=os.path.join(base_folder, \"data\", \"Characters\", 'Vader'),\n",
    "             save_path=os.path.join(base_folder, \"data\", \"Characters\", 'Vader'),\n",
    "             n_shuffles=10, shutdown_at_end=False)\n",
    "\n",
    "# Computations for Barney semantic classifier on different sets of sentences\n",
    "print(metric.compute(character='Vader',\n",
    "                     load_path=os.path.join(base_folder, \"Data\", \"Characters\", 'Vader', character_dict[\"Vader\"]['classifier_folder']),\n",
    "                     sentences=sentences_basic))\n",
    "print(metric.compute(character='Vader',\n",
    "                     load_path=os.path.join(base_folder, \"Data\", \"Characters\", 'Vader', character_dict[\"Vader\"]['classifier_folder']),\n",
    "                     sentences=sentences_vader))\n",
    "print(metric.compute(character='Vader',\n",
    "                     load_path=os.path.join(base_folder, \"Data\", \"Characters\", 'Vader', character_dict[\"Vader\"]['classifier_folder']),\n",
    "                     sentences=sentences_barney))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d93bf9405ee4fe996b74cf156b83e88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82cd68f178394805b1c7e30af3723f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdbc9e36c9b746bbb8abf15c3b967400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at E:\\University\\Esami da Superare\\Natural Language Processing\\BarneyBotGit\\BarneyBot\\Data\\Characters\\Vader\\vader_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n",
      "Using custom data configuration default-3e37c23a51e9d556\n",
      "Found cached dataset csv (E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-3e37c23a51e9d556/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2feb93973c5a43c883ae400924095c4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-3e37c23a51e9d556/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-6affe80547fc0f38.arrow and E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-3e37c23a51e9d556/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-6800bd34204a2976.arrow\n",
      "Loading cached split indices for dataset at E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-3e37c23a51e9d556/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-78893b2d23cae2d7.arrow and E:/University/Esami da Superare/Natural Language Processing/BarneyBotGit/BarneyBot/Src/cache/csv/default-3e37c23a51e9d556/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-38619cd96763b020.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc545f01a105456a971ca101f4b77028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/136 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb08b050fc2f485e91d570cb8173e0d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "545f90801d1b4df69f0cae4d1835f002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:34<00:00, 17.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 20.613982847195594}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Perplexity on an encoded test set (taken from one of our datasets)\n",
    "metric = BBMetric.load_metric(\"perplexity\")\n",
    "\n",
    "# Functions to load a dataset and prepare it, used for perplexity\n",
    "from transformers import TFAutoModelForCausalLM, AutoTokenizer\n",
    "from lib.BBData import character_dict, source_dict, random_state, model_name\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from lib.BBDataLoad import load_char_df, dialogpt_preprocess_function\n",
    "\n",
    "# Get dialogpt tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=os.path.join(base_folder, \"cache\"))\n",
    "tokenizer.pad_token = '#'\n",
    "\n",
    "# Select a batch size, used for perplexity\n",
    "batch_size = 8\n",
    "\n",
    "# Load the Vader dialogpt finetuned model\n",
    "model_vader = TFAutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=\\\n",
    "                    os.path.join(base_folder, 'Data', 'Characters', 'Vader', character_dict['Vader']['checkpoint_folder']))\n",
    "model_vader.compile()\n",
    "data_collator = DataCollatorForLanguageModeling(mlm=False, tokenizer=tokenizer, return_tensors='tf')\n",
    "\n",
    "# Load the Barney dataset and process it as a conversation\n",
    "vader_hg = load_char_df('Vader', base_folder)\n",
    "tokenized_vader_hg = vader_hg.map(lambda row: dialogpt_preprocess_function(row, tokenizer), batched=False)\n",
    "# Transform the HuggingFace dataset as a tensorflow one, ready to be fed to the model\n",
    "vader_test_set = tokenized_vader_hg[\"test\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "print(metric.compute(model=model_vader, encoded_test_set=vader_test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.2333333333333333, 'std': 0.09428090415820632}\n"
     ]
    }
   ],
   "source": [
    "# Repetitiveness of sentences\n",
    "metric = BBMetric.load_metric(\"repetitiveness\")\n",
    "\n",
    "print(metric.compute(sentences=sentences_basic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 76.66666666666667, 'std': 20.548046676563256}\n"
     ]
    }
   ],
   "source": [
    "# TER on a pair of sets of sentences\n",
    "metric = BBMetric.load_metric(\"term error rate\")\n",
    "\n",
    "print(metric.compute(predictions=sentences_basic, references=sentences_basic_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.8650024731953939, 'std': 0.09010275421675674}\n"
     ]
    }
   ],
   "source": [
    "# BERTScore on a pair of sets of sentences\n",
    "metric = BBMetric.load_metric(\"bertscore\")\n",
    "\n",
    "print(metric.compute(predictions=sentences_basic, references=sentences_basic_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eamt22-cometinho-da is already in cache.\n",
      "Path lightning_logs/cometinho_part-i/checkpoints/epoch=0-step=899999.ckpt does not exist!\n",
      "E:\\Programs\\Anaconda\\envs\\barneybot\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': -1.1263877749443054, 'std': 0.19313412800715013}\n"
     ]
    }
   ],
   "source": [
    "# COMET on a triple of sets of sentences\n",
    "metric = BBMetric.load_metric(\"comet\")\n",
    "\n",
    "print(metric.compute(sources=sentences_basic, predictions=sentences_barney, references=sentences_basic_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default BLEURT-Base checkpoint for sequence maximum length 128. You can use a bigger model for better results with e.g.: evaluate.load('bleurt', 'bleurt-large-512').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint C:\\Users\\tonel\\.cache\\huggingface\\metrics\\bleurt\\default\\downloads\\extracted\\b094b72f3dc7e1712a641ab624024c3b182ff714848ee334f1cc7a628d0b7798\\bleurt-base-128.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading checkpoint C:\\Users\\tonel\\.cache\\huggingface\\metrics\\bleurt\\default\\downloads\\extracted\\b094b72f3dc7e1712a641ab624024c3b182ff714848ee334f1cc7a628d0b7798\\bleurt-base-128.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config file found, reading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint bert_custom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will load checkpoint bert_custom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:bert_custom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "... name:bert_custom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "... vocab_file:vocab.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "... do_lower_case:True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "... max_seq_length:128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating WordPiece tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating WordPiece tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WordPiece tokenizer instantiated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BLEURT initialized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': -1.504532257715861, 'std': 0.18247956505028967}\n"
     ]
    }
   ],
   "source": [
    "# BLEURT on a pair of sets of sentences\n",
    "metric = BBMetric.load_metric(\"bleurt\")\n",
    "\n",
    "print(metric.compute(predictions=sentences_barney, references=sentences_basic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "adding document #0 to Dictionary(0 unique tokens: [])\n",
      "built Dictionary(2 unique tokens: ['come', 'dark']) from 2 documents (total 4 corpus positions)\n",
      "Dictionary lifecycle event {'msg': \"built Dictionary(2 unique tokens: ['come', 'dark']) from 2 documents (total 4 corpus positions)\", 'datetime': '2022-11-14T13:08:04.326902', 'gensim': '4.1.2', 'python': '3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "adding document #0 to Dictionary(0 unique tokens: [])\n",
      "built Dictionary(1 unique tokens: ['kill']) from 2 documents (total 2 corpus positions)\n",
      "Dictionary lifecycle event {'msg': \"built Dictionary(1 unique tokens: ['kill']) from 2 documents (total 2 corpus positions)\", 'datetime': '2022-11-14T13:08:04.537950', 'gensim': '4.1.2', 'python': '3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "Removed 2 and 2 OOV words from document 1 and 2 (respectively).\n",
      "At least one of the documents had no words that were in the vocabulary.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.0, 'std': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# Word Mover Distance on a pair of sets of sentences\n",
    "metric = BBMetric.load_metric(\"word mover distance\")\n",
    "\n",
    "print(metric.compute(predictions=sentences_vader, references=sentences_vader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': -4.447240034739177}\n"
     ]
    }
   ],
   "source": [
    "# BARTScore on a pair of sets of sentences\n",
    "metric = BBMetric.load_metric(\"bartscore\")\n",
    "\n",
    "print(metric.compute(predictions=sentences_barney, references=sentences_basic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.8360909024874369, 'std': 0.05260148578323149}\n"
     ]
    }
   ],
   "source": [
    "# EED on a pair of sets of sentences\n",
    "metric = BBMetric.load_metric(\"extended edit distance\")\n",
    "\n",
    "print(metric.compute(predictions=sentences_barney, references=sentences_basic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving & Loading Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric metadata creation\n",
    "metric_name = 'dummy metric'\n",
    "metric_name_pretty = 'Dummy Metric'\n",
    "metric_version = 1\n",
    "metric_actors = {\n",
    "    \"document\": [\n",
    "        MetricActor.DATASET_CHAR,\n",
    "        \"Barney\"\n",
    "    ],\n",
    "    \"training_set\": [\n",
    "        MetricActor.DATASET_CHAR,\n",
    "        \"Barney\"\n",
    "    ]\n",
    "}\n",
    "metric_result = {\n",
    "    \"score\": 0.9984034299850464,\n",
    "    \"std\": 0.027748608961701393\n",
    "}\n",
    "metric_attempt = 0\n",
    "metric_context = {                          \n",
    "    \"dialogpt_size\": \"small\",\n",
    "    \"dialogpt_context_sentences\": 5,\n",
    "    \"dialogpt_nbeams_beams\": 3,\n",
    "    \"dialogpt_sample_top_p\": 0.92,\n",
    "    \"dialogpt_sample_top_k\": 50\n",
    "}\n",
    "metric_params = {}\n",
    "metric_samples = 'Unknown'\n",
    "metric_hash = dict_hash({'metric_name': metric_name,\n",
    "                                         'metric_version': metric_version,\n",
    "                                         'metric_attempt': metric_attempt,\n",
    "                                         'metric_actors': metric_actors,\n",
    "                                         'context': metric_context,\n",
    "                                         'metric_params': metric_params,\n",
    "                                         'metric_samples': metric_samples})\n",
    "\n",
    "# This is the important one. Each metric should contain all these entries\n",
    "metric_dict = {\n",
    "        \"metric_name\": metric_name,           # Unique name of the metric\n",
    "        \"metric_version\": metric_version,     # Metric version (useful if you change how a metric works and recompute)\n",
    "        \"metric_attempt\": metric_attempt,     # Incremental value for multiple computations of the same metric (e.g. for std)\n",
    "        \"metric_actors\": metric_actors,       # Who this metric is computed on\n",
    "        \"metric_dependency\": get_metric_dependency(metric_name, metric_actors), # Is this metric a function of data or chatbot?\n",
    "        \"metric_params\": metric_params,       # Additional params of the metric (e.g. ngram_size for distinct)\n",
    "        \"context\": metric_context,            # External parameters, such as chatbot characteristics\n",
    "        \"metric_arity\": get_metric_arity(metric_name), # Metric arity\n",
    "        \"metric_samples\": metric_samples,     # Batch size of the metric\n",
    "        \"metric_determinism\": get_metric_determinism(metric_name, metric_version), # Is this metric algorithmic or not?\n",
    "        \"answer\": metric_result,              # Score of the metric, may include std (Any dictionary can go here)\n",
    "        \"hash\": metric_hash                   # Unique hash for this metric, used to not store duplicates\n",
    "    }\n",
    "\n",
    "metric_dict = {\n",
    "    metric_hash: metric_dict\n",
    "}\n",
    "printer.pprint(metric_dict) # Metric is now ready to be saved!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics\n",
    "save_metric_by_name(out_folder, 'Dummy Metric', metric_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metrics\n",
    "metric_dict = load_metric_by_name(out_folder, 'Dummy Metric')\n",
    "printer.pprint(metric_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "451812313a2cc9ef7b1a116a2be532c610a0f65ac693e04b1a4edd064a67cb06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

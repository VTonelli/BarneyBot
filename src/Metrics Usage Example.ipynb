{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will present examples on how to use library BBMetric for evaluate our chatbots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various imports to load the metric library, the model, the tokenizer, and the characters data\n",
    "from metrics import BBMetric\n",
    "from transformers import TFAutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "from data_utils import character_dict, source_dict, random_state\n",
    "from transformers import AdamWeightDecay\n",
    "import os\n",
    "\n",
    "# Mount google drive, if in Colaboratory environment\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive',force_remount=True)\n",
    "    base_folder = '/content/drive/My Drive/unibo/NLP_project/BarneyBot'\n",
    "    os.system(\"pip install datasets\")\n",
    "    os.system(\"pip install transformers\")\n",
    "    os.system(\"pip install rouge_score\")\n",
    "    os.system(\"pip install -U sentence-transformers\")\n",
    "else:\n",
    "    # base_folder = os.getcwd()\n",
    "    base_folder = '..'\n",
    "\n",
    "out_folder = os.path.join(base_folder, 'Data', 'Characters')\n",
    "# Get Barney character folder, as an example\n",
    "barney_folder = os.path.join(out_folder, 'Barney')\n",
    "if not os.path.exists(barney_folder):\n",
    "    os.makedirs(barney_folder)\n",
    "    \n",
    "# Create some basic sentences to feed to the metrics\n",
    "sentences_basic = [\"Hi!\", \"How are you?\", \"I hate you.\"]\n",
    "sentences_basic_2 = [\"Hello!\", \"How are you doing?\", \"I think this is good.\"]\n",
    "sentences_vader = [\"Come to the dark side!\", \"I will kill you!\", \"Luke, I am your father.\"]\n",
    "sentences_barney = [\"Did you get the suit?\", \"Legendary!\", \"I like girls.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below a model from the checkpoint `microsoft/DialoGPT-small` is loaded with its corresponding tokenizer and setting the padding token to `#`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and compile dialogpt defaul model\n",
    "model = TFAutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-small', cache_dir=os.path.join(os.getcwd(), \"cache\"))\n",
    "model.compile(optimizer=AdamWeightDecay(learning_rate=2e-5))\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small', cache_dir=os.path.join(os.getcwd(), \"cache\"))\n",
    "tokenizer.pad_token = '#'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell the Barney model is loaded and a conversation is preprocessed. Then the HuggingFace dataset is transformod into a tensorflow one, ready to be fed to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to load a dataset and prepare it, used for perplexity\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from data_utils import load_df, construct_conv, preprocess_function\n",
    "\n",
    "# Select a batch size, used for perplexity\n",
    "batch_size = 8\n",
    "\n",
    "# Load the Barney dialogpt finetuned model\n",
    "model_barney = TFAutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=\\\n",
    "                                            os.path.join(barney_folder, character_dict['Barney']['checkpoint_folder']))\n",
    "model_barney.compile(optimizer=AdamWeightDecay(learning_rate=2e-5))\n",
    "data_collator = DataCollatorForLanguageModeling(mlm=False, tokenizer=tokenizer, return_tensors='tf')\n",
    "\n",
    "# Load the Barney dataset and process it as a conversation\n",
    "barney_hg = load_df('Barney', base_folder)\n",
    "tokenized_barney_hg = barney_hg.map(preprocess_function, batched=False)\n",
    "# Transform the HuggingFace dataset as a tensorflow one, ready to be fed to the model\n",
    "barney_test_set = tokenized_barney_hg[\"test\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BBMetric.metrics_list` show up the list of all the available metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the available metrics list\n",
    "BBMetric.metrics_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BBMetric.load_metric(metric_name)` load the specified metric with name `metric_name` by loading the respective model or algorithm which computes it. It will return the `metric` asked ready to be compute by invoking `metric.compute`. Some metrics (such as the human ones and the semantic classifier) require training, in which case a method `metric.train` is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU on a pair of sets of sentences\n",
    "metric = BBMetric.load_metric(\"bleu\")\n",
    "\n",
    "print(metric.compute(predictions=sentences_basic, references=sentences_basic_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rouge-L on a pair of sets of sentences\n",
    "metric = BBMetric.load_metric(\"rouge l\")\n",
    "\n",
    "print(metric.compute(predictions=sentences_basic, references=sentences_basic_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distinct on a set of sentences\n",
    "metric = BBMetric.load_metric(\"distinct\")\n",
    "\n",
    "# ngram_size is optional, defaults to 3\n",
    "metric.compute(sentences=sentences_basic, ngram_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion labeling on a set of sentences\n",
    "metric = BBMetric.load_metric(\"emotion\")\n",
    "\n",
    "print(metric.compute(sentences=sentences_basic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic Similarity (similar to BERTScore) on a pair of sets of sentences\n",
    "metric = BBMetric.load_metric(\"semantic similarity\")\n",
    "\n",
    "print(metric.compute(sentences_a=sentences_basic, sentences_b=sentences_basic_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic Answer Similarity on a pair of sets of sentences\n",
    "metric = BBMetric.load_metric(\"semantic answer similarity\")\n",
    "\n",
    "print(metric.compute(predictions=sentences_basic, references=sentences_basic_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic Classifier on a set of sentences\n",
    "# Note: This Metric is not autonomous from project folder structure and datatypes! #\n",
    "metric = BBMetric.load_metric(\"semantic classifier\")\n",
    "\n",
    "# n_shuffles is optional, defaults to 10\n",
    "# from_saved_embeddings is optional, defaults to True\n",
    "# shutdown_at_end is optional, defaults to False\n",
    "metric.train(character='Barney', character_dict=character_dict, source_dict=source_dict, random_state=random_state,\n",
    "             base_folder=base_folder, n_shuffles=10, from_saved_embeddings=True, shutdown_at_end=False)\n",
    "\n",
    "# Computations for Barney semantic classifier on different sets of sentences\n",
    "print(metric.compute(character='Barney', character_dict=character_dict, base_folder=base_folder,\n",
    "               sentences=sentences_basic))\n",
    "print(metric.compute(character='Barney', character_dict=character_dict, base_folder=base_folder,\n",
    "               sentences=sentences_vader))\n",
    "print(metric.compute(character='Barney', character_dict=character_dict, base_folder=base_folder,\n",
    "               sentences=sentences_barney))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perplexity on an encoded test set (taken from one of our datasets)\n",
    "metric = BBMetric.load_metric(\"perplexity\")\n",
    "\n",
    "print(metric.compute(model=model_barney, encoded_test_set=barney_test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human - coherence metric\n",
    "metric = BBMetric.load_metric(\"human - coherence\")\n",
    "\n",
    "# Ask a human to perform evaluation\n",
    "# length is optional, defaults to 5\n",
    "metric.train(model=model, tokenizer=tokenizer,\n",
    "             filepath=os.path.join(os.getcwd(), \"Data\", \"Characters\", \"Default\", \"humancoherence.csv\"),\n",
    "             length=5)\n",
    "\n",
    "# Print score averages\n",
    "metric.compute(filepath=os.path.join(os.getcwd(), \"Data\", \"Characters\", \"Default\", \"humancoherence.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human - consistency metric\n",
    "metric = BBMetric.load_metric(\"human - consistency\")\n",
    "\n",
    "# Ask a human to perform evaluation\n",
    "metric.train(model=model, tokenizer=tokenizer,\n",
    "             filepath=os.path.join(os.getcwd(), \"Data\", \"Characters\", \"Default\", \"humanconsistency.csv\"))\n",
    "\n",
    "# Print score averages\n",
    "metric.compute(filepath=os.path.join(os.getcwd(), \"Data\", \"Characters\", \"Default\", \"humanconsistency.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Human - style metric\n",
    "metric = BBMetric.load_metric(\"human - style\")\n",
    "\n",
    "# Ask a human to perform evaluation\n",
    "metric.train(model=model, tokenizer=tokenizer,\n",
    "             filepath=os.path.join('..', \"Data\", \"Characters\", \"Default\", \"humanstyle.csv\"),\n",
    "             questions=barney_sentences)\n",
    "\n",
    "# Print score averages\n",
    "metric.compute(filepath=os.path.join('..', \"Data\", \"Characters\", \"Default\", \"humanstyle.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "451812313a2cc9ef7b1a116a2be532c610a0f65ac693e04b1a4edd064a67cb06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

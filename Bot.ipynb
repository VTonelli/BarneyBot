{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the dataset documents and store their data into a DataFrame\n",
    "def load_himym_dataset():\n",
    "    episodes_folder = os.path.join(os.getcwd(), \"Datasets\", \"Sources\", \"HIMYM\", \"Episodes\")\n",
    "    dataframe_rows = []\n",
    "    # Get number of documents and their names\n",
    "    documents_n = len(os.listdir(episodes_folder))\n",
    "    documents_names = os.listdir(episodes_folder)\n",
    "\n",
    "    # Loop over documents\n",
    "    for i in tqdm(range(documents_n)):\n",
    "        filename = documents_names[i]\n",
    "        # Open document\n",
    "        file = open(os.path.join(episodes_folder, filename))\n",
    "        episode_index = filename[:-4]\n",
    "        # Loop over lines (= words)\n",
    "        for line in file.readlines():\n",
    "                dataframe_row = {\n",
    "                    \"episode\": episode_index,\n",
    "                    \"line\": line,\n",
    "                }\n",
    "                dataframe_rows.append(dataframe_row)\n",
    "    # Build the dataframe from the words\n",
    "    df = pd.DataFrame(dataframe_rows)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 139/139 [00:00<00:00, 149.85it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "episode    39284\n",
       "line       39284\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute creation of dataset\n",
    "himym_df = load_himym_dataset()\n",
    "himym_df.head()\n",
    "himym_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30286\n"
     ]
    }
   ],
   "source": [
    "def process_himym_dataset(df):\n",
    "    df = df[~df['line'].str.startswith(\"[\")]\n",
    "    df = df[~df['line'].str.startswith(\"(\")]\n",
    "    df['line'] = df['line'].str.strip()\n",
    "    df['line'] = df['line'].str.replace(r\"\\(.*\\)\",\"\")\n",
    "    df[['character', 'line']] = df['line'].str.split(\":\", 1, expand=True)\n",
    "    df = df[~df['line'].isnull()]\n",
    "    df = df[~df['character'].isnull()]\n",
    "    df = df[~df['line'].str.isspace()]\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "    \n",
    "himym_df = process_himym_dataset(himym_df)\n",
    "print(len(himym_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>line</th>\n",
       "      <th>character</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01x01</td>\n",
       "      <td>Kids, I'm going to tell you an incredible sto...</td>\n",
       "      <td>Narrator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01x01</td>\n",
       "      <td>Are we being punished for something?</td>\n",
       "      <td>Son</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01x01</td>\n",
       "      <td>No</td>\n",
       "      <td>Narrator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01x01</td>\n",
       "      <td>Yeah, is this going to take a while?</td>\n",
       "      <td>Daughter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01x01</td>\n",
       "      <td>Yes.  Twenty-five years ago, before I was dad...</td>\n",
       "      <td>Narrator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>01x01</td>\n",
       "      <td>It was way back in 2005. I was twenty-seven j...</td>\n",
       "      <td>Narrator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>01x01</td>\n",
       "      <td>Will you marry me.</td>\n",
       "      <td>Marshall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>01x01</td>\n",
       "      <td>Yes, perfect! And then you're engaged, you po...</td>\n",
       "      <td>Ted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>01x01</td>\n",
       "      <td>Got it. Thanks for helping me plan this out, ...</td>\n",
       "      <td>Marshall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>01x01</td>\n",
       "      <td>Dude, are you kidding? It's you and Lily! I'v...</td>\n",
       "      <td>Ted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>01x01</td>\n",
       "      <td>yeah, sorry. We thought you were asleep.</td>\n",
       "      <td>Marshall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>01x01</td>\n",
       "      <td>It's physics Marshall, if the bottom bunk mov...</td>\n",
       "      <td>Ted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>01x01</td>\n",
       "      <td>Yeah, what are you doing tonight?</td>\n",
       "      <td>Marshall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>01x01</td>\n",
       "      <td>What was I doing? Your Uncle Marshall was tak...</td>\n",
       "      <td>Narrator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>01x01</td>\n",
       "      <td>hey, so you know how I've always had a thing...</td>\n",
       "      <td>Barney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>01x01</td>\n",
       "      <td>Hey, you wanna do something tonight?</td>\n",
       "      <td>Ted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>01x01</td>\n",
       "      <td>Okay, meet me at the bar in fifteen minutes, ...</td>\n",
       "      <td>Barney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>01x01</td>\n",
       "      <td>Hey.</td>\n",
       "      <td>Ted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>01x01</td>\n",
       "      <td>Where's your suit!? Just once when I say suit...</td>\n",
       "      <td>Barney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>01x01</td>\n",
       "      <td>I did that one time.</td>\n",
       "      <td>Ted</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode                                               line character\n",
       "0    01x01   Kids, I'm going to tell you an incredible sto...  Narrator\n",
       "1    01x01               Are we being punished for something?       Son\n",
       "2    01x01                                                 No  Narrator\n",
       "3    01x01               Yeah, is this going to take a while?  Daughter\n",
       "4    01x01   Yes.  Twenty-five years ago, before I was dad...  Narrator\n",
       "5    01x01   It was way back in 2005. I was twenty-seven j...  Narrator\n",
       "6    01x01                                 Will you marry me.  Marshall\n",
       "7    01x01   Yes, perfect! And then you're engaged, you po...       Ted\n",
       "8    01x01   Got it. Thanks for helping me plan this out, ...  Marshall\n",
       "9    01x01   Dude, are you kidding? It's you and Lily! I'v...       Ted\n",
       "10   01x01           yeah, sorry. We thought you were asleep.  Marshall\n",
       "11   01x01   It's physics Marshall, if the bottom bunk mov...       Ted\n",
       "12   01x01                  Yeah, what are you doing tonight?  Marshall\n",
       "13   01x01   What was I doing? Your Uncle Marshall was tak...  Narrator\n",
       "14   01x01    hey, so you know how I've always had a thing...    Barney\n",
       "15   01x01               Hey, you wanna do something tonight?       Ted\n",
       "16   01x01   Okay, meet me at the bar in fifteen minutes, ...    Barney\n",
       "17   01x01                                               Hey.       Ted\n",
       "18   01x01   Where's your suit!? Just once when I say suit...    Barney\n",
       "19   01x01                               I did that one time.       Ted"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "himym_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Narrator', 'Son', 'Daughter', 'Marshall', 'Ted', 'Barney',\n",
       "       'Yasmine', 'Lily', 'Robin', 'Cabdriver', \"Robin's Dumped Friend\",\n",
       "       'Producer', 'Waitor', 'Ranjit', 'Lily, Marshall and Barney',\n",
       "       'Son and Daughter', 'Rangit', 'Marshal', 'Carl', 'Cameraman',\n",
       "       'Leroy', 'Lily and Marshall', 'Fantasy Girl', 'Tatiana',\n",
       "       'Lily and Ted', 'Crowd', 'Carlos', 'Barney and Ted',\n",
       "       'Marshall, Lily and Ted', 'Mashall, Lily and Ted', 'Guy#1',\n",
       "       'Laura', 'Fight Attendant', 'Guy#2', 'Guy#3', 'Officer McNeil',\n",
       "       'b*mb Squad Guy', 'Derrick', 'Dana', 'Sascha', 'Cabdriver#2',\n",
       "       'Cute Girl', 'Stefanie', 'Marshall and Ted', 'Mr. Adams',\n",
       "       'Natalie', 'One Guest', 'All', 'Henry', 'Waiter', 'Claire',\n",
       "       'Bradley', 'Chris', 'Austin', 'Kelly', 'Bartender', 'Phil',\n",
       "       'Man on Street', 'Doorman#2', 'Woman', 'Coat Check Girl',\n",
       "       'Barney, Ted and Robin', 'Future Ted', 'Lily ', 'Barney ',\n",
       "       'Marshall, Lily, Barney', 'Lily, Marshall, Barney', 'Mike',\n",
       "       'Marshall ', 'King Costume Guy', 'Shagarats', 'Lily, Robin',\n",
       "       'Hula Girl', 'Angel Guy', 'Future Ted VO', 'Hula girl', 'Ellen',\n",
       "       'Sarah', 'Ted ', 'Sarah ', 'Sudeep', 'Lily, Marshall',\n",
       "       'Mr. Madsen', 'Waitress', 'Katie', 'Delivery guy', 'Girl #2',\n",
       "       'Kevin', 'Jackie', 'Doctor', 'Marshall, Ted', 'Everyone',\n",
       "       'Robin, Ted', 'Mr. Ericksen', 'Ericksens', 'Marcus',\n",
       "       'Mrs. Ericksen', 'Kendall', 'Pregnant Mrs. Ericksen', 'Marvin',\n",
       "       'Amanda', 'Clerk', 'Judy', 'Pete', 'Walter', 'Dancer',\n",
       "       'Daughter, Son', 'Marshall, Barney', 'Marshall, Lily', 'Guy',\n",
       "       'Trudy', \"Trudy's friend\", 'Ted from 2030', 'Mary Beth', 'Natalya',\n",
       "       'Moby', 'Not-Moby', 'Ted and Robin', 'Derek', 'Natalia', 'Claudia',\n",
       "       'Ted, Marshall', 'Stuart', 'Tanya', 'Victoria', 'Nirvana',\n",
       "       'Shannon', 'Man', 'Robin and Lily', 'Victoria and Robin',\n",
       "       'Dr. Birnholz', 'Voice', 'Man #1', 'Man #2', 'Bilson', 'Blauman',\n",
       "       \"Victoria's voice\", 'Barney, singing', 'Little Girl', 'Little Boy',\n",
       "       'Director', 'Cab driver', 'Marshall &amp; Lily', 'Korean Elvis',\n",
       "       'Mary', 'Sandy', 'Vampire Lou', 'Lily, waking up', 'Todd',\n",
       "       'Scooter', 'Security guard', 'Boy', 'Singer', 'Boy #1', 'Boy #2',\n",
       "       'Andrew', 'Giant Turtle', 'Security guard #1', 'Security guard #2',\n",
       "       'Security guards', 'Paramedic', 'Tracy', 'Bob', 'Weather man',\n",
       "       'Musician', 'Voice mail', 'Penelope', 'Veterinary', 'Waiters',\n",
       "       \"Ted's daughter\", \"Ted's son\", 'Robin ', 'George Clinton',\n",
       "       'Stripper', 'Waiter/Joey Adalian',\n",
       "       'DAY FIFTY-SEVEN, at the apartment.Ted from 2030',\n",
       "       \"At MacLaren's.Ted from 2030\", 'In San Francisco, in a bus.Man 1',\n",
       "       'Man 2', 'Girl', 'Amy', 'In a building.Ted from 2030',\n",
       "       'In a snack-bar.Ted from 2030', 'Twin 1', 'Twin 2', \"Ted's mother\",\n",
       "       'The previous day, at the apartment.Ted from 2030', \"Ted's father\",\n",
       "       'The waitress ',\n",
       "       'At \"Casa a pezzi\". Barney is playing the piano.Ted\\'s father',\n",
       "       \"Ted's father \", \"Ted's mother \", \"Outside.Ted's father\",\n",
       "       \"Back inside, later.Ted's mother\", 'Both Marshall and Barney',\n",
       "       'A girl', 'Girl 1', 'Girls', 'Guy 1', 'Guy 2', 'Guy 2 ', 'Girl 2',\n",
       "       'Kara', 'Bouncer', 'Girl ', 'Anna',\n",
       "       \"Anna's apartment. Anna's reading the letter Barney left.Barney's voice\",\n",
       "       'Head waiter', 'Brad', \"At MacLaren's\", 'Mr Druthers ',\n",
       "       'Mr Druthers', 'Pr Lewis', 'Client', 'Chinese 1', 'Chinese 2',\n",
       "       'Woman 2', 'Judge', 'Captain', 'Both Marshall and Lily', 'Girl #1',\n",
       "       'Girl #3', 'Priest', 'Robin Sparkles', 'Robot',\n",
       "       'Marshall, lily, Robin', 'Tous', 'James', 'Barney and James',\n",
       "       'Mere', 'Tes ', 'Women', 'James and Barney', 'Charles', 'Rosa',\n",
       "       'Ted to Robin', 'His mother', 'Clint', 'Truck driver', 'A kid',\n",
       "       'Kid', 'Stacy', 'Charity', 'Both kids', 'Robin is sick and in bed',\n",
       "       'Kyle', 'fact number one', 'Sylvia', 'Brian', 'Ted  ', 'His kids',\n",
       "       'The girl', 'Molly', 'Mr. Druthers', 'Co-worker', 'MP',\n",
       "       'Employees', 'Mr Druters', 'Men ', 'The waitress', 'Lou', 'Sid',\n",
       "       'Doug', 'Emmitt Smith', 'Barman', 'Man 1', 'Trish Sanchez',\n",
       "       'Rubin', 'Old lady', 'Policeman', 'Ex-girlfriend 1',\n",
       "       'Ex-girlfriend 2', 'Ex-girlfriend 3', 'Brother 1', 'Brother 2',\n",
       "       'Ted and Marshall singing', 'Mechanic', 'Robibn', '.Barney',\n",
       "       'Maechanic', 'Brother',\n",
       "       'So, I can have the moving truck here by 8',\n",
       "       'Hey, hypothetical question', 'The category', 'Tes', 'Bareney',\n",
       "       \"Barney's mom\", 'Bareny ', 'Rich', 'At the apartment, all',\n",
       "       'At the apartment, Barney', 'At the apartment, Lily',\n",
       "       'At the apartment, Ted', 'At the apartement, Lily',\n",
       "       'At the apartment, Robin', 'Millie', 'At the apartment, Marshall',\n",
       "       'Andrea', 'Bill', 'Cousin', 'Ben', 'Marshall and Lily',\n",
       "       \"Lily's grandma\", 'Janna', 'Gael', 'The Girl', 'Tyler',\n",
       "       \"Amy's friend\", 'Narrtor', 'Colleen', 'Lindsay', 'A guy ',\n",
       "       'Collee', 'Robin #2', 'Barney, Marshall', 'Rachel',\n",
       "       'Rachel, Trudy', \"Robin's date\", 'Blonde Girl', 'Brunette Girl',\n",
       "       'Trudy, Rachel', 'Trucy', \"Robins' date\", 'Little Robin', 'George',\n",
       "       'Little Barney', 'Brooke', 'Stacy, Barney', 'Blah-blah',\n",
       "       'Marshall VO', 'Lily VO', 'Marshall, Lily VO', 'College Marshall',\n",
       "       'College Lily', 'College Guy', 'College Ted', 'Audrey',\n",
       "       'College students', 'Warrior Avatar', 'Female Avatar', 'Alexa',\n",
       "       'Wendy', 'Leonard', 'Jefferson', 'Jeff', 'Meg', 'NArrator',\n",
       "       'The estate agent', 'Boys', 'Marshall, Ted, Barney', 'The girl ',\n",
       "       'Bank employee', 'Lilly', 'Robiin', 'Phone Voice', 'ted',\n",
       "       'Wendy, upset, with a black eye', 'The others', 'Together',\n",
       "       'Now, months later, the results of that exam were scheduled to post online at 10',\n",
       "       \"It's true what they say\", 'Ted &amp; Robin',\n",
       "       'Marshall, on the phone with Barney',\n",
       "       'Barney, with a funny accent', 'Marhall', 'Lily and Marshall ',\n",
       "       'wendy', 'A man', 'A women', 'The host', 'Curt', 'Michael',\n",
       "       'Productor', 'Marshall&amp;Lily ',\n",
       "       \"In fairness, I did call Ted's butt at, like, 2\",\n",
       "       \"So on March 17, 2008, I went to a big St. Patrick's Day party. And it's a good thing I did, because, funny story\",\n",
       "       \"Okay, it's official\", \"There's three rules of cheating\",\n",
       "       'First skipped message',\n",
       "       'And then last night started coming back to me', 'Next message',\n",
       "       'Stella', 'Woman 1, to Ted', 'Woman 2, to Woman 1',\n",
       "       'Woman 1, to Woman 2', 'Woman 2, to Woman 3', 'Woman 3, to Stella',\n",
       "       'Stella, to Woman 3', 'Woman 3, to Woman 2', 'Abby',\n",
       "       'Abby, laughing', 'Stella, laughing', 'Cleaner',\n",
       "       'Woman, slapping him', 'Mystery woman', 'Marshal, Lily and Ted',\n",
       "       'Kate', 'Holly', 'Mark', 'Arthur', 'Ferguson',\n",
       "       'Barney, pouring some water in a glass', 'Simon',\n",
       "       'Robin, giggling', 'Michelle', 'Punchy', 'Robin, laughing',\n",
       "       'Benjamin', 'Frank', 'Randy', 'Pitt', 'Starry', 'Willie', 'All ',\n",
       "       'Lawrence', 'Dr Greer', 'Lily &amp; Marshall', \"It's 10\",\n",
       "       'God sent those lice to my head like he sent the locusts to Egypt',\n",
       "       'I know what he saw', 'Mark Johnson', 'Marshall to lily',\n",
       "       'Marshall to Barney', 'Marshall to himself', 'Marshall to Robin',\n",
       "       'April', 'Wendy ', 'Everybody', 'Man in the street ',\n",
       "       'Marshall voiceover', 'Regis Philbin ', 'Regis Philbin',\n",
       "       '\"Million Dollar', 'Man in the streets', 'Regis', 'Lucy',\n",
       "       'Stella ', 'Joel', 'Biran',\n",
       "       'She leaves home and takes Stella the bike that is on the porch. It is 10',\n",
       "       '10', 'Matisse', 'Cindy', 'Stewart', 'Female', 'Sister Stella',\n",
       "       'Nora', 'Tony', 'Server', 'Purifier', 'Host', 'Father of Robin',\n",
       "       'Father',\n",
       "       'Then a woman walks with a dog and a child on his stomach, followed by several puppies Marshall',\n",
       "       'Men', 'Mr. Li', 'Driver', 'Jillian', 'Woman ', 'Sven', 'Misty',\n",
       "       'Vicky', 'Ike', 'Mitch', 'Boy 1', 'Boy 2', 'A brother', 'Woman 1',\n",
       "       'Male 2', 'Doug ', 'Children', 'Heather', 'Male', 'Man 3', 'Man ',\n",
       "       'Magazine', 'Leader', 'Seller',\n",
       "       'Marshall turns his sign which says', 'Melissa',\n",
       "       'Marshall at the office', 'Presenter', 'Rochelle', 'Radio',\n",
       "       'Referee', 'Loretta', 'Betty', 'Grant', 'Ted and Betty', '1',\n",
       "       'Karen', 'Nicole', 'Friday, 3', 'Lily, old', 'Ted, old',\n",
       "       'Marshall, old', 'Karen, old', 'Barney by nightdress',\n",
       "       'Robin, entering', 'Robin, old', 'Marshall and Barney', 'Roy',\n",
       "       'Louisa', 'Arty', 'Marshall, Lily and Barney', 'Snowshoeing',\n",
       "       'Pattern', 'McCracken',\n",
       "       'Ted  Then one day, in the suffering of the worst hangover of my life, I realized that one person in the world understood me',\n",
       "       'Roger Murtaugh', 'Robin and Barney', 'Kenny',\n",
       "       'Robin, out of the room', 'Toy-Man',\n",
       "       'Massage man, massaging the shoulders of Marshall', 'Douglas',\n",
       "       'Douglas, Barney', 'PJ', 'Ted, by phone at PJ interposed',\n",
       "       'Grub-man', 'Toy-man', 'Chow-Man', 'Stan', 'Clown',\n",
       "       'Barney, entering', 'Matthew', 'Woman 3', 'Pauline', 'Fran',\n",
       "       'Milt', 'Ted, crying', 'Ted, with a lookalike Stella',\n",
       "       'Lookalike Stella', 'Police', 'Woman, leaving her', 'Ms. Matsen',\n",
       "       'Stella, shouting', 'Tracey', 'Barney and Robin',\n",
       "       'And after having s*x', 'Blond Girl', 'A boy', 'Another boy',\n",
       "       'Another girl', 'Blond girl', 'Student', 'Other professor',\n",
       "       'Professor', 'Jen', 'Delivery girl', 'Marsahll', 'Ted and Jen',\n",
       "       'All the other', 'The other', 'Marhsall', 'Ted imagines', 'Girl 3',\n",
       "       'Shin-Ya',\n",
       "       'Barney and Robin are walking sadly in the street. We can hear a song',\n",
       "       'fighting', 'All singing', 'Ted, joining them',\n",
       "       'Marshall, in bar with Lily',\n",
       "       'Marshall, entering with \"a hat with drink\"', 'Lily, fighting Ted',\n",
       "       'Marshall and Ted, singing', 'Lily, on the roof of the car',\n",
       "       'Ted and Marshall', 'Alan',\n",
       "       'Barney, seeing himself in the window of the restaurant',\n",
       "       'Alan Thicke', 'Meg-the-crazy', 'Meg-the-insane', 'Ms. Stinsfire',\n",
       "       'Shelly', 'Ted, with chicken fingers in the mouth',\n",
       "       'All, making the chorus', 'Robin and Ted', 'Lily Young', 'Mickey',\n",
       "       'Rita', 'Grandfather', 'Neighbor', 'Whitney', 'Grocer', 'Mother',\n",
       "       'Marshall, in video on the computer',\n",
       "       'Mickey, sitting on the floor', 'Marshall, tapping on a glass',\n",
       "       'Voices', 'Older woman', 'Elderly Woman', 'Maggie', 'Lily, a man',\n",
       "       'Student 2', 'Luis', 'Jamie', 'Jim', 'Ted, always current',\n",
       "       'Robin, Jim', 'Ted, in front of Maggie', 'Adam',\n",
       "       'Waitress at the old Marshall', 'Old Marshall', 'Children of Ted',\n",
       "       'Don', 'Chief', 'Ted and Barney', 'Lily, with a deep voice',\n",
       "       'Don, who arrive without pants', 'Young Marshall',\n",
       "       'Marshall Young', 'Barmaid', 'Barney conscience',\n",
       "       'Barney, running down the street', 'Tim',\n",
       "       'Barney, singing and dancing in the street',\n",
       "       'Marshall, doing the same', 'Ted, Marshall, Lily',\n",
       "       'Barney, joining them', 'Jenkins', 'Scotty',\n",
       "       'All, the student bar', 'Ted, from seeing Barney', 'Overall',\n",
       "       'Nick', 'Mr Donovan', 'TV anchor', 'Red-sweater girl',\n",
       "       'All of them', 'All the them', 'Hot Chick', 'Tiffany', 'Lisa',\n",
       "       'Henrietta', 'Mother of Henrietta', 'Jack',\n",
       "       'Marshall, with the phone in the kitchen', 'Anita', 'Ted singing',\n",
       "       'Ted, still singing', 'Anita, Ted', 'The alarm goes off at 12',\n",
       "       'The clock shows 9', 'Marshall Robin. Your part is this',\n",
       "       'Raspberry', 'Mayor', 'Robin, out of his room', 'Delivery Man',\n",
       "       'Robin, who happens', 'Lily, what happens', 'Clint, singing',\n",
       "       'Man going', 'Virginia', 'Ted, leaving her room', 'Ted, an apron',\n",
       "       'Lily, thinking', 'Marshall, thinking', 'Robin, at the window',\n",
       "       'Robin, on the phone', 'Marissa', 'Lily, holding Barney',\n",
       "       'Marshall, laughing', 'Will Shortz', 'Will', 'Ted wrestler',\n",
       "       'Royce', 'Jed Mosley', 'Jed', 'Man and woman', 'Ted stood up',\n",
       "       'Child', 'Robin, on TV', 'Robin, thinking', 'Ted, thinking',\n",
       "       'Barney lookalike', 'Barney, thinking', 'Sosie', 'Sosia',\n",
       "       'Lookalike', \"Ted's wife\", 'Ted &amp; Barney',\n",
       "       'The Girl who was reading', \"Marshall's dad\", 'The man',\n",
       "       'Lauretta', 'Sam Gibbs', 'Sam', 'Basketball coatch', 'TV Speaker',\n",
       "       'Chrissy', 'Marsall', 'Folk singer', 'Speaker', 'Medic', 'Becky',\n",
       "       'Maury Povich', 'Man, in a calculator display', 'Max',\n",
       "       'tr*nsv*stite', \"Barney's Secretary\", 'Zoey', 'Rob',\n",
       "       'Marshall and his friends, singing', 'Announcer', 'then in a club',\n",
       "       \"Marshall's daughter\", 'ANNOUNCER', 'Student #1', 'Student #2',\n",
       "       'The Bar', 'GNB announcer', 'Randy, thinking', 'Randy, wailing',\n",
       "       'Arthur, entering the room', 'Russel', 'Ted, whispering',\n",
       "       'Museum guard', \"Ted's voice \", 'Guard', 'Jessica Glitter',\n",
       "       'Jessica, singing', 'Robin and Jessica', 'Jessica', 'Blitz',\n",
       "       'Bar Tender', 'Mysterious voice', 'Everyone, chanting', 'Steve',\n",
       "       'Babaka', 'Barney, laughing', 'Barney, crying', 'Liy',\n",
       "       'Robin, slurring', 'Sailor #1', 'Sailor #2', 'Animator', 'Jordan',\n",
       "       'Jordin', 'Ted, yelling', 'Barney, singsongy', 'Robin, stuttering',\n",
       "       'Stangel', 'Strangel', 'Man #3', 'Man #4', 'Robin, hushing',\n",
       "       'Reverend', 'Trey', 'Barney &amp; Ted', \"Marshall's brother #1\",\n",
       "       \"Marshall's brother #2\", 'Nathan Hale', \"Ted's dad\", \"Lily's dad\",\n",
       "       \"Robin's dad\", 'Barney, his voice breaking', \"Marvin's voice\",\n",
       "       'Barney, coughing', 'Honey', 'Robin, with a Minnesotan accent',\n",
       "       'Honey, whispering', 'Barney, whispering', 'Ted, laughing',\n",
       "       'Honey, giggling', 'Barney, sobbing', 'Neighbour',\n",
       "       'ARNOLD SCHWARZENEGGER', 'Saint Desperatius', 'Saint Valentine',\n",
       "       'Marshall, whispers', 'Bev', 'Ted &amp; Marshall', 'MARVIN',\n",
       "       'SCHWARZENEGGER ', 'Tv', 'Meeker', 'Nate', 'The cardiologist',\n",
       "       'Ted, on the phone with Robin', 'Jerry', 'Scott', 'Jerry ', 'Both',\n",
       "       'Cherryl', 'JJ', 'Prof. Rodriguez', 'Barney, weakly',\n",
       "       'Barney, with a mean laugh', 'Jerry, laughing', 'ROBIN',\n",
       "       'Ted, whispering to Robin', 'Jerry, looking horrified',\n",
       "       'Ted, loudly', 'Jerry, to a big tattoed biker',\n",
       "       'Jerry, to a dummy wearing a biker outfit', 'Ted, chuckling',\n",
       "       'Man, on the phone', 'Head of the environmental Organization',\n",
       "       'Assistant', 'Delivery Guy', 'Robin, in a high-pitched voice',\n",
       "       'Marshall, to the girl Barney is talking to',\n",
       "       'Marshall, talking to a mirror', 'Robin &amp; Lily', 'Chairman',\n",
       "       'Audience', 'Ghost', 'Ghost, shouting dramatically',\n",
       "       'Ted, panting', 'Ted, on recorder',\n",
       "       'Both Ted and Barney, at the same time', 'Foreman', 'Rod',\n",
       "       'Ted, while texting on his phone',\n",
       "       'Marshall, picking up his phone', 'announcer', \"Ted's voice\",\n",
       "       'voice', 'Jake', \"Ted, leaving the florist's shop\", 'Norah',\n",
       "       'Marshall, lifting the shirt of Lily', 'Kelly and Punchy', 'Clay',\n",
       "       'Ted, Barney and Robin', \"Kelly's father\", \"Punchy's father\",\n",
       "       'Garrison', 'Barney, turning', 'Lily, slapping him',\n",
       "       'Lily and Robin', \"Marshall's voice\"], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "himym_df['character'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: May consider feeding one sentence and one Barney reply or multiple sentences encoded with one Barney reply\n",
    "def get_barney(himym_df, level=2):\n",
    "    dataframe_rows = []\n",
    "    idxs_barney = himym_df[himym_df['character'] == 'Barney'].index\n",
    "    for i in range(-1, -level-1, -1):\n",
    "        for j in idxs_barney:\n",
    "            dataframe_row = {\n",
    "                \"reply\": himym_df['line'][j],\n",
    "                \"sentence\": himym_df['line'][j+i],\n",
    "            }\n",
    "            dataframe_rows.append(dataframe_row)\n",
    "    df = pd.DataFrame(dataframe_rows)\n",
    "    return df\n",
    "    \n",
    "barney_df = get_barney(himym_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reply</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hey, so you know how I've always had a thing...</td>\n",
       "      <td>What was I doing? Your Uncle Marshall was tak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Okay, meet me at the bar in fifteen minutes, ...</td>\n",
       "      <td>Hey, you wanna do something tonight?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Where's your suit!? Just once when I say suit...</td>\n",
       "      <td>Hey.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It was a blazer!</td>\n",
       "      <td>I did that one time.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I see what this is about. Have you forgotten ...</td>\n",
       "      <td>You know, ever since college it's been Marsha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               reply  \\\n",
       "0    hey, so you know how I've always had a thing...   \n",
       "1   Okay, meet me at the bar in fifteen minutes, ...   \n",
       "2   Where's your suit!? Just once when I say suit...   \n",
       "3                                   It was a blazer!   \n",
       "4   I see what this is about. Have you forgotten ...   \n",
       "\n",
       "                                            sentence  \n",
       "0   What was I doing? Your Uncle Marshall was tak...  \n",
       "1               Hey, you wanna do something tonight?  \n",
       "2                                               Hey.  \n",
       "3                               I did that one time.  \n",
       "4   You know, ever since college it's been Marsha...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "barney_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Why? Give me one good reason.\n"
     ]
    }
   ],
   "source": [
    "print(barney_df.iloc[8174][\"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "barney_path = os.path.join(os.getcwd(), \"Datasets\", \"Characters\", \"Barney\")\n",
    "if not os.path.exists(barney_path):\n",
    "    os.makedirs(barney_path)\n",
    "barney_df.to_csv(os.path.join(barney_path, \"Barney.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-6e7b55061fd540b5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:\\Users\\tonel\\.cache\\huggingface\\datasets\\csv\\default-6e7b55061fd540b5\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5149a958294c4dfa9449763be8d2e837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd2b1f478274bb6bfab325fb4e64ae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:\\Users\\tonel\\.cache\\huggingface\\datasets\\csv\\default-6e7b55061fd540b5\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a47a5aae6ea641699f503f31f73ad312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "barney_hg = load_dataset('csv', data_files=os.path.join(barney_path, \"Barney.csv\"))\n",
    "barney_hg = barney_hg[\"train\"].train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "499eb480ab71456e8db577c9f3e7bc1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feecce80b38a4c6bacc853d274895630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TEMPORARY FIX\n",
    "barney_hg = barney_hg.filter(lambda x: x['sentence'] != None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/facebook/blenderbot-400M-distill/resolve/main/config.json from cache at C:\\Users\\tonel/.cache\\huggingface\\transformers\\cec3df71c8d94a67ad9280220975d00732c9dbcf7adaa3afe440d6626e5fdf02.6d6f9df9c9b98c6aa3a827af7ec4aae285715ccb3673cadfe98cacb7235b5809\n",
      "Model config BlenderbotConfig {\n",
      "  \"_name_or_path\": \"./\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"BlenderbotForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1280,\n",
      "  \"decoder_attention_heads\": 32,\n",
      "  \"decoder_ffn_dim\": 5120,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 1,\n",
      "  \"do_blenderbot_90_layernorm\": true,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 32,\n",
      "  \"encoder_ffn_dim\": 5120,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 2,\n",
      "  \"encoder_no_repeat_ngram_size\": 3,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_layer_norm\": false,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layernorm_variant\": \"prelayernorm\",\n",
      "  \"length_penalty\": 0.65,\n",
      "  \"max_length\": 60,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"min_length\": 20,\n",
      "  \"model_type\": \"blenderbot\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 10,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"unk_token_id\": 3,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8008\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/facebook/blenderbot-400M-distill/resolve/main/pytorch_model.bin from cache at C:\\Users\\tonel/.cache\\huggingface\\transformers\\69114910cce7854dd35d7b32d82164ac7f2096786e5bfe6913ad20e0d7dd9232.6d6ac40db66ec473de81a5da226c78b367f22fac1e1967f21c6b4a51bed4ac77\n",
      "All model checkpoint weights were used when initializing BlenderbotForConditionalGeneration.\n",
      "\n",
      "All the weights of BlenderbotForConditionalGeneration were initialized from the model checkpoint at facebook/blenderbot-400M-distill.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BlenderbotForConditionalGeneration for predictions without further training.\n",
      "loading configuration file https://huggingface.co/facebook/blenderbot-400M-distill/resolve/main/config.json from cache at C:\\Users\\tonel/.cache\\huggingface\\transformers\\cec3df71c8d94a67ad9280220975d00732c9dbcf7adaa3afe440d6626e5fdf02.6d6f9df9c9b98c6aa3a827af7ec4aae285715ccb3673cadfe98cacb7235b5809\n",
      "Model config BlenderbotConfig {\n",
      "  \"_name_or_path\": \"facebook/blenderbot-400M-distill\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"BlenderbotForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1280,\n",
      "  \"decoder_attention_heads\": 32,\n",
      "  \"decoder_ffn_dim\": 5120,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 1,\n",
      "  \"do_blenderbot_90_layernorm\": true,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 32,\n",
      "  \"encoder_ffn_dim\": 5120,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 2,\n",
      "  \"encoder_no_repeat_ngram_size\": 3,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_layer_norm\": false,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layernorm_variant\": \"prelayernorm\",\n",
      "  \"length_penalty\": 0.65,\n",
      "  \"max_length\": 60,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"min_length\": 20,\n",
      "  \"model_type\": \"blenderbot\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 10,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"unk_token_id\": 3,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8008\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/facebook/blenderbot-400M-distill/resolve/main/vocab.json from cache at C:\\Users\\tonel/.cache\\huggingface\\transformers\\7028b91bb88b3857c8b39c3e87a2d058f5d222f0873f3cd04b99b2a1caaa2b0b.495aa40483935ec333dde5e4595b903db5aa0fc57a79f25022e27d3a75556a18\n",
      "loading file https://huggingface.co/facebook/blenderbot-400M-distill/resolve/main/merges.txt from cache at C:\\Users\\tonel/.cache\\huggingface\\transformers\\9946256ad3921b4510dd4dacd1ff79ab858e091a14d6f719399e1fa47fd1fa65.e6449ac70db2a08d967dd4fd7ba725d6cd04f73a7c5c96a8475f311c70f1a768\n",
      "loading file https://huggingface.co/facebook/blenderbot-400M-distill/resolve/main/tokenizer_config.json from cache at C:\\Users\\tonel/.cache\\huggingface\\transformers\\30e9f81770d0f4a691aaff06c586db5730e457fa96d754ac306acf916f0bbcd3.e04d97f4b5e234d44196511ac22d95e512c634fd8ad5a9376d98e27acf3d1064\n",
      "loading file https://huggingface.co/facebook/blenderbot-400M-distill/resolve/main/added_tokens.json from cache at C:\\Users\\tonel/.cache\\huggingface\\transformers\\3a3805b42f5ff431da2a76892c88011198b0b90326a890023c92a1d33d031ce5.7a6dfb78e62864844c95917f8aafc82e1fa5826992383e3ce9bc14e224d33b3c\n",
      "loading file https://huggingface.co/facebook/blenderbot-400M-distill/resolve/main/special_tokens_map.json from cache at C:\\Users\\tonel/.cache\\huggingface\\transformers\\3aa6ff874b3f48debb1da94a1c4322b8d620187229fb3072548a20c457c69076.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0\n",
      "loading configuration file https://huggingface.co/facebook/blenderbot-400M-distill/resolve/main/config.json from cache at C:\\Users\\tonel/.cache\\huggingface\\transformers\\cec3df71c8d94a67ad9280220975d00732c9dbcf7adaa3afe440d6626e5fdf02.6d6f9df9c9b98c6aa3a827af7ec4aae285715ccb3673cadfe98cacb7235b5809\n",
      "Model config BlenderbotConfig {\n",
      "  \"_name_or_path\": \"facebook/blenderbot-400M-distill\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"BlenderbotForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1280,\n",
      "  \"decoder_attention_heads\": 32,\n",
      "  \"decoder_ffn_dim\": 5120,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 1,\n",
      "  \"do_blenderbot_90_layernorm\": true,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 32,\n",
      "  \"encoder_ffn_dim\": 5120,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 2,\n",
      "  \"encoder_no_repeat_ngram_size\": 3,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_layer_norm\": false,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layernorm_variant\": \"prelayernorm\",\n",
      "  \"length_penalty\": 0.65,\n",
      "  \"max_length\": 60,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"min_length\": 20,\n",
      "  \"model_type\": \"blenderbot\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 10,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"unk_token_id\": 3,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8008\n",
      "}\n",
      "\n",
      "Adding <mask> to the vocabulary\n",
      "loading configuration file https://huggingface.co/facebook/blenderbot-400M-distill/resolve/main/config.json from cache at C:\\Users\\tonel/.cache\\huggingface\\transformers\\cec3df71c8d94a67ad9280220975d00732c9dbcf7adaa3afe440d6626e5fdf02.6d6f9df9c9b98c6aa3a827af7ec4aae285715ccb3673cadfe98cacb7235b5809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config BlenderbotConfig {\n",
      "  \"_name_or_path\": \"facebook/blenderbot-400M-distill\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"BlenderbotForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1280,\n",
      "  \"decoder_attention_heads\": 32,\n",
      "  \"decoder_ffn_dim\": 5120,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 1,\n",
      "  \"do_blenderbot_90_layernorm\": true,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 32,\n",
      "  \"encoder_ffn_dim\": 5120,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 2,\n",
      "  \"encoder_no_repeat_ngram_size\": 3,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_layer_norm\": false,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layernorm_variant\": \"prelayernorm\",\n",
      "  \"length_penalty\": 0.65,\n",
      "  \"max_length\": 60,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"min_length\": 20,\n",
      "  \"model_type\": \"blenderbot\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 10,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"unk_token_id\": 3,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8008\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BlenderbotForConditionalGeneration\n",
    "mname = 'facebook/blenderbot-400M-distill'\n",
    "model = BlenderbotForConditionalGeneration.from_pretrained(mname)\n",
    "tokenizer = AutoTokenizer.from_pretrained(mname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human:  Fuck you.\n",
      "Bot:   I know, right?  I was so mad.  I don't know what I would have done if it happened to me.\n"
     ]
    }
   ],
   "source": [
    "UTTERANCE = \"Fuck you.\"\n",
    "print(\"Human: \", UTTERANCE)\n",
    "inputs = tokenizer([UTTERANCE], return_tensors='pt')\n",
    "reply_ids = model.generate(**inputs)\n",
    "print(\"Bot: \", tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Unnamed: 0', 'reply', 'sentence'],\n",
      "        num_rows: 8932\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['Unnamed: 0', 'reply', 'sentence'],\n",
      "        num_rows: 993\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(barney_hg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b60302222eb04cfe8401980004fd98a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f19c73d6c384042b95ba3e945b7fc31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"sentence\"]\n",
    "    targets = examples[\"reply\"]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        model_targets = tokenizer(targets, max_length=128, truncation=True)\n",
    "    model_inputs[\"decoder_input_ids\"] = model_targets[\"input_ids\"]\n",
    "    model_inputs[\"decoder_attention_mask\"] = model_targets[\"attention_mask\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_barney_hg = barney_hg.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Unnamed: 0', 'reply', 'sentence', 'input_ids', 'attention_mask', 'decoder_input_ids', 'decoder_attention_mask'],\n",
      "        num_rows: 8932\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['Unnamed: 0', 'reply', 'sentence', 'input_ids', 'attention_mask', 'decoder_input_ids', 'decoder_attention_mask'],\n",
      "        num_rows: 993\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "print(tokenized_barney_hg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_barney_hg['train'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor layer in model.layers[0].encoder.layers:\\n    layer.trainable = False\\nfor i in range(10):\\n    model.layers[0].decoder.layers[i].trainable = False\\n    \\nmodel.summary()\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for layer in model.layers[0].encoder.layers:\n",
    "    layer.trainable = False\n",
    "for i in range(10):\n",
    "    model.layers[0].decoder.layers[i].trainable = False\n",
    "    \n",
    "model.summary()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BlenderbotForConditionalGeneration.forward` and have been ignored: sentence, reply, Unnamed: 0. If sentence, reply, Unnamed: 0 are not expected by `BlenderbotForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 8932\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 559\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mconvert_to_tensors\u001b[1;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[0;32m    707\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 708\u001b[1;33m                     \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mas_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 7 at dim 1 (got 12)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-34af8cc7f131>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1373\u001b[0m             \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1374\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1376\u001b[0m                 \u001b[1;31m# Skip past any already trained steps if resuming training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    568\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\data\\data_collator.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, features, return_tensors)\u001b[0m\n\u001b[0;32m    584\u001b[0m                     \u001b[0mfeature\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"labels\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mremainder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"labels\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m         features = self.tokenizer.pad(\n\u001b[0m\u001b[0;32m    587\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mpad\u001b[1;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[0;32m   2860\u001b[0m                 \u001b[0mbatch_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2862\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mBatchEncoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2863\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2864\u001b[0m     def create_token_type_ids_from_sequences(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_sequences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprepend_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprepend_batch_axis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mconvert_to_tensors\u001b[1;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[0;32m    722\u001b[0m                         \u001b[1;34m\"Please see if a fast version of this tokenizer is available to have this feature available.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    723\u001b[0m                     )\n\u001b[1;32m--> 724\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m    725\u001b[0m                     \u001b[1;34m\"Unable to create tensor, you should probably activate truncation and/or padding \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m                     \u001b[1;34m\"with 'padding=True' 'truncation=True' to have batched tensors with the same length.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length."
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_barney_hg[\"train\"],\n",
    "    eval_dataset=tokenized_barney_hg[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntf_train_set = tokenized_barney_hg[\"train\"].to_tf_dataset(\\n    columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\"],\\n    shuffle=True,\\n    batch_size=16,\\n    collate_fn=data_collator,\\n)\\n\\ntf_test_set = tokenized_barney_hg[\"test\"].to_tf_dataset(\\n    columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\"],\\n    shuffle=False,\\n    batch_size=16,\\n    collate_fn=data_collator,\\n)\\n\\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5))\\nmodel.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3)\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "tf_train_set = tokenized_barney_hg[\"train\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_test_set = tokenized_barney_hg[\"test\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5))\n",
    "model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UTTERANCE = \"My friends are cool but they eat too many carbs.\"\n",
    "print(\"Human: \", UTTERANCE)\n",
    "inputs = tokenizer([UTTERANCE], return_tensors='pt')\n",
    "reply_ids = model.generate(**inputs)\n",
    "print(\"Bot: \", tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
